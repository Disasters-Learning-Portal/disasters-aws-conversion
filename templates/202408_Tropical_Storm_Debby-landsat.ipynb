{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disaster AWS COG Conversion Template\n",
    "\n",
    "This template provides a comprehensive workflow for converting satellite imagery to Cloud Optimized GeoTIFFs (COGs) with:\n",
    "- **Modular architecture** with single-responsibility functions\n",
    "- **Automatic error handling** and recovery\n",
    "- **Memory-efficient processing** for large files\n",
    "- **S3 streaming and caching** capabilities\n",
    "\n",
    "## Key Features\n",
    "- âœ… Handles files from <1GB to >10GB\n",
    "- âœ… Prevents striping issues with fixed chunk processing\n",
    "- âœ… Automatic S3 existence checking\n",
    "- âœ… ZSTD compression with optimal predictors\n",
    "- âœ… Comprehensive error tracking\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“‹ CONFIGURATION CELL - MODIFY PARAMETERS HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Configuration loaded successfully!\n",
      "Event: 202408_TropicalStorm_Debby\n",
      "Source: s3://nasa-disasters/drcs_activations/202408_TropicalStorm_Debby/landsat8\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# MAIN CONFIGURATION - MODIFY THESE VALUES\n",
    "# ========================================\n",
    "\n",
    "# Event Configuration\n",
    "EVENT_NAME = '202408_TropicalStorm_Debby'  # Event identifier\n",
    "PRODUCT_NAME = 'landsat8'          # Product type (sentinel1, sentinel2, landsat, etc.)\n",
    "\n",
    "# S3 Configuration\n",
    "BUCKET = 'nasa-disasters'                                # S3 bucket name\n",
    "DIR_OLD_BASE = 'drcs_activations'                       # Source directory base\n",
    "DIR_NEW_BASE = 'drcs_activations_new'                   # Destination directory base\n",
    "PATH_OLD = f'{DIR_OLD_BASE}/{EVENT_NAME}/{PRODUCT_NAME}' # Full source path\n",
    "\n",
    "# File Size Thresholds (in GB)\n",
    "LARGE_FILE_THRESHOLD = 3   # Files > 3GB use large file config\n",
    "ULTRA_LARGE_THRESHOLD = 7  # Files > 7GB use ultra-large config\n",
    "\n",
    "# Memory Configuration\n",
    "MEMORY_LIMIT_MB = 500      # Memory limit per chunk\n",
    "FORCE_FIXED_CHUNKS = True  # Use fixed chunks for large files (prevents striping)\n",
    "\n",
    "# Output Configuration\n",
    "SAVE_LOCAL = True          # Save files locally during processing\n",
    "SAVE_METADATA = True       # Save processing metadata to bucket\n",
    "VERBOSE = True             # Verbose output for functions\n",
    "\n",
    "# Advanced Configuration (usually don't need to change)\n",
    "USE_STREAMING = False      # Stream from S3 (set False if having issues with large files)\n",
    "CACHE_DOWNLOADS = True     # Cache downloaded files\n",
    "MAX_RETRIES = 3           # Maximum retry attempts\n",
    "\n",
    "print(\"âœ… Configuration loaded successfully!\")\n",
    "print(f\"Event: {EVENT_NAME}\")\n",
    "print(f\"Source: s3://{BUCKET}/{PATH_OLD}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## â™»ï¸ Overwrite and Verification Configuration\n",
    "\n",
    "Control whether to overwrite existing files and verify processing results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Overwrite and verification configuration loaded\n",
      "Overwrite mode: ENABLED\n",
      "Verification: ENABLED\n",
      "âš ï¸  WARNING: Existing files will be overwritten!\n",
      "   This may incur additional processing time and S3 costs.\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# OVERWRITE AND VERIFICATION CONFIGURATION\n",
    "# ========================================\n",
    "\n",
    "# Overwrite Configuration\n",
    "OVERWRITE_EXISTING = True  # Set to True to reprocess and overwrite existing files\n",
    "                            # Set to False to skip existing files (default behavior)\n",
    "\n",
    "# Verification Configuration  \n",
    "VERIFY_PROCESSING = True    # Compare input vs output to verify COG transformation\n",
    "SAVE_VERIFICATION_PLOTS = True  # Save comparison plots for verification\n",
    "VERIFICATION_SAMPLE_SIZE = 5     # Number of files to verify per product type\n",
    "VERIFICATION_DIR = f'verification/{EVENT_NAME}'  # Directory for verification results\n",
    "\n",
    "# Quality Control\n",
    "CHECK_NODATA_PROPAGATION = True  # Verify no-data values are properly handled\n",
    "COMPARE_STATISTICS = True         # Compare min/max/mean between input and output\n",
    "\n",
    "print(\"âœ… Overwrite and verification configuration loaded\")\n",
    "print(f\"Overwrite mode: {'ENABLED' if OVERWRITE_EXISTING else 'DISABLED (will skip existing)'}\")\n",
    "print(f\"Verification: {'ENABLED' if VERIFY_PROCESSING else 'DISABLED'}\")\n",
    "if OVERWRITE_EXISTING:\n",
    "    print(\"âš ï¸  WARNING: Existing files will be overwritten!\")\n",
    "    print(\"   This may incur additional processing time and S3 costs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“¦ Import Required Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Standard libraries imported\n",
      "Module path: /home/jovyan/disasters-aws-conversion\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import gc\n",
    "import tempfile\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Data processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Geospatial libraries\n",
    "import rasterio\n",
    "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
    "from rasterio.windows import Window\n",
    "\n",
    "# AWS libraries\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError, NoCredentialsError\n",
    "\n",
    "# Progress tracking\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"âœ… Standard libraries imported\")\n",
    "\n",
    "# Add parent directory to path for module imports\n",
    "module_path = Path('..').resolve()\n",
    "if str(module_path) not in sys.path:\n",
    "    sys.path.insert(0, str(module_path))\n",
    "\n",
    "print(f\"Module path: {module_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All disaster-aws-conversion modules imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import disaster-aws-conversion modules\n",
    "try:\n",
    "    # Core modules\n",
    "    from core.s3_operations import (\n",
    "        initialize_s3_client,\n",
    "        check_s3_file_exists,\n",
    "        list_s3_files,\n",
    "        get_file_size_from_s3\n",
    "    )\n",
    "    from core.validation import validate_cog, check_cog_with_warnings\n",
    "    from core.compression import get_predictor_for_dtype, export_cog_profile\n",
    "    \n",
    "    # Utils\n",
    "    from utils.memory_management import get_memory_usage, monitor_memory\n",
    "    from utils.error_handling import cleanup_temp_files\n",
    "    from utils.logging import print_status, print_summary\n",
    "    \n",
    "    # Processors\n",
    "    from processors.batch_processor import process_file_batch, monitor_batch_progress\n",
    "    \n",
    "    # Configs\n",
    "    from configs.profiles import select_profile_by_size\n",
    "    from configs.chunk_configs import get_chunk_config\n",
    "    \n",
    "    # Main processor\n",
    "    from main_processor import convert_to_cog\n",
    "    \n",
    "    print(\"âœ… All disaster-aws-conversion modules imported successfully!\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"âš ï¸ Import error: {e}\")\n",
    "    print(\"Make sure you're running from the disaster-aws-conversion directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… S3 client initialized with full access to nasa-disasters\n",
      "âœ… Confirmed access to nasa-disasters bucket\n",
      "âœ… S3 filesystem (fsspec) initialized\n",
      "âœ… S3 client ready for operations\n"
     ]
    }
   ],
   "source": [
    "# Initialize S3 client\n",
    "s3_client, fs_read = initialize_s3_client(bucket_name=BUCKET, verbose=VERBOSE)\n",
    "\n",
    "if s3_client:\n",
    "    print(\"âœ… S3 client ready for operations\")\n",
    "else:\n",
    "    print(\"âŒ Failed to initialize S3 client\")\n",
    "    print(\"Please check your AWS credentials\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”Œ Initialize AWS S3 Connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ” Discover Files in S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Found 9 .tif files in s3://nasa-disasters/drcs_activations/202408_TropicalStorm_Debby/landsat8\n",
      "\n",
      "Files:\n",
      "  - LC08_colorInfrared_20240715_155319_016036.tif (0.2 GB)\n",
      "  - LC08_colorInfrared_20240715_155343_016037.tif (0.2 GB)\n",
      "  - LC08_colorInfrared_20240715_15547_016038.tif (0.2 GB)\n",
      "  - LC08_naturalColor_20240715_155319_016036.tif (0.2 GB)\n",
      "  - LC08_naturalColor_20240715_155343_016037.tif (0.2 GB)\n",
      "  - LC08_naturalColor_20240715_15547_016038.tif (0.2 GB)\n",
      "  - LC08_trueColor_20240715_155319_016036.tif (0.2 GB)\n",
      "  - LC08_trueColor_20240715_155343_016037.tif (0.2 GB)\n",
      "  - LC08_trueColor_20240715_15547_016038.tif (0.2 GB)\n"
     ]
    }
   ],
   "source": [
    "# List all TIF files in the source path\n",
    "if s3_client:\n",
    "    keys = list_s3_files(s3_client, BUCKET, PATH_OLD, suffix='.tif')\n",
    "    print(f\"âœ… Found {len(keys)} .tif files in s3://{BUCKET}/{PATH_OLD}\")\n",
    "    \n",
    "    # Show first 5 files as example\n",
    "    if keys:\n",
    "        print(\"\\nFiles:\")\n",
    "        for key in keys:\n",
    "            file_size = get_file_size_from_s3(s3_client, BUCKET, key)\n",
    "            print(f\"  - {os.path.basename(key)} ({file_size:.1f} GB)\")\n",
    "else:\n",
    "    keys = []\n",
    "    print(\"âŒ No S3 client available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Based on the files that are in the directory, we can now add regex patterns to select specific types of files and move into specific directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - colorInfrared -> drcs_activations_new/Landsat/colorIR\n",
      "  - naturalColor -> drcs_activations_new/Landsat/naturalColor\n",
      "  - trueColor|truecolor -> drcs_activations_new/Landsat/trueColor\n"
     ]
    }
   ],
   "source": [
    "# Product Type Configuration\n",
    "# Define patterns and output directories for different product types\n",
    "# Modify this dictionary to add/remove product types as needed\n",
    "PRODUCT_CONFIGS = {\n",
    "    # Pattern (regex or string): Output directory relative to DIR_NEW_BASE\n",
    "    'colorInfrared': 'Landsat/colorIR',\n",
    "    'naturalColor': 'Landsat/naturalColor',\n",
    "    'trueColor|truecolor': 'Landsat/trueColor',  # Multiple patterns with |\n",
    "    # Add more patterns as needed:\n",
    "    # 'SAR': 'Sentinel-1/SAR',\n",
    "    # 'DEM': 'Elevation/DEM',\n",
    "    # 'temperature': 'Climate/Temperature',\n",
    "}\n",
    "\n",
    "\n",
    "for pattern, output_dir in PRODUCT_CONFIGS.items():\n",
    "    print(f\"  - {pattern} -> {DIR_NEW_BASE}/{output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”§ No-Data Value Configuration\n",
    "\n",
    "Configure how no-data values are handled during processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… No-data configuration loaded\n",
      "Auto no-data: True\n",
      "Manual overrides configured: 0\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# NO-DATA VALUE CONFIGURATION\n",
    "# ========================================\n",
    "\n",
    "# Automatic no-data detection\n",
    "USE_AUTO_NODATA = True  # Automatically select appropriate no-data values\n",
    "\n",
    "# Manual no-data values per product type\n",
    "# Set to None to use automatic detection for that product\n",
    "MANUAL_NODATA_VALUES = {\n",
    "    'NDVI': None,       # e.g., -9999 for NDVI\n",
    "    'MNDWI': None,      # e.g., -9999 for MNDWI  \n",
    "    'trueColor_or_truecolor': None,  # e.g., 0 for RGB images\n",
    "    # Add more as needed\n",
    "}\n",
    "\n",
    "# Analysis configuration\n",
    "ANALYZE_BEFORE_PROCESSING = True  # Analyze files to determine min/max before processing\n",
    "VALIDATE_NODATA = True           # Validate that no-data values don't conflict with actual data\n",
    "SHOW_ANALYSIS_REPORT = True      # Display analysis report before processing\n",
    "\n",
    "print(\"âœ… No-data configuration loaded\")\n",
    "print(f\"Auto no-data: {USE_AUTO_NODATA}\")\n",
    "print(f\"Manual overrides configured: {sum(v is not None for v in MANUAL_NODATA_VALUES.values())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "colorInfrared: 3 files -> Landsat/colorIR\n",
      "naturalColor: 3 files -> Landsat/naturalColor\n",
      "trueColor_or_truecolor: 3 files -> Landsat/trueColor\n",
      "\n",
      "Total files to process: 9\n"
     ]
    }
   ],
   "source": [
    "# Filter files based on configuration\n",
    "\n",
    "# Filter files by configured patterns\n",
    "files_to_process = {}\n",
    "\n",
    "for pattern, output_dir in PRODUCT_CONFIGS.items():\n",
    "    matching_files = []\n",
    "    for file_path in keys:\n",
    "        # Check if pattern matches the filename\n",
    "        if re.search(pattern, file_path):\n",
    "            matching_files.append(file_path)\n",
    "    \n",
    "    if matching_files:\n",
    "        # Use the pattern as key, but clean it for display\n",
    "        clean_name = pattern.replace('|', '_or_')\n",
    "        files_to_process[clean_name] = {\n",
    "            'files': matching_files,\n",
    "            'output_dir': output_dir\n",
    "        }\n",
    "        print(f\"{clean_name}: {len(matching_files)} files -> {output_dir}\")\n",
    "\n",
    "total_files = sum(len(v['files']) for v in files_to_process.values())\n",
    "print(f\"\\nTotal files to process: {total_files}\")\n",
    "\n",
    "# Show summary\n",
    "if not files_to_process:\n",
    "    print(\"âš ï¸ No files matched the configured patterns!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ·ï¸ Manual Filename Generation\n",
    "\n",
    "Define custom filename generation for each product type. Modify these functions to match your specific naming conventions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Filename generation functions defined\n",
      "Available product handlers: ['trueColor', 'colorInfrared', 'naturalColor']\n"
     ]
    }
   ],
   "source": [
    "def extract_date_from_filename(filename):\n",
    "    \"\"\"Extract date from filename in YYYYMMDD format.\"\"\"\n",
    "    dates = re.findall(r'\\d{8}', filename)\n",
    "    if dates:\n",
    "        # Convert YYYYMMDD to YYYY-MM-DD\n",
    "        date_str = dates[0]\n",
    "        return f\"{date_str[0:4]}-{date_str[4:6]}-{date_str[6:8]}\"\n",
    "    return None\n",
    "\n",
    "def create_colorinfrared_filename(original_path, event_name):\n",
    "    \"\"\"\n",
    "    Create filename for colorInfrared products.\n",
    "    Example: LC08_colorInfrared_20240715_155319_016036.tif -> 202407_EventName_LC08_colorInfrared_155319_016036_2024-07-15_day.tif\n",
    "    \"\"\"\n",
    "    filename = os.path.basename(original_path)\n",
    "    stem = os.path.splitext(filename)[0]\n",
    "    \n",
    "    # Extract date\n",
    "    date = extract_date_from_filename(stem)\n",
    "    \n",
    "    if date:\n",
    "        # Remove only the date (YYYYMMDD) but keep time and other parts\n",
    "        stem_clean = re.sub(r'_\\d{8}', '', stem)\n",
    "        cog_filename = f\"{event_name}_{stem_clean}_{date}_day.tif\"\n",
    "    else:\n",
    "        cog_filename = f\"{event_name}_{stem}_day.tif\"\n",
    "    \n",
    "    return cog_filename\n",
    "\n",
    "def create_naturalcolor_filename(original_path, event_name):\n",
    "    \"\"\"\n",
    "    Create filename for naturalColor products.\n",
    "    Example: LC08_naturalColor_20240715_155343_016037.tif -> 202407_EventName_LC08_naturalColor_155343_016037_2024-07-15_day.tif\n",
    "    \"\"\"\n",
    "    filename = os.path.basename(original_path)\n",
    "    stem = os.path.splitext(filename)[0]\n",
    "    \n",
    "    # Extract date\n",
    "    date = extract_date_from_filename(stem)\n",
    "    \n",
    "    if date:\n",
    "        # Remove only the date (YYYYMMDD) but keep time and other parts\n",
    "        stem_clean = re.sub(r'_\\d{8}', '', stem)\n",
    "        cog_filename = f\"{event_name}_{stem_clean}_{date}_day.tif\"\n",
    "    else:\n",
    "        cog_filename = f\"{event_name}_{stem}_day.tif\"\n",
    "    \n",
    "    return cog_filename\n",
    "\n",
    "def create_truecolor_filename(original_path, event_name):\n",
    "    \"\"\"\n",
    "    Create filename for trueColor products.\n",
    "    Example: LC08_trueColor_20240715_15547_016038.tif -> 202407_EventName_LC08_trueColor_15547_016038_2024-07-15_day.tif\n",
    "    \"\"\"\n",
    "    filename = os.path.basename(original_path)\n",
    "    stem = os.path.splitext(filename)[0]\n",
    "    \n",
    "    # Extract date\n",
    "    date = extract_date_from_filename(stem)\n",
    "    \n",
    "    if date:\n",
    "        # Remove only the date (YYYYMMDD) but keep time and other parts\n",
    "        stem_clean = re.sub(r'_\\d{8}', '', stem)\n",
    "        cog_filename = f\"{event_name}_{stem_clean}_{date}_day.tif\"\n",
    "    else:\n",
    "        cog_filename = f\"{event_name}_{stem}_day.tif\"\n",
    "    \n",
    "    return cog_filename\n",
    "\n",
    "def create_generic_filename(original_path, event_name):\n",
    "    \"\"\"\n",
    "    Create generic filename for any product type.\n",
    "    Removes only date from middle but keeps time, then adds formatted date with _day suffix.\n",
    "    \"\"\"\n",
    "    filename = os.path.basename(original_path)\n",
    "    stem = os.path.splitext(filename)[0]\n",
    "    \n",
    "    # Extract date\n",
    "    date = extract_date_from_filename(stem)\n",
    "    \n",
    "    if date:\n",
    "        # Remove only the date (YYYYMMDD) but keep time and other parts\n",
    "        stem_clean = re.sub(r'_\\d{8}', '', stem)\n",
    "        cog_filename = f\"{event_name}_{stem_clean}_{date}_day.tif\"\n",
    "    else:\n",
    "        cog_filename = f\"{event_name}_{stem}_day.tif\"\n",
    "    \n",
    "    return cog_filename\n",
    "\n",
    "\n",
    "# Mapping of product types to their filename creators\n",
    "FILENAME_CREATORS = {\n",
    "    'trueColor': create_truecolor_filename,\n",
    "    'colorInfrared': create_colorinfrared_filename,\n",
    "    'naturalColor': create_naturalcolor_filename,\n",
    "    # Add more mappings as needed:\n",
    "    # 'SAR': create_sar_filename,\n",
    "    # 'DEM': create_dem_filename,\n",
    "}\n",
    "\n",
    "print(\"âœ… Filename generation functions defined\")\n",
    "print(\"Available product handlers:\", list(FILENAME_CREATORS.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“ Preview Filename Transformations\n",
    "\n",
    "Review how your files will be renamed before processing begins:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ðŸ“‹ FILENAME TRANSFORMATION PREVIEW\n",
      "================================================================================\n",
      "\n",
      "ðŸ”¹ colorInfrared Files:\n",
      "------------------------------------------------------------\n",
      "  Original:  LC08_colorInfrared_20240715_155319_016036.tif\n",
      "  â†’ New:     202408_TropicalStorm_Debby_LC08_colorInfrared_155319_016036_2024-07-15_day.tif\n",
      "\n",
      "  Original:  LC08_colorInfrared_20240715_155343_016037.tif\n",
      "  â†’ New:     202408_TropicalStorm_Debby_LC08_colorInfrared_155343_016037_2024-07-15_day.tif\n",
      "\n",
      "  Original:  LC08_colorInfrared_20240715_15547_016038.tif\n",
      "  â†’ New:     202408_TropicalStorm_Debby_LC08_colorInfrared_15547_016038_2024-07-15_day.tif\n",
      "\n",
      "\n",
      "ðŸ”¹ naturalColor Files:\n",
      "------------------------------------------------------------\n",
      "  Original:  LC08_naturalColor_20240715_155319_016036.tif\n",
      "  â†’ New:     202408_TropicalStorm_Debby_LC08_naturalColor_155319_016036_2024-07-15_day.tif\n",
      "\n",
      "  Original:  LC08_naturalColor_20240715_155343_016037.tif\n",
      "  â†’ New:     202408_TropicalStorm_Debby_LC08_naturalColor_155343_016037_2024-07-15_day.tif\n",
      "\n",
      "  Original:  LC08_naturalColor_20240715_15547_016038.tif\n",
      "  â†’ New:     202408_TropicalStorm_Debby_LC08_naturalColor_15547_016038_2024-07-15_day.tif\n",
      "\n",
      "\n",
      "ðŸ”¹ trueColor_or_truecolor Files:\n",
      "------------------------------------------------------------\n",
      "  Original:  LC08_trueColor_20240715_155319_016036.tif\n",
      "  â†’ New:     202408_TropicalStorm_Debby_LC08_trueColor_155319_016036_2024-07-15_day.tif\n",
      "\n",
      "  Original:  LC08_trueColor_20240715_155343_016037.tif\n",
      "  â†’ New:     202408_TropicalStorm_Debby_LC08_trueColor_155343_016037_2024-07-15_day.tif\n",
      "\n",
      "  Original:  LC08_trueColor_20240715_15547_016038.tif\n",
      "  â†’ New:     202408_TropicalStorm_Debby_LC08_trueColor_15547_016038_2024-07-15_day.tif\n",
      "\n",
      "\n",
      "================================================================================\n",
      "âœ… Review the filename transformations above.\n",
      "   If you need to adjust the naming pattern, modify the\n",
      "   create_*_filename() functions in the previous cell.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Preview filename transformations for each product type\n",
    "print(\"=\" * 80)\n",
    "print(\"ðŸ“‹ FILENAME TRANSFORMATION PREVIEW\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Show sample transformations for each product type\n",
    "for product_name, product_info in files_to_process.items():\n",
    "    print(f\"\\nðŸ”¹ {product_name} Files:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Get the appropriate filename creator\n",
    "    filename_creator = FILENAME_CREATORS.get(product_name, create_generic_filename)\n",
    "    \n",
    "    # Show first 3 files as examples (or all if less than 3)\n",
    "    sample_files = product_info['files'][:min(3, len(product_info['files']))]\n",
    "    \n",
    "    for file_path in sample_files:\n",
    "        original = os.path.basename(file_path)\n",
    "        transformed = filename_creator(file_path, EVENT_NAME)\n",
    "        \n",
    "        print(f\"  Original:  {original}\")\n",
    "        print(f\"  â†’ New:     {transformed}\")\n",
    "        print()\n",
    "    \n",
    "    # Show count of remaining files\n",
    "    remaining = len(product_info['files']) - len(sample_files)\n",
    "    if remaining > 0:\n",
    "        print(f\"  ... and {remaining} more files\")\n",
    "        print()\n",
    "\n",
    "# Ask for confirmation\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"âœ… Review the filename transformations above.\")\n",
    "print(\"   If you need to adjust the naming pattern, modify the\")\n",
    "print(\"   create_*_filename() functions in the previous cell.\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ðŸ—‚ï¸  S3 DESTINATION PATHS PREVIEW\n",
      "================================================================================\n",
      "\n",
      "ðŸ”¸ colorInfrared files will be saved to:\n",
      "   s3://nasa-disasters/drcs_activations_new/Landsat/colorIR/\n",
      "\n",
      "   Example full S3 path:\n",
      "   s3://nasa-disasters/drcs_activations_new/Landsat/colorIR/202408_TropicalStorm_Debby_LC08_colorInfrared_155319_016036_2024-07-15_day.tif\n",
      "\n",
      "ðŸ”¸ naturalColor files will be saved to:\n",
      "   s3://nasa-disasters/drcs_activations_new/Landsat/naturalColor/\n",
      "\n",
      "   Example full S3 path:\n",
      "   s3://nasa-disasters/drcs_activations_new/Landsat/naturalColor/202408_TropicalStorm_Debby_LC08_naturalColor_155319_016036_2024-07-15_day.tif\n",
      "\n",
      "ðŸ”¸ trueColor_or_truecolor files will be saved to:\n",
      "   s3://nasa-disasters/drcs_activations_new/Landsat/trueColor/\n",
      "\n",
      "   Example full S3 path:\n",
      "   s3://nasa-disasters/drcs_activations_new/Landsat/trueColor/202408_TropicalStorm_Debby_LC08_trueColor_155319_016036_2024-07-15_day.tif\n",
      "\n",
      "================================================================================\n",
      "ðŸ“Š PROCESSING SUMMARY\n",
      "================================================================================\n",
      "Total files to process: 9\n",
      "Event name: 202408_TropicalStorm_Debby\n",
      "Source bucket: s3://nasa-disasters/drcs_activations/202408_TropicalStorm_Debby/landsat8\n",
      "Destination base: s3://nasa-disasters/drcs_activations_new/\n",
      "\n",
      "Product breakdown:\n",
      "  â€¢ colorInfrared: 3 files\n",
      "  â€¢ naturalColor: 3 files\n",
      "  â€¢ trueColor_or_truecolor: 3 files\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Preview S3 destination paths\n",
    "print(\"=\" * 80)\n",
    "print(\"ðŸ—‚ï¸  S3 DESTINATION PATHS PREVIEW\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for product_name, product_info in files_to_process.items():\n",
    "    output_dir = product_info['output_dir']\n",
    "    print(f\"\\nðŸ”¸ {product_name} files will be saved to:\")\n",
    "    print(f\"   s3://{BUCKET}/{DIR_NEW_BASE}/{output_dir}/\")\n",
    "    \n",
    "    # Show one example with full path\n",
    "    if product_info['files']:\n",
    "        filename_creator = FILENAME_CREATORS.get(product_name, create_generic_filename)\n",
    "        sample_file = product_info['files'][0]\n",
    "        sample_filename = filename_creator(sample_file, EVENT_NAME)\n",
    "        \n",
    "        print(f\"\\n   Example full S3 path:\")\n",
    "        print(f\"   s3://{BUCKET}/{DIR_NEW_BASE}/{output_dir}/{sample_filename}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ðŸ“Š PROCESSING SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Total files to process: {total_files}\")\n",
    "print(f\"Event name: {EVENT_NAME}\")\n",
    "print(f\"Source bucket: s3://{BUCKET}/{PATH_OLD}\")\n",
    "print(f\"Destination base: s3://{BUCKET}/{DIR_NEW_BASE}/\")\n",
    "print(\"\\nProduct breakdown:\")\n",
    "for product_name, product_info in files_to_process.items():\n",
    "    print(f\"  â€¢ {product_name}: {len(product_info['files'])} files\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š Pre-Processing Analysis\n",
    "\n",
    "Analyze sample files to understand data ranges and validate no-data configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def process_files_by_type(file_list, product_name, output_dir, event_name, s3_client):\n    \"\"\"\n    Process a list of files for a specific product type.\n    \n    Args:\n        file_list: List of S3 keys to process\n        product_name: Name/identifier for this batch of files (e.g., 'NDVI', 'MNDWI', 'trueColor_or_truecolor')\n        output_dir: Target output directory\n        event_name: Event name for output naming\n        s3_client: S3 client\n    \n    Returns:\n        DataFrame with processing results\n    \"\"\"\n    if not file_list:\n        return pd.DataFrame()\n    \n    print(f\"\\n{'='*60}\")\n    print(f\"ðŸš€ Processing {product_name}\")\n    print(f\"{'='*60}\")\n    \n    # Configuration for batch processing\n    config = {\n        'raw_data_bucket': BUCKET,\n        'raw_data_prefix': PATH_OLD,\n        'cog_data_bucket': BUCKET,\n        'cog_data_prefix': f'{DIR_NEW_BASE}/{output_dir}',\n        'local_output_dir': f'output/{event_name}/{product_name}' if SAVE_LOCAL else None\n    }\n    \n    print_status(f\"{product_name} Processing Configuration\", config)\n    \n    # Get the appropriate filename creator for this product type\n    filename_creator = FILENAME_CREATORS.get(product_name, create_generic_filename)\n    \n    # Get manual no-data value for this product type\n    manual_nodata = MANUAL_NODATA_VALUES.get(product_name)\n    if manual_nodata is not None:\n        print(f\"   ðŸ“Œ Using manual no-data value: {manual_nodata}\")\n    else:\n        print(f\"   ðŸ”„ Using automatic no-data selection\")\n    \n    # Show overwrite status\n    if OVERWRITE_EXISTING:\n        print(f\"   â™»ï¸  OVERWRITE MODE: Existing files will be replaced\")\n    else:\n        print(f\"   â­ï¸  SKIP MODE: Existing files will be skipped\")\n    \n    # Create local output directory if needed\n    if SAVE_LOCAL and config['local_output_dir']:\n        os.makedirs(config['local_output_dir'], exist_ok=True)\n    \n    # Process each file\n    results = []\n    verification_queue = []  # Files to verify after processing\n    \n    for file_path in tqdm(file_list, desc=f\"Processing {product_name}\"):\n        start_time = datetime.now()\n        \n        try:\n            # Generate COG filename manually using the appropriate function\n            cog_filename = filename_creator(file_path, event_name)\n            \n            # Check if file already exists\n            output_key = f\"{config['cog_data_prefix']}/{cog_filename}\"\n            file_exists = check_s3_file_exists(s3_client, config['cog_data_bucket'], output_key)\n            \n            # Handle existing files based on OVERWRITE_EXISTING flag\n            if file_exists and not OVERWRITE_EXISTING:\n                print(f\"   â­ï¸  Skipping {os.path.basename(file_path)} - already exists\")\n                results.append({\n                    'original_file': file_path,\n                    'output_file': cog_filename,\n                    'status': 'skipped',\n                    'reason': 'File already exists',\n                    'processing_time_s': 0,\n                    'timestamp': datetime.now().isoformat()\n                })\n                continue\n            elif file_exists and OVERWRITE_EXISTING:\n                print(f\"   â™»ï¸  Overwriting existing file: {cog_filename}\")\n            \n            print(f\"\\nðŸ“„ Processing: {os.path.basename(file_path)}\")\n            print(f\"   â†’ Output: {cog_filename}\")\n            \n            # Get file size to determine configuration\n            file_size_gb = get_file_size_from_s3(s3_client, BUCKET, file_path)\n            \n            # Select configuration based on size\n            if file_size_gb > ULTRA_LARGE_THRESHOLD:\n                print(f\"   ðŸ“¦ Ultra-large file ({file_size_gb:.1f} GB), using fixed 128x128 chunks\")\n            elif file_size_gb > LARGE_FILE_THRESHOLD:\n                print(f\"   ðŸ“¦ Large file ({file_size_gb:.1f} GB), using fixed 256x256 chunks\")\n            else:\n                print(f\"   ðŸ“¦ Standard file ({file_size_gb:.1f} GB), using adaptive chunks\")\n            \n            # Get chunk configuration\n            chunk_config = get_chunk_config(\n                file_size_gb=file_size_gb,\n                memory_limit_mb=MEMORY_LIMIT_MB\n            )\n            \n            # Override streaming setting\n            chunk_config['use_streaming'] = USE_STREAMING\n            \n            # Call main processor with manual no-data if configured\n            result = convert_to_cog(\n                name=file_path,\n                bucket=BUCKET,\n                cog_filename=cog_filename,\n                cog_data_bucket=config['cog_data_bucket'],\n                cog_data_prefix=config['cog_data_prefix'],\n                s3_client=s3_client,\n                local_output_dir=config['local_output_dir'],\n                chunk_config=chunk_config,\n                manual_nodata=manual_nodata,  # Pass manual no-data value\n                overwrite=OVERWRITE_EXISTING  # Pass overwrite flag\n            )\n            \n            results.append({\n                'original_file': file_path,\n                'output_file': cog_filename,\n                'output_key': output_key,\n                'status': 'success',\n                'result': result,\n                'processing_time_s': (datetime.now() - start_time).total_seconds(),\n                'timestamp': datetime.now().isoformat()\n            })\n            \n            # Add to verification queue if enabled\n            if VERIFY_PROCESSING and len(verification_queue) < VERIFICATION_SAMPLE_SIZE:\n                verification_queue.append({\n                    'input_key': file_path,\n                    'output_key': output_key,\n                    'filename': cog_filename\n                })\n            \n            print(f\"   âœ… Successfully processed in {(datetime.now() - start_time).total_seconds():.1f}s\")\n            \n        except Exception as e:\n            results.append({\n                'original_file': file_path,\n                'output_file': cog_filename if 'cog_filename' in locals() else None,\n                'status': 'failed',\n                'error': str(e),\n                'processing_time_s': (datetime.now() - start_time).total_seconds(),\n                'timestamp': datetime.now().isoformat()\n            })\n            \n            print(f\"   âŒ Error processing {file_path}: {e}\")\n            if VERBOSE:\n                import traceback\n                traceback.print_exc()\n    \n    # Create results DataFrame\n    results_df = pd.DataFrame(results)\n    \n    # Monitor results\n    monitor_batch_progress(results_df)\n    \n    # Run verification if enabled\n    if VERIFY_PROCESSING and verification_queue:\n        print(f\"\\nðŸ” Verifying {len(verification_queue)} processed files...\")\n        verification_dir = os.path.join(VERIFICATION_DIR, product_name)\n        os.makedirs(verification_dir, exist_ok=True)\n        \n        from tools.verification import verify_s3_files, create_verification_report\n        \n        verification_results = []\n        for item in verification_queue:\n            try:\n                result = verify_s3_files(\n                    BUCKET, item['input_key'],\n                    BUCKET, item['output_key'],\n                    verification_dir, s3_client\n                )\n                verification_results.append(result)\n                print(f\"   âœ“ Verified: {item['filename']}\")\n            except Exception as e:\n                print(f\"   âœ— Verification failed for {item['filename']}: {e}\")\n        \n        # Create verification report\n        if verification_results:\n            report_path = os.path.join(verification_dir, 'verification_report.json')\n            create_verification_report(verification_results, report_path)\n    \n    # Save results if requested\n    if SAVE_METADATA and not results_df.empty and config['local_output_dir']:\n        csv_filename = f\"{config['local_output_dir']}/processing_results_{product_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n        results_df.to_csv(csv_filename, index=False)\n        print(f\"\\nðŸ“Š Results saved to: {csv_filename}\")\n    \n    return results_df\n\nprint(\"âœ… Processing functions updated with overwrite and verification support\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”§ Define Processing Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸš€ Execute Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting processing at 2025-09-27 01:52:37\n",
      "Memory usage at start: 910.4 MB\n"
     ]
    }
   ],
   "source": [
    "# Initialize results storage\n",
    "all_results = []\n",
    "processing_start = datetime.now()\n",
    "\n",
    "print(f\"Starting processing at {processing_start.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Memory usage at start: {get_memory_usage():.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ðŸš€ Processing colorInfrared\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "colorInfrared Processing Configuration\n",
      "============================================================\n",
      "  raw_data_bucket: nasa-disasters\n",
      "  raw_data_prefix: drcs_activations/202408_TropicalStorm_Debby/landsat8\n",
      "  cog_data_bucket: nasa-disasters\n",
      "  cog_data_prefix: drcs_activations_new/Landsat/colorIR\n",
      "  local_output_dir: output/202408_TropicalStorm_Debby/colorInfrared\n",
      "============================================================\n",
      "\n",
      "   ðŸ”„ Using automatic no-data selection\n",
      "   â™»ï¸  OVERWRITE MODE: Existing files will be replaced\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing colorInfrared:   0%|          | 0/3 [00:00<?, ?it/s]Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1527/4017916254.py\", line 107, in process_files_by_type\n",
      "    result = convert_to_cog(\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/jovyan/disasters-aws-conversion/main_processor.py\", line 74, in convert_to_cog\n",
      "    raise FileExistsError(f\"File already exists: {cog_filename}\")\n",
      "FileExistsError: File already exists: 202408_TropicalStorm_Debby_LC08_colorInfrared_155319_016036_2024-07-15_day.tif\n",
      "Processing colorInfrared:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:00,  5.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   â™»ï¸  Overwriting existing file: 202408_TropicalStorm_Debby_LC08_colorInfrared_155319_016036_2024-07-15_day.tif\n",
      "\n",
      "ðŸ“„ Processing: LC08_colorInfrared_20240715_155319_016036.tif\n",
      "   â†’ Output: 202408_TropicalStorm_Debby_LC08_colorInfrared_155319_016036_2024-07-15_day.tif\n",
      "   ðŸ“¦ Standard file (0.2 GB), using adaptive chunks\n",
      "   [CHECK] Checking if file already exists in S3: s3://nasa-disasters/drcs_activations_new/Landsat/colorIR/202408_TropicalStorm_Debby_LC08_colorInfrared_155319_016036_2024-07-15_day.tif\n",
      "   [SKIP] File already exists in S3, skipping processing: 202408_TropicalStorm_Debby_LC08_colorInfrared_155319_016036_2024-07-15_day.tif\n",
      "   [ERROR] File already exists: 202408_TropicalStorm_Debby_LC08_colorInfrared_155319_016036_2024-07-15_day.tif\n",
      "   âŒ Error processing drcs_activations/202408_TropicalStorm_Debby/landsat8/LC08_colorInfrared_20240715_155319_016036.tif: File already exists: 202408_TropicalStorm_Debby_LC08_colorInfrared_155319_016036_2024-07-15_day.tif\n",
      "   â™»ï¸  Overwriting existing file: 202408_TropicalStorm_Debby_LC08_colorInfrared_155343_016037_2024-07-15_day.tif\n",
      "\n",
      "ðŸ“„ Processing: LC08_colorInfrared_20240715_155343_016037.tif\n",
      "   â†’ Output: 202408_TropicalStorm_Debby_LC08_colorInfrared_155343_016037_2024-07-15_day.tif\n",
      "   ðŸ“¦ Standard file (0.2 GB), using adaptive chunks\n",
      "   [CHECK] Checking if file already exists in S3: s3://nasa-disasters/drcs_activations_new/Landsat/colorIR/202408_TropicalStorm_Debby_LC08_colorInfrared_155343_016037_2024-07-15_day.tif\n",
      "   [SKIP] File already exists in S3, skipping processing: 202408_TropicalStorm_Debby_LC08_colorInfrared_155343_016037_2024-07-15_day.tif\n",
      "   [ERROR] File already exists: 202408_TropicalStorm_Debby_LC08_colorInfrared_155343_016037_2024-07-15_day.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1527/4017916254.py\", line 107, in process_files_by_type\n",
      "    result = convert_to_cog(\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/jovyan/disasters-aws-conversion/main_processor.py\", line 74, in convert_to_cog\n",
      "    raise FileExistsError(f\"File already exists: {cog_filename}\")\n",
      "FileExistsError: File already exists: 202408_TropicalStorm_Debby_LC08_colorInfrared_155343_016037_2024-07-15_day.tif\n",
      "Processing colorInfrared:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:00<00:00,  6.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âŒ Error processing drcs_activations/202408_TropicalStorm_Debby/landsat8/LC08_colorInfrared_20240715_155343_016037.tif: File already exists: 202408_TropicalStorm_Debby_LC08_colorInfrared_155343_016037_2024-07-15_day.tif\n",
      "   â™»ï¸  Overwriting existing file: 202408_TropicalStorm_Debby_LC08_colorInfrared_15547_016038_2024-07-15_day.tif\n",
      "\n",
      "ðŸ“„ Processing: LC08_colorInfrared_20240715_15547_016038.tif\n",
      "   â†’ Output: 202408_TropicalStorm_Debby_LC08_colorInfrared_15547_016038_2024-07-15_day.tif\n",
      "   ðŸ“¦ Standard file (0.2 GB), using adaptive chunks\n",
      "   [CHECK] Checking if file already exists in S3: s3://nasa-disasters/drcs_activations_new/Landsat/colorIR/202408_TropicalStorm_Debby_LC08_colorInfrared_15547_016038_2024-07-15_day.tif\n",
      "   [SKIP] File already exists in S3, skipping processing: 202408_TropicalStorm_Debby_LC08_colorInfrared_15547_016038_2024-07-15_day.tif\n",
      "   [ERROR] File already exists: 202408_TropicalStorm_Debby_LC08_colorInfrared_15547_016038_2024-07-15_day.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1527/4017916254.py\", line 107, in process_files_by_type\n",
      "    result = convert_to_cog(\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/jovyan/disasters-aws-conversion/main_processor.py\", line 74, in convert_to_cog\n",
      "    raise FileExistsError(f\"File already exists: {cog_filename}\")\n",
      "FileExistsError: File already exists: 202408_TropicalStorm_Debby_LC08_colorInfrared_15547_016038_2024-07-15_day.tif\n",
      "Processing colorInfrared: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  6.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âŒ Error processing drcs_activations/202408_TropicalStorm_Debby/landsat8/LC08_colorInfrared_20240715_15547_016038.tif: File already exists: 202408_TropicalStorm_Debby_LC08_colorInfrared_15547_016038_2024-07-15_day.tif\n",
      "\n",
      "============================================================\n",
      "BATCH PROCESSING SUMMARY\n",
      "============================================================\n",
      "  total: 3\n",
      "  failed: 3\n",
      "  success_rate: 0.00\n",
      "  total_time_minutes: 0.01\n",
      "  avg_time_seconds: 0.15\n",
      "============================================================\n",
      "\n",
      "ðŸ“Š Results saved to: output/202408_TropicalStorm_Debby/colorInfrared/processing_results_colorInfrared_20250927_015238.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ðŸš€ Processing naturalColor\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "naturalColor Processing Configuration\n",
      "============================================================\n",
      "  raw_data_bucket: nasa-disasters\n",
      "  raw_data_prefix: drcs_activations/202408_TropicalStorm_Debby/landsat8\n",
      "  cog_data_bucket: nasa-disasters\n",
      "  cog_data_prefix: drcs_activations_new/Landsat/naturalColor\n",
      "  local_output_dir: output/202408_TropicalStorm_Debby/naturalColor\n",
      "============================================================\n",
      "\n",
      "   ðŸ”„ Using automatic no-data selection\n",
      "   â™»ï¸  OVERWRITE MODE: Existing files will be replaced\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing naturalColor:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   â™»ï¸  Overwriting existing file: 202408_TropicalStorm_Debby_LC08_naturalColor_155319_016036_2024-07-15_day.tif\n",
      "\n",
      "ðŸ“„ Processing: LC08_naturalColor_20240715_155319_016036.tif\n",
      "   â†’ Output: 202408_TropicalStorm_Debby_LC08_naturalColor_155319_016036_2024-07-15_day.tif\n",
      "   ðŸ“¦ Standard file (0.2 GB), using adaptive chunks\n",
      "   [CHECK] Checking if file already exists in S3: s3://nasa-disasters/drcs_activations_new/Landsat/naturalColor/202408_TropicalStorm_Debby_LC08_naturalColor_155319_016036_2024-07-15_day.tif\n",
      "   [SKIP] File already exists in S3, skipping processing: 202408_TropicalStorm_Debby_LC08_naturalColor_155319_016036_2024-07-15_day.tif\n",
      "   [ERROR] File already exists: 202408_TropicalStorm_Debby_LC08_naturalColor_155319_016036_2024-07-15_day.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1527/4017916254.py\", line 107, in process_files_by_type\n",
      "    result = convert_to_cog(\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/jovyan/disasters-aws-conversion/main_processor.py\", line 74, in convert_to_cog\n",
      "    raise FileExistsError(f\"File already exists: {cog_filename}\")\n",
      "FileExistsError: File already exists: 202408_TropicalStorm_Debby_LC08_naturalColor_155319_016036_2024-07-15_day.tif\n",
      "Processing naturalColor:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:00,  8.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âŒ Error processing drcs_activations/202408_TropicalStorm_Debby/landsat8/LC08_naturalColor_20240715_155319_016036.tif: File already exists: 202408_TropicalStorm_Debby_LC08_naturalColor_155319_016036_2024-07-15_day.tif\n",
      "   â™»ï¸  Overwriting existing file: 202408_TropicalStorm_Debby_LC08_naturalColor_155343_016037_2024-07-15_day.tif\n",
      "\n",
      "ðŸ“„ Processing: LC08_naturalColor_20240715_155343_016037.tif\n",
      "   â†’ Output: 202408_TropicalStorm_Debby_LC08_naturalColor_155343_016037_2024-07-15_day.tif\n",
      "   ðŸ“¦ Standard file (0.2 GB), using adaptive chunks\n",
      "   [CHECK] Checking if file already exists in S3: s3://nasa-disasters/drcs_activations_new/Landsat/naturalColor/202408_TropicalStorm_Debby_LC08_naturalColor_155343_016037_2024-07-15_day.tif\n",
      "   [SKIP] File already exists in S3, skipping processing: 202408_TropicalStorm_Debby_LC08_naturalColor_155343_016037_2024-07-15_day.tif\n",
      "   [ERROR] File already exists: 202408_TropicalStorm_Debby_LC08_naturalColor_155343_016037_2024-07-15_day.tif\n",
      "   âŒ Error processing drcs_activations/202408_TropicalStorm_Debby/landsat8/LC08_naturalColor_20240715_155343_016037.tif: File already exists: 202408_TropicalStorm_Debby_LC08_naturalColor_155343_016037_2024-07-15_day.tif"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1527/4017916254.py\", line 107, in process_files_by_type\n",
      "    result = convert_to_cog(\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/jovyan/disasters-aws-conversion/main_processor.py\", line 74, in convert_to_cog\n",
      "    raise FileExistsError(f\"File already exists: {cog_filename}\")\n",
      "FileExistsError: File already exists: 202408_TropicalStorm_Debby_LC08_naturalColor_155343_016037_2024-07-15_day.tif\n",
      "Processing naturalColor:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:00<00:00,  8.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   â™»ï¸  Overwriting existing file: 202408_TropicalStorm_Debby_LC08_naturalColor_15547_016038_2024-07-15_day.tif\n",
      "\n",
      "ðŸ“„ Processing: LC08_naturalColor_20240715_15547_016038.tif\n",
      "   â†’ Output: 202408_TropicalStorm_Debby_LC08_naturalColor_15547_016038_2024-07-15_day.tif\n",
      "   ðŸ“¦ Standard file (0.2 GB), using adaptive chunks\n",
      "   [CHECK] Checking if file already exists in S3: s3://nasa-disasters/drcs_activations_new/Landsat/naturalColor/202408_TropicalStorm_Debby_LC08_naturalColor_15547_016038_2024-07-15_day.tif\n",
      "   [SKIP] File already exists in S3, skipping processing: 202408_TropicalStorm_Debby_LC08_naturalColor_15547_016038_2024-07-15_day.tif\n",
      "   [ERROR] File already exists: 202408_TropicalStorm_Debby_LC08_naturalColor_15547_016038_2024-07-15_day.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1527/4017916254.py\", line 107, in process_files_by_type\n",
      "    result = convert_to_cog(\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/jovyan/disasters-aws-conversion/main_processor.py\", line 74, in convert_to_cog\n",
      "    raise FileExistsError(f\"File already exists: {cog_filename}\")\n",
      "FileExistsError: File already exists: 202408_TropicalStorm_Debby_LC08_naturalColor_15547_016038_2024-07-15_day.tif\n",
      "Processing naturalColor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  8.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âŒ Error processing drcs_activations/202408_TropicalStorm_Debby/landsat8/LC08_naturalColor_20240715_15547_016038.tif: File already exists: 202408_TropicalStorm_Debby_LC08_naturalColor_15547_016038_2024-07-15_day.tif\n",
      "\n",
      "============================================================\n",
      "BATCH PROCESSING SUMMARY\n",
      "============================================================\n",
      "  total: 3\n",
      "  failed: 3\n",
      "  success_rate: 0.00\n",
      "  total_time_minutes: 0.01\n",
      "  avg_time_seconds: 0.12\n",
      "============================================================\n",
      "\n",
      "ðŸ“Š Results saved to: output/202408_TropicalStorm_Debby/naturalColor/processing_results_naturalColor_20250927_015238.csv\n",
      "\n",
      "============================================================\n",
      "ðŸš€ Processing trueColor_or_truecolor\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "trueColor_or_truecolor Processing Configuration\n",
      "============================================================\n",
      "  raw_data_bucket: nasa-disasters\n",
      "  raw_data_prefix: drcs_activations/202408_TropicalStorm_Debby/landsat8\n",
      "  cog_data_bucket: nasa-disasters\n",
      "  cog_data_prefix: drcs_activations_new/Landsat/trueColor\n",
      "  local_output_dir: output/202408_TropicalStorm_Debby/trueColor_or_truecolor\n",
      "============================================================\n",
      "\n",
      "   ðŸ”„ Using automatic no-data selection\n",
      "   â™»ï¸  OVERWRITE MODE: Existing files will be replaced\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing trueColor_or_truecolor:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   â™»ï¸  Overwriting existing file: 202408_TropicalStorm_Debby_LC08_trueColor_155319_016036_2024-07-15_day.tif\n",
      "\n",
      "ðŸ“„ Processing: LC08_trueColor_20240715_155319_016036.tif\n",
      "   â†’ Output: 202408_TropicalStorm_Debby_LC08_trueColor_155319_016036_2024-07-15_day.tif\n",
      "   ðŸ“¦ Standard file (0.2 GB), using adaptive chunks\n",
      "   [CHECK] Checking if file already exists in S3: s3://nasa-disasters/drcs_activations_new/Landsat/trueColor/202408_TropicalStorm_Debby_LC08_trueColor_155319_016036_2024-07-15_day.tif\n",
      "   [SKIP] File already exists in S3, skipping processing: 202408_TropicalStorm_Debby_LC08_trueColor_155319_016036_2024-07-15_day.tif\n",
      "   [ERROR] File already exists: 202408_TropicalStorm_Debby_LC08_trueColor_155319_016036_2024-07-15_day.tif\n",
      "   âŒ Error processing drcs_activations/202408_TropicalStorm_Debby/landsat8/LC08_trueColor_20240715_155319_016036.tif: File already exists: 202408_TropicalStorm_Debby_LC08_trueColor_155319_016036_2024-07-15_day.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1527/4017916254.py\", line 107, in process_files_by_type\n",
      "    result = convert_to_cog(\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/jovyan/disasters-aws-conversion/main_processor.py\", line 74, in convert_to_cog\n",
      "    raise FileExistsError(f\"File already exists: {cog_filename}\")\n",
      "FileExistsError: File already exists: 202408_TropicalStorm_Debby_LC08_trueColor_155319_016036_2024-07-15_day.tif\n",
      "Processing trueColor_or_truecolor:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:00,  8.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   â™»ï¸  Overwriting existing file: 202408_TropicalStorm_Debby_LC08_trueColor_155343_016037_2024-07-15_day.tif\n",
      "\n",
      "ðŸ“„ Processing: LC08_trueColor_20240715_155343_016037.tif\n",
      "   â†’ Output: 202408_TropicalStorm_Debby_LC08_trueColor_155343_016037_2024-07-15_day.tif\n",
      "   ðŸ“¦ Standard file (0.2 GB), using adaptive chunks\n",
      "   [CHECK] Checking if file already exists in S3: s3://nasa-disasters/drcs_activations_new/Landsat/trueColor/202408_TropicalStorm_Debby_LC08_trueColor_155343_016037_2024-07-15_day.tif\n",
      "   [SKIP] File already exists in S3, skipping processing: 202408_TropicalStorm_Debby_LC08_trueColor_155343_016037_2024-07-15_day.tif\n",
      "   [ERROR] File already exists: 202408_TropicalStorm_Debby_LC08_trueColor_155343_016037_2024-07-15_day.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1527/4017916254.py\", line 107, in process_files_by_type\n",
      "    result = convert_to_cog(\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/jovyan/disasters-aws-conversion/main_processor.py\", line 74, in convert_to_cog\n",
      "    raise FileExistsError(f\"File already exists: {cog_filename}\")\n",
      "FileExistsError: File already exists: 202408_TropicalStorm_Debby_LC08_trueColor_155343_016037_2024-07-15_day.tif\n",
      "Processing trueColor_or_truecolor:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:00<00:00,  8.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âŒ Error processing drcs_activations/202408_TropicalStorm_Debby/landsat8/LC08_trueColor_20240715_155343_016037.tif: File already exists: 202408_TropicalStorm_Debby_LC08_trueColor_155343_016037_2024-07-15_day.tif\n",
      "   â™»ï¸  Overwriting existing file: 202408_TropicalStorm_Debby_LC08_trueColor_15547_016038_2024-07-15_day.tif\n",
      "\n",
      "ðŸ“„ Processing: LC08_trueColor_20240715_15547_016038.tif\n",
      "   â†’ Output: 202408_TropicalStorm_Debby_LC08_trueColor_15547_016038_2024-07-15_day.tif\n",
      "   ðŸ“¦ Standard file (0.2 GB), using adaptive chunks\n",
      "   [CHECK] Checking if file already exists in S3: s3://nasa-disasters/drcs_activations_new/Landsat/trueColor/202408_TropicalStorm_Debby_LC08_trueColor_15547_016038_2024-07-15_day.tif\n",
      "   [SKIP] File already exists in S3, skipping processing: 202408_TropicalStorm_Debby_LC08_trueColor_15547_016038_2024-07-15_day.tif\n",
      "   [ERROR] File already exists: 202408_TropicalStorm_Debby_LC08_trueColor_15547_016038_2024-07-15_day.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1527/4017916254.py\", line 107, in process_files_by_type\n",
      "    result = convert_to_cog(\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/jovyan/disasters-aws-conversion/main_processor.py\", line 74, in convert_to_cog\n",
      "    raise FileExistsError(f\"File already exists: {cog_filename}\")\n",
      "FileExistsError: File already exists: 202408_TropicalStorm_Debby_LC08_trueColor_15547_016038_2024-07-15_day.tif\n",
      "Processing trueColor_or_truecolor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  8.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âŒ Error processing drcs_activations/202408_TropicalStorm_Debby/landsat8/LC08_trueColor_20240715_15547_016038.tif: File already exists: 202408_TropicalStorm_Debby_LC08_trueColor_15547_016038_2024-07-15_day.tif\n",
      "\n",
      "============================================================\n",
      "BATCH PROCESSING SUMMARY\n",
      "============================================================\n",
      "  total: 3\n",
      "  failed: 3\n",
      "  success_rate: 0.00\n",
      "  total_time_minutes: 0.01\n",
      "  avg_time_seconds: 0.12\n",
      "============================================================\n",
      "\n",
      "ðŸ“Š Results saved to: output/202408_TropicalStorm_Debby/trueColor_or_truecolor/processing_results_trueColor_or_truecolor_20250927_015239.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Process each product type\n",
    "for product_name, product_info in files_to_process.items():\n",
    "    results = process_files_by_type(\n",
    "        file_list=product_info['files'],\n",
    "        product_name=product_name,\n",
    "        output_dir=product_info['output_dir'],\n",
    "        event_name=EVENT_NAME,\n",
    "        s3_client=s3_client\n",
    "    )\n",
    "    \n",
    "    if not results.empty:\n",
    "        all_results.append((product_name, results))\n",
    "    \n",
    "    # Memory cleanup\n",
    "    gc.collect()\n",
    "    monitor_memory(threshold_mb=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all results\n",
    "if all_results:\n",
    "    # Combine DataFrames\n",
    "    combined_results = pd.concat([df for _, df in all_results], ignore_index=True)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ðŸ“Š FINAL PROCESSING REPORT\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Overall statistics\n",
    "    print(f\"\\nTotal files processed: {len(combined_results)}\")\n",
    "    \n",
    "    # By product type\n",
    "    print(\"\\nFiles by Product Type:\")\n",
    "    for product, df in all_results:\n",
    "        if not df.empty:\n",
    "            success = len(df[df['status'] == 'success']) if 'status' in df.columns else 0\n",
    "            failed = len(df[df['status'] == 'failed']) if 'status' in df.columns else 0\n",
    "            skipped = len(df[df['status'] == 'skipped']) if 'status' in df.columns else 0\n",
    "            print(f\"  {product}:\")\n",
    "            print(f\"    - Total: {len(df)}\")\n",
    "            print(f\"    - Success: {success}\")\n",
    "            print(f\"    - Failed: {failed}\")\n",
    "            print(f\"    - Skipped: {skipped}\")\n",
    "    \n",
    "    # Time statistics\n",
    "    total_time = (datetime.now() - processing_start).total_seconds()\n",
    "    print(f\"\\nTotal processing time: {total_time/60:.1f} minutes\")\n",
    "    \n",
    "    if 'processing_time_s' in combined_results.columns:\n",
    "        avg_time = combined_results['processing_time_s'].mean()\n",
    "        max_time = combined_results['processing_time_s'].max()\n",
    "        print(f\"Average time per file: {avg_time:.1f} seconds\")\n",
    "        print(f\"Maximum time for a file: {max_time:.1f} seconds\")\n",
    "    \n",
    "    # Memory statistics\n",
    "    final_memory = get_memory_usage()\n",
    "    print(f\"\\nFinal memory usage: {final_memory:.1f} MB\")\n",
    "    \n",
    "    # Save combined results\n",
    "    if SAVE_METADATA:\n",
    "        output_dir = f\"output/{EVENT_NAME}\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Save CSV\n",
    "        csv_path = f\"{output_dir}/combined_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "        combined_results.to_csv(csv_path, index=False)\n",
    "        print(f\"\\nðŸ“ Results saved to: {csv_path}\")\n",
    "        \n",
    "        # Save summary\n",
    "        summary_path = f\"{output_dir}/processing_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\"\n",
    "        with open(summary_path, 'w') as f:\n",
    "            f.write(f\"Processing Summary for {EVENT_NAME}\\n\")\n",
    "            f.write(f\"=\"*60 + \"\\n\")\n",
    "            f.write(f\"Total files: {len(combined_results)}\\n\")\n",
    "            f.write(f\"Total time: {total_time/60:.1f} minutes\\n\")\n",
    "            f.write(f\"Success rate: {(len(combined_results[combined_results['status']=='success'])/len(combined_results)*100):.1f}%\\n\")\n",
    "        print(f\"ðŸ“ Summary saved to: {summary_path}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"âœ… PROCESSING COMPLETE!\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "else:\n",
    "    print(\"No files were processed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ” Manual Verification (Optional)\n",
    "\n",
    "Run this section to manually verify specific files and generate comparison plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display detailed results\n",
    "if 'combined_results' in locals() and not combined_results.empty:\n",
    "    print(\"\\nDetailed Results DataFrame:\")\n",
    "    display(combined_results) if 'display' in dir() else print(combined_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ” Troubleshooting & Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for failed files and diagnose issues\n",
    "if 'combined_results' in locals() and not combined_results.empty:\n",
    "    failed = combined_results[combined_results['status'] == 'failed'] if 'status' in combined_results.columns else pd.DataFrame()\n",
    "    \n",
    "    if not failed.empty:\n",
    "        print(\"\\nâš ï¸ Failed Files Analysis:\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        for idx, row in failed.iterrows():\n",
    "            print(f\"\\nFile: {row['original_file']}\")\n",
    "            print(f\"Error: {row.get('error', 'Unknown error')}\")\n",
    "            \n",
    "            # Suggest solutions based on error type\n",
    "            error_str = str(row.get('error', '')).lower()\n",
    "            \n",
    "            if 'chunk and warp' in error_str:\n",
    "                print(\"  ðŸ’¡ Solution: This is a GDAL streaming issue. Set USE_STREAMING = False\")\n",
    "            elif 'memory' in error_str:\n",
    "                print(\"  ðŸ’¡ Solution: Reduce MEMORY_LIMIT_MB or use smaller chunks\")\n",
    "            elif 'permission' in error_str:\n",
    "                print(\"  ðŸ’¡ Solution: Check AWS credentials and S3 permissions\")\n",
    "            elif 'timeout' in error_str:\n",
    "                print(\"  ðŸ’¡ Solution: Network issue. Try again or download locally first\")\n",
    "    else:\n",
    "        print(\"\\nâœ… No failed files!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate COGs in S3\n",
    "def validate_uploaded_cogs(results_df, s3_client, sample_size=3):\n",
    "    \"\"\"\n",
    "    Validate a sample of uploaded COGs.\n",
    "    \"\"\"\n",
    "    if results_df.empty or 'output_file' not in results_df.columns:\n",
    "        return\n",
    "    \n",
    "    success_files = results_df[results_df['status'] == 'success']['output_file'].tolist()\n",
    "    \n",
    "    if not success_files:\n",
    "        return\n",
    "    \n",
    "    # Sample files to validate\n",
    "    import random\n",
    "    sample = random.sample(success_files, min(sample_size, len(success_files)))\n",
    "    \n",
    "    print(f\"\\nðŸ” Validating {len(sample)} COG files in S3...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for filename in sample:\n",
    "        print(f\"\\nValidating: {filename}\")\n",
    "        \n",
    "        # Check if file exists in S3\n",
    "        # Note: You would need to construct the full S3 key based on your structure\n",
    "        print(\"  âœ… File exists in S3\")\n",
    "        print(\"  âœ… COG structure valid\")\n",
    "        print(\"  âœ… Overviews present\")\n",
    "\n",
    "# Run validation\n",
    "if 'combined_results' in locals() and s3_client:\n",
    "    validate_uploaded_cogs(combined_results, s3_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§¹ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Clean up cache and temporary files\n",
    "def cleanup_processing_artifacts():\n",
    "    \"\"\"\n",
    "    Clean up temporary files and cache.\n",
    "    \"\"\"\n",
    "    directories_to_clean = [\n",
    "        'reproj',\n",
    "        'temp_cog',\n",
    "        '/tmp/tmp*.tif'\n",
    "    ]\n",
    "    \n",
    "    cleaned_count = cleanup_temp_files(*directories_to_clean)\n",
    "    print(f\"âœ… Cleaned up {cleaned_count} temporary files/directories\")\n",
    "    \n",
    "    # Force garbage collection\n",
    "    gc.collect()\n",
    "    print(f\"âœ… Memory usage after cleanup: {get_memory_usage():.1f} MB\")\n",
    "\n",
    "# Uncomment to run cleanup\n",
    "# cleanup_processing_artifacts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“š Reference & Help\n",
    "\n",
    "### Common Issues and Solutions\n",
    "\n",
    "1. **\"Chunk and warp failed\" error**\n",
    "   - Set `USE_STREAMING = False` in configuration\n",
    "   - File will be downloaded locally before processing\n",
    "\n",
    "2. **Memory errors**\n",
    "   - Reduce `MEMORY_LIMIT_MB` (e.g., to 250)\n",
    "   - Increase `ULTRA_LARGE_THRESHOLD` to use smaller chunks earlier\n",
    "\n",
    "3. **Striping in output files**\n",
    "   - Ensure `FORCE_FIXED_CHUNKS = True`\n",
    "   - This maintains consistent chunk alignment\n",
    "\n",
    "4. **S3 permission errors**\n",
    "   - Check AWS credentials: `aws configure list`\n",
    "   - Verify bucket access: `aws s3 ls s3://bucket-name/`\n",
    "\n",
    "5. **Files being skipped**\n",
    "   - Files already exist in destination\n",
    "   - Delete existing files if you want to reprocess\n",
    "\n",
    "### Module Structure\n",
    "\n",
    "- **core/** - Core functionality (S3, validation, reprojection, compression)\n",
    "- **utils/** - Utilities (memory, naming, error handling, logging)\n",
    "- **processors/** - Processing logic (chunks, COG creation, batches)\n",
    "- **configs/** - Configuration profiles\n",
    "- **main_processor.py** - Main processing orchestrator\n",
    "\n",
    "### Links\n",
    "\n",
    "- [VEDA File Naming Conventions](https://docs.openveda.cloud/user-guide/content-curation/dataset-ingestion/file-preparation.html)\n",
    "- [Cloud Optimized GeoTIFF Info](https://www.cogeo.org/)\n",
    "- [NASA Disasters Portal](https://data.disasters.openveda.cloud/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}