{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Files from CSV Mapping\n",
    "\n",
    "This notebook loads a filename mapping CSV (created by `renaming_file_template.ipynb`) and processes files to Cloud Optimized GeoTIFFs (COGs) with the new filenames.\n",
    "\n",
    "## Features\n",
    "- **Load CSV mapping** - Import pre-defined filename transformations\n",
    "- **Preview before processing** - Review what will be processed\n",
    "- **Batch COG conversion** - Convert all files to COGs\n",
    "- **Track results** - Save processing results to CSV\n",
    "\n",
    "## Workflow\n",
    "1. Generate mapping CSV using `renaming_file_template.ipynb`\n",
    "2. Review and validate the CSV\n",
    "3. Run this notebook to process files\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Step 1: Basic Configuration\n",
    "\n",
    "Set your event name to load the corresponding CSV mapping file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# INPUTS\n",
    "# ========================================\n",
    "\n",
    "# S3 Configuration\n",
    "BUCKET = 'nasa-disasters'    # S3 bucket (DO NOT CHANGE)\n",
    "\n",
    "# Event Details\n",
    "EVENT_NAME = '202510_Flood_AK'  # Must match the event name used in renaming_file_template.ipynb\n",
    "SUB_PRODUCT_NAME = 'sentinel2'\n",
    "\n",
    "# CSV Mapping Configuration\n",
    "CSV_DIR = f'file-mapping/{EVENT_NAME}'  # Directory where CSV mappings are stored\n",
    "CSV_FILENAME = f'{EVENT_NAME}-{SUB_PRODUCT_NAME}.csv'  # CSV filename (usually {EVENT_NAME}-{subproductName}.csv)\n",
    "\n",
    "# Processing Options\n",
    "CHECK_SOURCE_IS_COG = True  # Check if source files are already valid COGs\n",
    "SKIP_IF_SOURCE_IS_COG = True  # Skip processing if source is already a valid COG\n",
    "OVERWRITE = False      # Set to True to replace existing files in S3 (after converting to COG)\n",
    "VERIFY = True          # Set to True to verify COGs after creation\n",
    "SAVE_RESULTS = True    # Save processing results to CSV\n",
    "\n",
    "# Output\n",
    "OUTPUT_DIR = 'csv_stats'  # Directory for results CSV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "# Import all required libraries\nimport pandas as pd\nimport time\nfrom pathlib import Path\nfrom datetime import datetime\nimport sys\nimport os\n\n# Add parent directory to path for importing custom modules\nparent_dir = Path('..').resolve()\nif str(parent_dir) not in sys.path:\n    sys.path.insert(0, str(parent_dir))\n\n# Import custom modules\nfrom lib.core.s3_operations import initialize_s3_client, check_s3_file_exists\nfrom lib.core.cog_processing import process_single_file\n\nprint(\"‚úÖ All libraries imported successfully\")",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÇ Step 2: Load CSV Mapping\n",
    "\n",
    "Load the filename mapping CSV created by the renaming template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Construct CSV path\ncsv_path = Path(CSV_DIR) / CSV_FILENAME\n\nprint(\"üìÇ LOADING CSV MAPPING\")\nprint(\"=\"*80)\nprint(f\"\\nLooking for: {csv_path}\")\n\n# Check if CSV exists\nif not csv_path.exists():\n    print(f\"\\n‚ùå ERROR: CSV file not found!\")\n    print(f\"\\nMake sure you've run 'renaming_file_template.ipynb' first to create the mapping CSV.\")\n    print(f\"Expected location: {csv_path.absolute()}\")\n    mapping_df = pd.DataFrame()\nelse:\n    # Load CSV\n    mapping_df = pd.read_csv(csv_path)\n    \n    print(f\"\\n‚úÖ Successfully loaded CSV\")\n    print(f\"\\nüìä Mapping details:\")\n    print(f\"   Total entries: {len(mapping_df)}\")\n    \n    if 'status' in mapping_df.columns:\n        print(f\"   Valid files:   {len(mapping_df[mapping_df['status'] == 'valid'])}\")\n        invalid_count = len(mapping_df[mapping_df['status'] != 'valid'])\n        if invalid_count > 0:\n            print(f\"   Invalid files: {invalid_count}\")\n    \n    if 'file_size_gb' in mapping_df.columns:\n        print(f\"   Total size:    {mapping_df['file_size_gb'].sum():.2f} GB\")\n    \n    if 'category' in mapping_df.columns:\n        print(f\"\\nüìÇ By category:\")\n        category_counts = mapping_df['category'].value_counts()\n        for category, count in category_counts.items():\n            print(f\"   ‚Ä¢ {category}: {count} files\")\n    \n    if 'nodata_value' in mapping_df.columns:\n        print(f\"\\nüî¢ Nodata values:\")\n        nodata_counts = mapping_df['nodata_value'].value_counts()\n        for nodata, count in nodata_counts.items():\n            print(f\"   ‚Ä¢ {nodata}: {count} files\")\n    \n    print(f\"\\n‚úÖ Ready to filter and process files\")\n    print(\"\\n\" + \"=\"*80)\n    \n    # Display first few rows\n    print(\"\\nüìã Preview (first 5 rows):\")\n    display(mapping_df.head())"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Step 3: Filter Files to Process\n",
    "\n",
    "Optionally filter which files to process (by category, size, etc.):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILES TO PROCESS\n",
      "================================================================================\n",
      "\n",
      "Total files: 123\n",
      "Total size:  31.09 GB\n",
      "\n",
      "By category:\n",
      "   ‚Ä¢ colorInfrared: 41 files\n",
      "   ‚Ä¢ shortwaveIR: 41 files\n",
      "   ‚Ä¢ trueColor: 41 files\n",
      "\n",
      "‚úÖ Ready to process 123 files\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "if not mapping_df.empty:\n",
    "    # Filter out uncategorized files (they won't be processed)\n",
    "    files_to_process = mapping_df[mapping_df['status'] == 'valid'].copy()\n",
    "    \n",
    "    # Optional: Filter by category\n",
    "    # Uncomment and modify to process only specific categories:\n",
    "    # CATEGORIES_TO_PROCESS = ['trueColor', 'colorInfrared']\n",
    "    # files_to_process = files_to_process[files_to_process['category'].isin(CATEGORIES_TO_PROCESS)]\n",
    "    \n",
    "    # Optional: Filter by file size\n",
    "    # Uncomment to process only files smaller than a certain size:\n",
    "    # MAX_SIZE_GB = 5.0\n",
    "    # files_to_process = files_to_process[files_to_process['file_size_gb'] <= MAX_SIZE_GB]\n",
    "    \n",
    "    print(\"FILES TO PROCESS\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nTotal files: {len(files_to_process)}\")\n",
    "    print(f\"Total size:  {files_to_process['file_size_gb'].sum():.2f} GB\")\n",
    "    \n",
    "    if len(files_to_process) > 0:\n",
    "        print(f\"\\nBy category:\")\n",
    "        category_counts = files_to_process['category'].value_counts()\n",
    "        for category, count in category_counts.items():\n",
    "            print(f\"   ‚Ä¢ {category}: {count} files\")\n",
    "        \n",
    "        print(f\"\\n‚úÖ Ready to process {len(files_to_process)} files\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è  No files match the filter criteria\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No mapping data loaded. Check Step 2.\")\n",
    "    files_to_process = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üåê Step 4: Connect to S3\n",
    "\n",
    "Initialize S3 client for downloading source files and uploading processed COGs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"üåê Connecting to S3...\")\ns3_client, fs = initialize_s3_client(bucket_name=BUCKET, verbose=True)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Step 5: Process Files to COGs\n",
    "\n",
    "Convert files to Cloud Optimized GeoTIFFs with the new filenames:"
   ]
  },
  {
   "cell_type": "code",
   "source": "if not files_to_process.empty and s3_client:\n    print(\"üöÄ STARTING COG PROCESSING\")\n    print(\"=\"*80)\n    print(f\"\\nProcessing {len(files_to_process)} files...\")\n    print(\"This may take several minutes depending on file sizes.\\n\")\n    \n    # Display processing options\n    print(f\"Processing options:\")\n    print(f\"  Check source is COG: {CHECK_SOURCE_IS_COG}\")\n    print(f\"  Skip if source is COG: {SKIP_IF_SOURCE_IS_COG}\")\n    print(f\"  Overwrite existing: {OVERWRITE}\")\n    print(f\"  Verify COGs: {VERIFY}\\n\")\n    \n    # Track results\n    results = []\n    \n    for idx, row in files_to_process.iterrows():\n        start_time = time.time()\n        \n        source_path = row['original_s3_path']\n        dest_path = row['output_s3_path']\n        category = row['category']\n        \n        # Get nodata value from CSV (if available)\n        nodata = row.get('nodata_value', None)\n        if nodata is not None and pd.isna(nodata):\n            nodata = None  # Handle NaN values\n        \n        print(f\"\\n[{idx+1}/{len(files_to_process)}] Processing: {row['original_filename']}\")\n        print(f\"    Category: {category}\")\n        print(f\"    Size: {row['file_size_gb']:.2f} GB\")\n        print(f\"    Nodata: {nodata}\")\n        print(f\"    Output: {row['new_filename']}\")\n        \n        # Check if destination file already exists (unless OVERWRITE is True)\n        if not OVERWRITE:\n            if check_s3_file_exists(s3_client, BUCKET, dest_path):\n                print(f\"    ‚è≠Ô∏è  SKIPPED (already exists in destination)\")\n                results.append({\n                    'source_file': row['original_filename'],\n                    'output_file': row['new_filename'],\n                    'category': category,\n                    'status': 'skipped',\n                    'time_seconds': 0,\n                    'error': 'File already exists'\n                })\n                continue\n        \n        try:\n            # Process file to COG\n            # This function will:\n            # 1. Check if source is already a COG (if CHECK_SOURCE_IS_COG=True)\n            # 2. Skip processing and copy if source is COG (if SKIP_IF_SOURCE_IS_COG=True)\n            # 3. Download, convert to COG with nodata value, verify, and upload\n            success = process_single_file(\n                s3_client=s3_client,\n                bucket=BUCKET,\n                source_key=source_path,\n                dest_key=dest_path,\n                nodata=nodata,  # Use nodata from CSV\n                verify=VERIFY,\n                check_source_is_cog=CHECK_SOURCE_IS_COG,\n                skip_if_source_is_cog=SKIP_IF_SOURCE_IS_COG,\n                verbose=True\n            )\n            \n            elapsed = time.time() - start_time\n            \n            if success:\n                print(f\"    ‚úÖ SUCCESS ({elapsed:.1f}s)\")\n                results.append({\n                    'source_file': row['original_filename'],\n                    'output_file': row['new_filename'],\n                    'category': category,\n                    'status': 'success',\n                    'time_seconds': elapsed,\n                    'error': None\n                })\n            else:\n                print(f\"    ‚ùå FAILED ({elapsed:.1f}s)\")\n                results.append({\n                    'source_file': row['original_filename'],\n                    'output_file': row['new_filename'],\n                    'category': category,\n                    'status': 'failed',\n                    'time_seconds': elapsed,\n                    'error': 'Processing failed'\n                })\n        \n        except Exception as e:\n            elapsed = time.time() - start_time\n            print(f\"    ‚ùå ERROR: {str(e)}\")\n            results.append({\n                'source_file': row['original_filename'],\n                'output_file': row['new_filename'],\n                'category': category,\n                'status': 'failed',\n                'time_seconds': elapsed,\n                'error': str(e)\n            })\n    \n    # Create results DataFrame\n    results_df = pd.DataFrame(results)\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"\\nüéâ PROCESSING COMPLETE!\")\n    \nelse:\n    print(\"‚ö†Ô∏è Cannot process: No files to process or S3 not connected\")\n    results_df = pd.DataFrame()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ Step 7: Save Results to CSV\n",
    "\n",
    "Save processing results for future reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not results_df.empty and SAVE_RESULTS:\n",
    "    # Create output directory\n",
    "    output_path = Path(OUTPUT_DIR) / EVENT_NAME\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Generate filename with timestamp\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    results_filename = f\"processing_results_{timestamp}.csv\"\n",
    "    results_path = output_path / results_filename\n",
    "    \n",
    "    # Save results\n",
    "    results_df.to_csv(results_path, index=False)\n",
    "    \n",
    "    print(\"üíæ RESULTS SAVED\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\n‚úÖ Saved results to: {results_path.absolute()}\")\n",
    "    print(f\"\\nüìä Summary:\")\n",
    "    print(f\"   Records saved: {len(results_df)}\")\n",
    "    print(f\"   Successful:    {len(results_df[results_df['status'] == 'success'])}\")\n",
    "    print(f\"   Failed:        {len(results_df[results_df['status'] == 'failed'])}\")\n",
    "    print(f\"   Skipped:       {len(results_df[results_df['status'] == 'skipped'])}\")\n",
    "    \n",
    "elif results_df.empty:\n",
    "    print(\"‚ö†Ô∏è No results to save. Check Step 5.\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è  Results saving disabled (SAVE_RESULTS = False)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí° Next Steps\n",
    "\n",
    "After processing:\n",
    "\n",
    "1. **Review results** - Check the statistics and any failed files\n",
    "2. **Verify outputs** - Spot-check processed COGs in S3\n",
    "3. **Handle failures** - Investigate and retry any failed files\n",
    "4. **Update metadata** - Add metadata to processed files if needed\n",
    "\n",
    "## üîß Troubleshooting\n",
    "\n",
    "### Common Issues:\n",
    "\n",
    "1. **\"CSV file not found\"**\n",
    "   - Run `renaming_file_template.ipynb` first\n",
    "   - Check `EVENT_NAME` matches the CSV filename\n",
    "   - Verify `CSV_DIR` path is correct\n",
    "\n",
    "2. **\"Connection failed\"**\n",
    "   - Check AWS credentials\n",
    "   - Verify external ID is configured (if using upload role)\n",
    "   - Test with `test_upload.py`\n",
    "\n",
    "3. **\"All files skipped\"**\n",
    "   - Files already exist in destination\n",
    "   - Set `OVERWRITE = True` to replace existing files\n",
    "\n",
    "4. **\"Processing failures\"**\n",
    "   - Check source files are valid GeoTIFFs\n",
    "   - Verify enough disk space for temp files\n",
    "   - Check S3 write permissions\n",
    "   - Review error messages in results\n",
    "\n",
    "5. **\"Slow processing\"**\n",
    "   - Large files take longer to process\n",
    "   - Consider processing in smaller batches\n",
    "   - Use filters in Step 3 to process by size/category\n",
    "\n",
    "## üìù Notes\n",
    "\n",
    "- Processing time varies based on file size (typically 30s-5min per file)\n",
    "- COGs are created with zstd compression, level 9\n",
    "- Nodata values are auto-detected unless specified\n",
    "- All COGs include 5 overview levels for better performance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}