{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Files from CSV Mapping\n",
    "\n",
    "This notebook loads a filename mapping CSV (created by `renaming_file_template.ipynb`) and processes files to Cloud Optimized GeoTIFFs (COGs) with the new filenames.\n",
    "\n",
    "## Features\n",
    "- **Load CSV mapping** - Import pre-defined filename transformations\n",
    "- **Preview before processing** - Review what will be processed\n",
    "- **Batch COG conversion** - Convert all files to COGs\n",
    "- **Track results** - Save processing results to CSV\n",
    "\n",
    "## Workflow\n",
    "1. Generate mapping CSV using `renaming_file_template.ipynb`\n",
    "2. Review and validate the CSV\n",
    "3. Run this notebook to process files\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Step 1: Basic Configuration\n",
    "\n",
    "Set your event name to load the corresponding CSV mapping file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# INPUTS\n",
    "# ========================================\n",
    "\n",
    "# S3 Configuration\n",
    "BUCKET = 'nasa-disasters'    # S3 bucket (DO NOT CHANGE)\n",
    "\n",
    "# Event Details\n",
    "EVENT_NAME = '202510_Flood_AK'  # Must match the event name used in renaming_file_template.ipynb\n",
    "SUB_PRODUCT_NAME = 'sentinel2'\n",
    "\n",
    "# CSV Mapping Configuration\n",
    "CSV_DIR = f'file-mapping/{EVENT_NAME}'  # Directory where CSV mappings are stored\n",
    "CSV_FILENAME = f'{EVENT_NAME}-{SUB_PRODUCT_NAME}.csv'  # CSV filename (usually {EVENT_NAME}-{subproductName}.csv)\n",
    "\n",
    "# Processing Options\n",
    "CHECK_SOURCE_IS_COG = True  # Check if source files are already valid COGs\n",
    "SKIP_IF_SOURCE_IS_COG = True  # Skip processing if source is already a valid COG\n",
    "OVERWRITE = False      # Set to True to replace existing files in S3 (after converting to COG)\n",
    "VERIFY = True          # Set to True to verify COGs after creation\n",
    "SAVE_RESULTS = True    # Save processing results to CSV\n",
    "\n",
    "# Output\n",
    "OUTPUT_DIR = 'csv_stats'  # Directory for results CSV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÇ Step 2: Load CSV Mapping\n",
    "\n",
    "Load the filename mapping CSV created by the renaming template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ LOADING CSV MAPPING\n",
      "================================================================================\n",
      "\n",
      "Looking for: file-mapping/202510_Flood_AK/202510_Flood_AK-sentinel2.csv\n",
      "\n",
      "Successfully loaded CSV\n",
      "\n",
      "Mapping details:\n",
      "   Total entries: 123\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_filename</th>\n",
       "      <th>new_filename</th>\n",
       "      <th>category</th>\n",
       "      <th>file_size_gb</th>\n",
       "      <th>nodata_value</th>\n",
       "      <th>status</th>\n",
       "      <th>original_s3_path</th>\n",
       "      <th>output_s3_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>S2B_MSIL2A_colorInfrared_20250913_222529_T03VV...</td>\n",
       "      <td>202510_Flood_AK_S2B_MSIL2A_colorInfrared_22252...</td>\n",
       "      <td>colorInfrared</td>\n",
       "      <td>0.336965</td>\n",
       "      <td>0</td>\n",
       "      <td>valid</td>\n",
       "      <td>drcs_activations/202510_Flood_AK/sentinel2/S2B...</td>\n",
       "      <td>drcs_activations_new/Sentinel-2/colorIR/202510...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>S2B_MSIL2A_colorInfrared_20250913_222529_T03VV...</td>\n",
       "      <td>202510_Flood_AK_S2B_MSIL2A_colorInfrared_22252...</td>\n",
       "      <td>colorInfrared</td>\n",
       "      <td>0.336965</td>\n",
       "      <td>0</td>\n",
       "      <td>valid</td>\n",
       "      <td>drcs_activations/202510_Flood_AK/sentinel2/S2B...</td>\n",
       "      <td>drcs_activations_new/Sentinel-2/colorIR/202510...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>S2B_MSIL2A_colorInfrared_20250913_222529_T03VV...</td>\n",
       "      <td>202510_Flood_AK_S2B_MSIL2A_colorInfrared_22252...</td>\n",
       "      <td>colorInfrared</td>\n",
       "      <td>0.336965</td>\n",
       "      <td>0</td>\n",
       "      <td>valid</td>\n",
       "      <td>drcs_activations/202510_Flood_AK/sentinel2/S2B...</td>\n",
       "      <td>drcs_activations_new/Sentinel-2/colorIR/202510...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>S2B_MSIL2A_colorInfrared_20250913_222529_T03VW...</td>\n",
       "      <td>202510_Flood_AK_S2B_MSIL2A_colorInfrared_22252...</td>\n",
       "      <td>colorInfrared</td>\n",
       "      <td>0.336965</td>\n",
       "      <td>0</td>\n",
       "      <td>valid</td>\n",
       "      <td>drcs_activations/202510_Flood_AK/sentinel2/S2B...</td>\n",
       "      <td>drcs_activations_new/Sentinel-2/colorIR/202510...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>S2B_MSIL2A_colorInfrared_20250913_222529_T03VW...</td>\n",
       "      <td>202510_Flood_AK_S2B_MSIL2A_colorInfrared_22252...</td>\n",
       "      <td>colorInfrared</td>\n",
       "      <td>0.336965</td>\n",
       "      <td>0</td>\n",
       "      <td>valid</td>\n",
       "      <td>drcs_activations/202510_Flood_AK/sentinel2/S2B...</td>\n",
       "      <td>drcs_activations_new/Sentinel-2/colorIR/202510...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>S2C_MSIL2A_trueColor_20251021_223601_T03VXJ.tif</td>\n",
       "      <td>202510_Flood_AK_S2C_MSIL2A_trueColor_223601_T0...</td>\n",
       "      <td>trueColor</td>\n",
       "      <td>0.336965</td>\n",
       "      <td>0</td>\n",
       "      <td>valid</td>\n",
       "      <td>drcs_activations/202510_Flood_AK/sentinel2/S2C...</td>\n",
       "      <td>drcs_activations_new/Sentinel-2/trueColor/2025...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>S2C_MSIL2A_trueColor_20251021_223601_T03VXK.tif</td>\n",
       "      <td>202510_Flood_AK_S2C_MSIL2A_trueColor_223601_T0...</td>\n",
       "      <td>trueColor</td>\n",
       "      <td>0.336965</td>\n",
       "      <td>0</td>\n",
       "      <td>valid</td>\n",
       "      <td>drcs_activations/202510_Flood_AK/sentinel2/S2C...</td>\n",
       "      <td>drcs_activations_new/Sentinel-2/trueColor/2025...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>S2C_MSIL2A_trueColor_20251021_223601_T03VXL.tif</td>\n",
       "      <td>202510_Flood_AK_S2C_MSIL2A_trueColor_223601_T0...</td>\n",
       "      <td>trueColor</td>\n",
       "      <td>0.336965</td>\n",
       "      <td>0</td>\n",
       "      <td>valid</td>\n",
       "      <td>drcs_activations/202510_Flood_AK/sentinel2/S2C...</td>\n",
       "      <td>drcs_activations_new/Sentinel-2/trueColor/2025...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>S2C_MSIL2A_trueColor_20251021_223601_T03WWQ.tif</td>\n",
       "      <td>202510_Flood_AK_S2C_MSIL2A_trueColor_223601_T0...</td>\n",
       "      <td>trueColor</td>\n",
       "      <td>0.336965</td>\n",
       "      <td>0</td>\n",
       "      <td>valid</td>\n",
       "      <td>drcs_activations/202510_Flood_AK/sentinel2/S2C...</td>\n",
       "      <td>drcs_activations_new/Sentinel-2/trueColor/2025...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>S2C_MSIL2A_trueColor_20251021_223601_T03WXQ.tif</td>\n",
       "      <td>202510_Flood_AK_S2C_MSIL2A_trueColor_223601_T0...</td>\n",
       "      <td>trueColor</td>\n",
       "      <td>0.336965</td>\n",
       "      <td>0</td>\n",
       "      <td>valid</td>\n",
       "      <td>drcs_activations/202510_Flood_AK/sentinel2/S2C...</td>\n",
       "      <td>drcs_activations_new/Sentinel-2/trueColor/2025...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>123 rows √ó 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     original_filename  \\\n",
       "0    S2B_MSIL2A_colorInfrared_20250913_222529_T03VV...   \n",
       "1    S2B_MSIL2A_colorInfrared_20250913_222529_T03VV...   \n",
       "2    S2B_MSIL2A_colorInfrared_20250913_222529_T03VV...   \n",
       "3    S2B_MSIL2A_colorInfrared_20250913_222529_T03VW...   \n",
       "4    S2B_MSIL2A_colorInfrared_20250913_222529_T03VW...   \n",
       "..                                                 ...   \n",
       "118    S2C_MSIL2A_trueColor_20251021_223601_T03VXJ.tif   \n",
       "119    S2C_MSIL2A_trueColor_20251021_223601_T03VXK.tif   \n",
       "120    S2C_MSIL2A_trueColor_20251021_223601_T03VXL.tif   \n",
       "121    S2C_MSIL2A_trueColor_20251021_223601_T03WWQ.tif   \n",
       "122    S2C_MSIL2A_trueColor_20251021_223601_T03WXQ.tif   \n",
       "\n",
       "                                          new_filename       category  \\\n",
       "0    202510_Flood_AK_S2B_MSIL2A_colorInfrared_22252...  colorInfrared   \n",
       "1    202510_Flood_AK_S2B_MSIL2A_colorInfrared_22252...  colorInfrared   \n",
       "2    202510_Flood_AK_S2B_MSIL2A_colorInfrared_22252...  colorInfrared   \n",
       "3    202510_Flood_AK_S2B_MSIL2A_colorInfrared_22252...  colorInfrared   \n",
       "4    202510_Flood_AK_S2B_MSIL2A_colorInfrared_22252...  colorInfrared   \n",
       "..                                                 ...            ...   \n",
       "118  202510_Flood_AK_S2C_MSIL2A_trueColor_223601_T0...      trueColor   \n",
       "119  202510_Flood_AK_S2C_MSIL2A_trueColor_223601_T0...      trueColor   \n",
       "120  202510_Flood_AK_S2C_MSIL2A_trueColor_223601_T0...      trueColor   \n",
       "121  202510_Flood_AK_S2C_MSIL2A_trueColor_223601_T0...      trueColor   \n",
       "122  202510_Flood_AK_S2C_MSIL2A_trueColor_223601_T0...      trueColor   \n",
       "\n",
       "     file_size_gb  nodata_value status  \\\n",
       "0        0.336965             0  valid   \n",
       "1        0.336965             0  valid   \n",
       "2        0.336965             0  valid   \n",
       "3        0.336965             0  valid   \n",
       "4        0.336965             0  valid   \n",
       "..            ...           ...    ...   \n",
       "118      0.336965             0  valid   \n",
       "119      0.336965             0  valid   \n",
       "120      0.336965             0  valid   \n",
       "121      0.336965             0  valid   \n",
       "122      0.336965             0  valid   \n",
       "\n",
       "                                      original_s3_path  \\\n",
       "0    drcs_activations/202510_Flood_AK/sentinel2/S2B...   \n",
       "1    drcs_activations/202510_Flood_AK/sentinel2/S2B...   \n",
       "2    drcs_activations/202510_Flood_AK/sentinel2/S2B...   \n",
       "3    drcs_activations/202510_Flood_AK/sentinel2/S2B...   \n",
       "4    drcs_activations/202510_Flood_AK/sentinel2/S2B...   \n",
       "..                                                 ...   \n",
       "118  drcs_activations/202510_Flood_AK/sentinel2/S2C...   \n",
       "119  drcs_activations/202510_Flood_AK/sentinel2/S2C...   \n",
       "120  drcs_activations/202510_Flood_AK/sentinel2/S2C...   \n",
       "121  drcs_activations/202510_Flood_AK/sentinel2/S2C...   \n",
       "122  drcs_activations/202510_Flood_AK/sentinel2/S2C...   \n",
       "\n",
       "                                        output_s3_path  \n",
       "0    drcs_activations_new/Sentinel-2/colorIR/202510...  \n",
       "1    drcs_activations_new/Sentinel-2/colorIR/202510...  \n",
       "2    drcs_activations_new/Sentinel-2/colorIR/202510...  \n",
       "3    drcs_activations_new/Sentinel-2/colorIR/202510...  \n",
       "4    drcs_activations_new/Sentinel-2/colorIR/202510...  \n",
       "..                                                 ...  \n",
       "118  drcs_activations_new/Sentinel-2/trueColor/2025...  \n",
       "119  drcs_activations_new/Sentinel-2/trueColor/2025...  \n",
       "120  drcs_activations_new/Sentinel-2/trueColor/2025...  \n",
       "121  drcs_activations_new/Sentinel-2/trueColor/2025...  \n",
       "122  drcs_activations_new/Sentinel-2/trueColor/2025...  \n",
       "\n",
       "[123 rows x 8 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add parent directory to path for importing functions\n",
    "sys.path.insert(0, str(Path('..').resolve()))\n",
    "\n",
    "# Construct CSV path\n",
    "csv_path = Path(CSV_DIR) / CSV_FILENAME\n",
    "\n",
    "print(\"üìÇ LOADING CSV MAPPING\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nLooking for: {csv_path}\")\n",
    "\n",
    "# Check if CSV exists\n",
    "if not csv_path.exists():\n",
    "    print(f\"\\nERROR: CSV file not found!\")\n",
    "    print(f\"Expected location: {csv_path.absolute()}\")\n",
    "    mapping_df = pd.DataFrame()\n",
    "else:\n",
    "    # Load CSV\n",
    "    mapping_df = pd.read_csv(csv_path)\n",
    "    \n",
    "    print(f\"\\nSuccessfully loaded CSV\")\n",
    "    print(f\"\\nMapping details:\")\n",
    "    print(f\"   Total entries: {len(mapping_df)}\")\n",
    "\n",
    "    # Display first few rows\n",
    "    print(display(mapping_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Step 3: Filter Files to Process\n",
    "\n",
    "Optionally filter which files to process (by category, size, etc.):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILES TO PROCESS\n",
      "================================================================================\n",
      "\n",
      "Total files: 123\n",
      "Total size:  31.09 GB\n",
      "\n",
      "By category:\n",
      "   ‚Ä¢ colorInfrared: 41 files\n",
      "   ‚Ä¢ shortwaveIR: 41 files\n",
      "   ‚Ä¢ trueColor: 41 files\n",
      "\n",
      "‚úÖ Ready to process 123 files\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "if not mapping_df.empty:\n",
    "    # Filter out uncategorized files (they won't be processed)\n",
    "    files_to_process = mapping_df[mapping_df['status'] == 'valid'].copy()\n",
    "    \n",
    "    # Optional: Filter by category\n",
    "    # Uncomment and modify to process only specific categories:\n",
    "    # CATEGORIES_TO_PROCESS = ['trueColor', 'colorInfrared']\n",
    "    # files_to_process = files_to_process[files_to_process['category'].isin(CATEGORIES_TO_PROCESS)]\n",
    "    \n",
    "    # Optional: Filter by file size\n",
    "    # Uncomment to process only files smaller than a certain size:\n",
    "    # MAX_SIZE_GB = 5.0\n",
    "    # files_to_process = files_to_process[files_to_process['file_size_gb'] <= MAX_SIZE_GB]\n",
    "    \n",
    "    print(\"FILES TO PROCESS\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nTotal files: {len(files_to_process)}\")\n",
    "    print(f\"Total size:  {files_to_process['file_size_gb'].sum():.2f} GB\")\n",
    "    \n",
    "    if len(files_to_process) > 0:\n",
    "        print(f\"\\nBy category:\")\n",
    "        category_counts = files_to_process['category'].value_counts()\n",
    "        for category, count in category_counts.items():\n",
    "            print(f\"   ‚Ä¢ {category}: {count} files\")\n",
    "        \n",
    "        print(f\"\\n‚úÖ Ready to process {len(files_to_process)} files\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è  No files match the filter criteria\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No mapping data loaded. Check Step 2.\")\n",
    "    files_to_process = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üåê Step 4: Connect to S3\n",
    "\n",
    "Initialize S3 client for downloading source files and uploading processed COGs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåê Connecting to S3...\n",
      "üîë Attempting to authenticate with external ID for upload permissions...\n",
      "‚úÖ S3 client initialized with UPLOAD permissions via external ID\n",
      "‚úÖ Confirmed access to nasa-disasters bucket\n",
      "‚úÖ S3 filesystem (fsspec) initialized\n"
     ]
    }
   ],
   "source": [
    "from core.s3_operations import initialize_s3_client\n",
    "\n",
    "print(\"üåê Connecting to S3...\")\n",
    "s3_client, fs = initialize_s3_client(bucket_name=BUCKET, verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Step 5: Process Files to COGs\n",
    "\n",
    "Convert files to Cloud Optimized GeoTIFFs with the new filenames:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Step 6: View Results\n",
    "\n",
    "Display processing statistics and results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'core.cog_processing'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m files_to_process.empty \u001b[38;5;129;01mand\u001b[39;00m s3_client:\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcog_processing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m process_single_file\n\u001b[32m      3\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01ms3_operations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m check_s3_file_exists\n\u001b[32m      4\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtime\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'core.cog_processing'"
     ]
    }
   ],
   "source": [
    "if not files_to_process.empty and s3_client:\n",
    "    from core.cog_processing import process_single_file\n",
    "    from core.s3_operations import check_s3_file_exists\n",
    "    import time\n",
    "    \n",
    "    print(\"üöÄ STARTING COG PROCESSING\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nProcessing {len(files_to_process)} files...\")\n",
    "    print(\"This may take several minutes depending on file sizes.\\n\")\n",
    "    \n",
    "    # Display processing options\n",
    "    print(f\"Processing options:\")\n",
    "    print(f\"  Check source is COG: {CHECK_SOURCE_IS_COG}\")\n",
    "    print(f\"  Skip if source is COG: {SKIP_IF_SOURCE_IS_COG}\")\n",
    "    print(f\"  Overwrite existing: {OVERWRITE}\")\n",
    "    print(f\"  Verify COGs: {VERIFY}\\n\")\n",
    "    \n",
    "    # Track results\n",
    "    results = []\n",
    "    \n",
    "    for idx, row in files_to_process.iterrows():\n",
    "        start_time = time.time()\n",
    "        \n",
    "        source_path = row['original_s3_path']\n",
    "        dest_path = row['output_s3_path']\n",
    "        category = row['category']\n",
    "        \n",
    "        # Get nodata value from CSV (if available)\n",
    "        nodata = row.get('nodata_value', None)\n",
    "        if nodata is not None and pd.isna(nodata):\n",
    "            nodata = None  # Handle NaN values\n",
    "        \n",
    "        print(f\"\\n[{idx+1}/{len(files_to_process)}] Processing: {row['original_filename']}\")\n",
    "        print(f\"    Category: {category}\")\n",
    "        print(f\"    Size: {row['file_size_gb']:.2f} GB\")\n",
    "        print(f\"    Nodata: {nodata}\")\n",
    "        print(f\"    Output: {row['new_filename']}\")\n",
    "        \n",
    "        # Check if file already exists (unless OVERWRITE is True)\n",
    "        if not OVERWRITE:\n",
    "            if check_s3_file_exists(s3_client, BUCKET, dest_path):\n",
    "                print(f\"    ‚è≠Ô∏è  SKIPPED (already exists)\")\n",
    "                results.append({\n",
    "                    'source_file': row['original_filename'],\n",
    "                    'output_file': row['new_filename'],\n",
    "                    'category': category,\n",
    "                    'status': 'skipped',\n",
    "                    'time_seconds': 0,\n",
    "                    'error': 'File already exists'\n",
    "                })\n",
    "                continue\n",
    "        \n",
    "        try:\n",
    "            # Process file to COG (COG checking is built into this function)\n",
    "            success = process_single_file(\n",
    "                s3_client=s3_client,\n",
    "                bucket=BUCKET,\n",
    "                source_key=source_path,\n",
    "                dest_key=dest_path,\n",
    "                nodata=nodata,  # Use nodata from CSV\n",
    "                verify=VERIFY,\n",
    "                check_source_is_cog=CHECK_SOURCE_IS_COG,\n",
    "                skip_if_source_is_cog=SKIP_IF_SOURCE_IS_COG,\n",
    "                verbose=True\n",
    "            )\n",
    "            \n",
    "            elapsed = time.time() - start_time\n",
    "            \n",
    "            if success:\n",
    "                print(f\"    ‚úÖ SUCCESS ({elapsed:.1f}s)\")\n",
    "                results.append({\n",
    "                    'source_file': row['original_filename'],\n",
    "                    'output_file': row['new_filename'],\n",
    "                    'category': category,\n",
    "                    'status': 'success',\n",
    "                    'time_seconds': elapsed,\n",
    "                    'error': None\n",
    "                })\n",
    "            else:\n",
    "                print(f\"    ‚ùå FAILED ({elapsed:.1f}s)\")\n",
    "                results.append({\n",
    "                    'source_file': row['original_filename'],\n",
    "                    'output_file': row['new_filename'],\n",
    "                    'category': category,\n",
    "                    'status': 'failed',\n",
    "                    'time_seconds': elapsed,\n",
    "                    'error': 'Processing failed'\n",
    "                })\n",
    "        \n",
    "        except Exception as e:\n",
    "            elapsed = time.time() - start_time\n",
    "            print(f\"    ‚ùå ERROR: {str(e)}\")\n",
    "            results.append({\n",
    "                'source_file': row['original_filename'],\n",
    "                'output_file': row['new_filename'],\n",
    "                'category': category,\n",
    "                'status': 'failed',\n",
    "                'time_seconds': elapsed,\n",
    "                'error': str(e)\n",
    "            })\n",
    "    \n",
    "    # Create results DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"\\nüéâ PROCESSING COMPLETE!\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Cannot process: No files to process or S3 not connected\")\n",
    "    results_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ Step 7: Save Results to CSV\n",
    "\n",
    "Save processing results for future reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not results_df.empty and SAVE_RESULTS:\n",
    "    # Create output directory\n",
    "    output_path = Path(OUTPUT_DIR) / EVENT_NAME\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Generate filename with timestamp\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    results_filename = f\"processing_results_{timestamp}.csv\"\n",
    "    results_path = output_path / results_filename\n",
    "    \n",
    "    # Save results\n",
    "    results_df.to_csv(results_path, index=False)\n",
    "    \n",
    "    print(\"üíæ RESULTS SAVED\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\n‚úÖ Saved results to: {results_path.absolute()}\")\n",
    "    print(f\"\\nüìä Summary:\")\n",
    "    print(f\"   Records saved: {len(results_df)}\")\n",
    "    print(f\"   Successful:    {len(results_df[results_df['status'] == 'success'])}\")\n",
    "    print(f\"   Failed:        {len(results_df[results_df['status'] == 'failed'])}\")\n",
    "    print(f\"   Skipped:       {len(results_df[results_df['status'] == 'skipped'])}\")\n",
    "    \n",
    "elif results_df.empty:\n",
    "    print(\"‚ö†Ô∏è No results to save. Check Step 5.\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è  Results saving disabled (SAVE_RESULTS = False)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí° Next Steps\n",
    "\n",
    "After processing:\n",
    "\n",
    "1. **Review results** - Check the statistics and any failed files\n",
    "2. **Verify outputs** - Spot-check processed COGs in S3\n",
    "3. **Handle failures** - Investigate and retry any failed files\n",
    "4. **Update metadata** - Add metadata to processed files if needed\n",
    "\n",
    "## üîß Troubleshooting\n",
    "\n",
    "### Common Issues:\n",
    "\n",
    "1. **\"CSV file not found\"**\n",
    "   - Run `renaming_file_template.ipynb` first\n",
    "   - Check `EVENT_NAME` matches the CSV filename\n",
    "   - Verify `CSV_DIR` path is correct\n",
    "\n",
    "2. **\"Connection failed\"**\n",
    "   - Check AWS credentials\n",
    "   - Verify external ID is configured (if using upload role)\n",
    "   - Test with `test_upload.py`\n",
    "\n",
    "3. **\"All files skipped\"**\n",
    "   - Files already exist in destination\n",
    "   - Set `OVERWRITE = True` to replace existing files\n",
    "\n",
    "4. **\"Processing failures\"**\n",
    "   - Check source files are valid GeoTIFFs\n",
    "   - Verify enough disk space for temp files\n",
    "   - Check S3 write permissions\n",
    "   - Review error messages in results\n",
    "\n",
    "5. **\"Slow processing\"**\n",
    "   - Large files take longer to process\n",
    "   - Consider processing in smaller batches\n",
    "   - Use filters in Step 3 to process by size/category\n",
    "\n",
    "## üìù Notes\n",
    "\n",
    "- Processing time varies based on file size (typically 30s-5min per file)\n",
    "- COGs are created with zstd compression, level 9\n",
    "- Nodata values are auto-detected unless specified\n",
    "- All COGs include 5 overview levels for better performance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
