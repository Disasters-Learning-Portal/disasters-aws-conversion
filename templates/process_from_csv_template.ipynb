{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Files from CSV Mapping\n",
    "\n",
    "This notebook loads a filename mapping CSV (created by `renaming_file_template.ipynb`) and processes files to Cloud Optimized GeoTIFFs (COGs) with the new filenames.\n",
    "\n",
    "## Features\n",
    "- **Load CSV mapping** - Import pre-defined filename transformations\n",
    "- **Preview before processing** - Review what will be processed\n",
    "- **Batch COG conversion** - Convert all files to COGs\n",
    "- **Track results** - Save processing results to CSV\n",
    "\n",
    "## Workflow\n",
    "1. Generate mapping CSV using `renaming_file_template.ipynb`\n",
    "2. Review and validate the CSV\n",
    "3. Run this notebook to process files\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Step 1: Basic Configuration\n",
    "\n",
    "Set your event name to load the corresponding CSV mapping file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# INPUTS\n",
    "# ========================================\n",
    "\n",
    "# S3 Configuration\n",
    "BUCKET = 'nasa-disasters'    # S3 bucket (DO NOT CHANGE)\n",
    "\n",
    "# Event Details\n",
    "EVENT_NAME = '202510_Flood_AK'  # Must match the event name used in renaming_file_template.ipynb\n",
    "SUB_PRODUCT_NAME = 'sentinel2'\n",
    "\n",
    "# CSV Mapping Configuration\n",
    "CSV_DIR = 'file-mapping'  # Directory where CSV mappings are stored\n",
    "CSV_FILENAME = f'{EVENT_NAME}-{SUB_PRODUCT_NAME}.csv'  # CSV filename (usually {EVENT_NAME}-{subproductName}.csv)\n",
    "\n",
    "# Processing Options\n",
    "CHECK_SOURCE_IS_COG = True  # Check if source files are already valid COGs\n",
    "SKIP_IF_SOURCE_IS_COG = True  # Skip processing if source is already a valid COG\n",
    "OVERWRITE = False      # Set to True to replace existing files in S3 (after converting to COG)\n",
    "VERIFY = True          # Set to True to verify COGs after creation\n",
    "SAVE_RESULTS = True    # Save processing results to CSV\n",
    "\n",
    "# Output\n",
    "OUTPUT_DIR = 'csv_stats'  # Directory for results CSV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÇ Step 2: Load CSV Mapping\n",
    "\n",
    "Load the filename mapping CSV created by the renaming template:"
   ]
  },
  {
   "cell_type": "code",
   "source": "import pandas as pd\nfrom pathlib import Path\n\n# Construct CSV path\ncsv_path = Path(CSV_DIR) / CSV_FILENAME\n\nprint(\"üìÇ LOADING CSV MAPPING\")\nprint(\"=\"*80)\nprint(f\"\\nLooking for: {csv_path}\")\n\n# Check if CSV exists\nif not csv_path.exists():\n    print(f\"\\n‚ùå ERROR: CSV file not found!\")\n    print(f\"\\nMake sure you've run 'renaming_file_template.ipynb' first to create the mapping CSV.\")\n    print(f\"Expected location: {csv_path.absolute()}\")\n    mapping_df = pd.DataFrame()\nelse:\n    # Load CSV\n    mapping_df = pd.read_csv(csv_path)\n    \n    print(f\"\\n‚úÖ Successfully loaded CSV\")\n    print(f\"\\nüìä Mapping details:\")\n    print(f\"   Total entries: {len(mapping_df)}\")\n    \n    if 'status' in mapping_df.columns:\n        print(f\"   Valid files:   {len(mapping_df[mapping_df['status'] == 'valid'])}\")\n        print(f\"   Invalid files: {len(mapping_df[mapping_df['status'] == 'invalid'])}\")\n    \n    if 'file_size_gb' in mapping_df.columns:\n        print(f\"   Total size:    {mapping_df['file_size_gb'].sum():.2f} GB\")\n    \n    if 'category' in mapping_df.columns:\n        print(f\"\\nüìÇ By category:\")\n        category_counts = mapping_df['category'].value_counts()\n        for category, count in category_counts.items():\n            print(f\"   ‚Ä¢ {category}: {count} files\")\n    \n    print(f\"\\n‚úÖ Ready to filter and process files\")\n    print(\"\\n\" + \"=\"*80)\n    \n    # Display first few rows\n    print(\"\\nüìã Preview (first 5 rows):\")\n    print(mapping_df.head())",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Step 3: Filter Files to Process\n",
    "\n",
    "Optionally filter which files to process (by category, size, etc.):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mapping_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mmapping_df\u001b[49m.empty:\n\u001b[32m      2\u001b[39m     \u001b[38;5;66;03m# Filter out uncategorized files (they won't be processed)\u001b[39;00m\n\u001b[32m      3\u001b[39m     files_to_process = mapping_df[mapping_df[\u001b[33m'\u001b[39m\u001b[33mstatus\u001b[39m\u001b[33m'\u001b[39m] == \u001b[33m'\u001b[39m\u001b[33mvalid\u001b[39m\u001b[33m'\u001b[39m].copy()\n\u001b[32m      5\u001b[39m     \u001b[38;5;66;03m# Optional: Filter by category\u001b[39;00m\n\u001b[32m      6\u001b[39m     \u001b[38;5;66;03m# Uncomment and modify to process only specific categories:\u001b[39;00m\n\u001b[32m      7\u001b[39m     \u001b[38;5;66;03m# CATEGORIES_TO_PROCESS = ['trueColor', 'colorInfrared']\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     12\u001b[39m     \u001b[38;5;66;03m# MAX_SIZE_GB = 5.0\u001b[39;00m\n\u001b[32m     13\u001b[39m     \u001b[38;5;66;03m# files_to_process = files_to_process[files_to_process['file_size_gb'] <= MAX_SIZE_GB]\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'mapping_df' is not defined"
     ]
    }
   ],
   "source": [
    "if not mapping_df.empty:\n",
    "    # Filter out uncategorized files (they won't be processed)\n",
    "    files_to_process = mapping_df[mapping_df['status'] == 'valid'].copy()\n",
    "    \n",
    "    # Optional: Filter by category\n",
    "    # Uncomment and modify to process only specific categories:\n",
    "    # CATEGORIES_TO_PROCESS = ['trueColor', 'colorInfrared']\n",
    "    # files_to_process = files_to_process[files_to_process['category'].isin(CATEGORIES_TO_PROCESS)]\n",
    "    \n",
    "    # Optional: Filter by file size\n",
    "    # Uncomment to process only files smaller than a certain size:\n",
    "    # MAX_SIZE_GB = 5.0\n",
    "    # files_to_process = files_to_process[files_to_process['file_size_gb'] <= MAX_SIZE_GB]\n",
    "    \n",
    "    print(\"üìä FILES TO PROCESS\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nTotal files: {len(files_to_process)}\")\n",
    "    print(f\"Total size:  {files_to_process['file_size_gb'].sum():.2f} GB\")\n",
    "    \n",
    "    if len(files_to_process) > 0:\n",
    "        print(f\"\\nüìÇ By category:\")\n",
    "        category_counts = files_to_process['category'].value_counts()\n",
    "        for category, count in category_counts.items():\n",
    "            print(f\"   ‚Ä¢ {category}: {count} files\")\n",
    "        \n",
    "        print(f\"\\n‚úÖ Ready to process {len(files_to_process)} files\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è  No files match the filter criteria\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No mapping data loaded. Check Step 2.\")\n",
    "    files_to_process = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üåê Step 4: Connect to S3\n",
    "\n",
    "Initialize S3 client for downloading source files and uploading processed COGs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.s3_operations import initialize_s3_client\n",
    "\n",
    "print(\"üåê Connecting to S3...\")\n",
    "s3_client, fs = initialize_s3_client(bucket_name=BUCKET, verbose=True)\n",
    "\n",
    "if s3_client:\n",
    "    print(\"\\n‚úÖ Successfully connected to S3\")\n",
    "    print(\"   Ready to download source files and upload processed COGs\")\n",
    "else:\n",
    "    print(\"\\n‚ùå Failed to connect to S3\")\n",
    "    print(\"   Check your AWS credentials and permissions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Step 5: Process Files to COGs\n",
    "\n",
    "Convert files to Cloud Optimized GeoTIFFs with the new filenames:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Step 3.5: Validate Source Files (Check if Already COGs)\n",
    "\n",
    "Check if source files are already valid COGs to avoid unnecessary processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not files_to_process.empty and CHECK_SOURCE_IS_COG and s3_client:\n",
    "    from core.validation import is_s3_file_cog\n",
    "    \n",
    "    print(\"üîç VALIDATING SOURCE FILES\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nChecking if {len(files_to_process)} source files are already valid COGs...\")\n",
    "    print(\"This may take a while as files need to be downloaded for validation.\\n\")\n",
    "    \n",
    "    # Add COG status columns\n",
    "    files_to_process['source_is_cog'] = False\n",
    "    files_to_process['source_cog_details'] = None\n",
    "    \n",
    "    # Check each file\n",
    "    already_cog_count = 0\n",
    "    not_cog_count = 0\n",
    "    error_count = 0\n",
    "    \n",
    "    for idx, row in files_to_process.iterrows():\n",
    "        source_path = row['original_s3_path']\n",
    "        filename = row['original_filename']\n",
    "        \n",
    "        print(f\"[{idx+1}/{len(files_to_process)}] Checking: {filename}...\", end=\" \")\n",
    "        \n",
    "        try:\n",
    "            is_cog, details = is_s3_file_cog(s3_client, BUCKET, source_path, verbose=False)\n",
    "            \n",
    "            files_to_process.at[idx, 'source_is_cog'] = is_cog\n",
    "            files_to_process.at[idx, 'source_cog_details'] = str(details)\n",
    "            \n",
    "            if is_cog:\n",
    "                print(\"‚úÖ Already a COG\")\n",
    "                already_cog_count += 1\n",
    "            else:\n",
    "                print(\"‚ùå Not a COG\")\n",
    "                not_cog_count += 1\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Error: {str(e)}\")\n",
    "            error_count += 1\n",
    "            files_to_process.at[idx, 'source_is_cog'] = False\n",
    "            files_to_process.at[idx, 'source_cog_details'] = f\"Error: {str(e)}\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"\\nüìä VALIDATION RESULTS:\")\n",
    "    print(f\"   ‚úÖ Already valid COGs: {already_cog_count}\")\n",
    "    print(f\"   ‚ùå Not COGs (need processing): {not_cog_count}\")\n",
    "    print(f\"   ‚ö†Ô∏è  Errors: {error_count}\")\n",
    "    \n",
    "    # Filter out files that are already COGs if SKIP_IF_SOURCE_IS_COG is True\n",
    "    if SKIP_IF_SOURCE_IS_COG:\n",
    "        files_before = len(files_to_process)\n",
    "        files_to_process = files_to_process[files_to_process['source_is_cog'] == False].copy()\n",
    "        files_after = len(files_to_process)\n",
    "        \n",
    "        skipped = files_before - files_after\n",
    "        if skipped > 0:\n",
    "            print(f\"\\n‚è≠Ô∏è  SKIPPING {skipped} files that are already valid COGs\")\n",
    "            print(f\"   Files remaining to process: {files_after}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    \n",
    "elif not CHECK_SOURCE_IS_COG:\n",
    "    print(\"‚ÑπÔ∏è  Source COG validation disabled (CHECK_SOURCE_IS_COG = False)\")\n",
    "elif files_to_process.empty:\n",
    "    print(\"‚ö†Ô∏è  No files to validate\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  S3 client not connected, skipping validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not files_to_process.empty and s3_client:\n",
    "    from core.cog_processing import process_single_file\n",
    "    from core.s3_operations import check_s3_file_exists\n",
    "    import time\n",
    "    \n",
    "    print(\"üöÄ STARTING COG PROCESSING\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nProcessing {len(files_to_process)} files...\")\n",
    "    print(\"This may take several minutes depending on file sizes.\\n\")\n",
    "    \n",
    "    # Track results\n",
    "    results = []\n",
    "    \n",
    "    for idx, row in files_to_process.iterrows():\n",
    "        start_time = time.time()\n",
    "        \n",
    "        source_path = row['original_s3_path']\n",
    "        dest_path = row['output_s3_path']\n",
    "        category = row['category']\n",
    "        \n",
    "        print(f\"\\n[{idx+1}/{len(files_to_process)}] Processing: {row['original_filename']}\")\n",
    "        print(f\"    Category: {category}\")\n",
    "        print(f\"    Size: {row['file_size_gb']:.2f} GB\")\n",
    "        print(f\"    Output: {row['new_filename']}\")\n",
    "        \n",
    "        # Check if file already exists (unless OVERWRITE is True)\n",
    "        if not OVERWRITE:\n",
    "            if check_s3_file_exists(s3_client, BUCKET, dest_path):\n",
    "                print(f\"    ‚è≠Ô∏è  SKIPPED (already exists)\")\n",
    "                results.append({\n",
    "                    'source_file': row['original_filename'],\n",
    "                    'output_file': row['new_filename'],\n",
    "                    'category': category,\n",
    "                    'status': 'skipped',\n",
    "                    'time_seconds': 0,\n",
    "                    'error': 'File already exists'\n",
    "                })\n",
    "                continue\n",
    "        \n",
    "        try:\n",
    "            # Process file to COG\n",
    "            success = process_single_file(\n",
    "                s3_client=s3_client,\n",
    "                bucket=BUCKET,\n",
    "                source_key=source_path,\n",
    "                dest_key=dest_path,\n",
    "                nodata=None,  # Auto-detect nodata value\n",
    "                verify=VERIFY\n",
    "            )\n",
    "            \n",
    "            elapsed = time.time() - start_time\n",
    "            \n",
    "            if success:\n",
    "                print(f\"    ‚úÖ SUCCESS ({elapsed:.1f}s)\")\n",
    "                results.append({\n",
    "                    'source_file': row['original_filename'],\n",
    "                    'output_file': row['new_filename'],\n",
    "                    'category': category,\n",
    "                    'status': 'success',\n",
    "                    'time_seconds': elapsed,\n",
    "                    'error': None\n",
    "                })\n",
    "            else:\n",
    "                print(f\"    ‚ùå FAILED ({elapsed:.1f}s)\")\n",
    "                results.append({\n",
    "                    'source_file': row['original_filename'],\n",
    "                    'output_file': row['new_filename'],\n",
    "                    'category': category,\n",
    "                    'status': 'failed',\n",
    "                    'time_seconds': elapsed,\n",
    "                    'error': 'Processing failed'\n",
    "                })\n",
    "        \n",
    "        except Exception as e:\n",
    "            elapsed = time.time() - start_time\n",
    "            print(f\"    ‚ùå ERROR: {str(e)}\")\n",
    "            results.append({\n",
    "                'source_file': row['original_filename'],\n",
    "                'output_file': row['new_filename'],\n",
    "                'category': category,\n",
    "                'status': 'failed',\n",
    "                'time_seconds': elapsed,\n",
    "                'error': str(e)\n",
    "            })\n",
    "    \n",
    "    # Create results DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"\\nüéâ PROCESSING COMPLETE!\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Cannot process: No files to process or S3 not connected\")\n",
    "    results_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Step 6: View Results\n",
    "\n",
    "Display processing statistics and results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not results_df.empty:\n",
    "    print(\"üìä PROCESSING STATISTICS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Overall stats\n",
    "    total = len(results_df)\n",
    "    success = len(results_df[results_df['status'] == 'success'])\n",
    "    failed = len(results_df[results_df['status'] == 'failed'])\n",
    "    skipped = len(results_df[results_df['status'] == 'skipped'])\n",
    "    \n",
    "    print(f\"\\nüî¢ Overall:\")\n",
    "    print(f\"   Total files:   {total}\")\n",
    "    print(f\"   ‚úÖ Success:    {success} ({success/total*100:.1f}%)\")\n",
    "    print(f\"   ‚ùå Failed:     {failed} ({failed/total*100:.1f}%)\")\n",
    "    print(f\"   ‚è≠Ô∏è  Skipped:    {skipped} ({skipped/total*100:.1f}%)\")\n",
    "    \n",
    "    # By category\n",
    "    print(f\"\\nüìÇ By category:\")\n",
    "    category_stats = results_df.groupby(['category', 'status']).size().unstack(fill_value=0)\n",
    "    print(category_stats.to_string())\n",
    "    \n",
    "    # Timing stats (for successful files)\n",
    "    success_df = results_df[results_df['status'] == 'success']\n",
    "    if not success_df.empty:\n",
    "        print(f\"\\n‚è±Ô∏è  Timing (successful files):\")\n",
    "        print(f\"   Average: {success_df['time_seconds'].mean():.1f}s per file\")\n",
    "        print(f\"   Fastest: {success_df['time_seconds'].min():.1f}s\")\n",
    "        print(f\"   Slowest: {success_df['time_seconds'].max():.1f}s\")\n",
    "        print(f\"   Total:   {success_df['time_seconds'].sum()/60:.1f} minutes\")\n",
    "    \n",
    "    # Failed files detail\n",
    "    if failed > 0:\n",
    "        print(f\"\\n‚ùå Failed files:\")\n",
    "        failed_df = results_df[results_df['status'] == 'failed']\n",
    "        for idx, row in failed_df.iterrows():\n",
    "            print(f\"   ‚Ä¢ {row['source_file']}: {row['error']}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    \n",
    "    # Display full results table\n",
    "    print(\"\\nüìã Full results:\")\n",
    "    display(results_df)\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No results to display. Check Step 5.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ Step 7: Save Results to CSV\n",
    "\n",
    "Save processing results for future reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not results_df.empty and SAVE_RESULTS:\n",
    "    # Create output directory\n",
    "    output_path = Path(OUTPUT_DIR) / EVENT_NAME\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Generate filename with timestamp\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    results_filename = f\"processing_results_{timestamp}.csv\"\n",
    "    results_path = output_path / results_filename\n",
    "    \n",
    "    # Save results\n",
    "    results_df.to_csv(results_path, index=False)\n",
    "    \n",
    "    print(\"üíæ RESULTS SAVED\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\n‚úÖ Saved results to: {results_path.absolute()}\")\n",
    "    print(f\"\\nüìä Summary:\")\n",
    "    print(f\"   Records saved: {len(results_df)}\")\n",
    "    print(f\"   Successful:    {len(results_df[results_df['status'] == 'success'])}\")\n",
    "    print(f\"   Failed:        {len(results_df[results_df['status'] == 'failed'])}\")\n",
    "    print(f\"   Skipped:       {len(results_df[results_df['status'] == 'skipped'])}\")\n",
    "    \n",
    "elif results_df.empty:\n",
    "    print(\"‚ö†Ô∏è No results to save. Check Step 5.\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è  Results saving disabled (SAVE_RESULTS = False)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí° Next Steps\n",
    "\n",
    "After processing:\n",
    "\n",
    "1. **Review results** - Check the statistics and any failed files\n",
    "2. **Verify outputs** - Spot-check processed COGs in S3\n",
    "3. **Handle failures** - Investigate and retry any failed files\n",
    "4. **Update metadata** - Add metadata to processed files if needed\n",
    "\n",
    "## üîß Troubleshooting\n",
    "\n",
    "### Common Issues:\n",
    "\n",
    "1. **\"CSV file not found\"**\n",
    "   - Run `renaming_file_template.ipynb` first\n",
    "   - Check `EVENT_NAME` matches the CSV filename\n",
    "   - Verify `CSV_DIR` path is correct\n",
    "\n",
    "2. **\"Connection failed\"**\n",
    "   - Check AWS credentials\n",
    "   - Verify external ID is configured (if using upload role)\n",
    "   - Test with `test_upload.py`\n",
    "\n",
    "3. **\"All files skipped\"**\n",
    "   - Files already exist in destination\n",
    "   - Set `OVERWRITE = True` to replace existing files\n",
    "\n",
    "4. **\"Processing failures\"**\n",
    "   - Check source files are valid GeoTIFFs\n",
    "   - Verify enough disk space for temp files\n",
    "   - Check S3 write permissions\n",
    "   - Review error messages in results\n",
    "\n",
    "5. **\"Slow processing\"**\n",
    "   - Large files take longer to process\n",
    "   - Consider processing in smaller batches\n",
    "   - Use filters in Step 3 to process by size/category\n",
    "\n",
    "## üìù Notes\n",
    "\n",
    "- Processing time varies based on file size (typically 30s-5min per file)\n",
    "- COGs are created with zstd compression, level 9\n",
    "- Nodata values are auto-detected unless specified\n",
    "- All COGs include 5 overview levels for better performance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}