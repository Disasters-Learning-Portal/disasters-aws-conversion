{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üåç Simple Disaster COG Processing\n",
    "\n",
    "This simplified notebook converts disaster satellite imagery to Cloud Optimized GeoTIFFs (COGs) with just a few cells.\n",
    "\n",
    "## ‚ú® Features\n",
    "- **See files first** - List S3 files before configuring\n",
    "- **Smart configuration** - Define filename functions after seeing actual files\n",
    "- **Auto-discovery** - Automatically categorizes your files\n",
    "- **Simple processing** - Just run the cells in order\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Step 1: Basic Configuration\n",
    "\n",
    "Set your event details and S3 paths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# BASIC CONFIGURATION\n",
    "# ========================================\n",
    "\n",
    "# Event Details\n",
    "EVENT_NAME = '202408_TropicalStorm_Debby'  # Your disaster event name\n",
    "PRODUCT_NAME = 'landsat8'                   # Product type (sentinel1, sentinel2, landsat, etc.)\n",
    "\n",
    "# S3 Paths\n",
    "BUCKET = 'nasa-disasters'                                          # S3 bucket\n",
    "SOURCE_PATH = f'drcs_activations/{EVENT_NAME}/{PRODUCT_NAME}'      # Where your files are\n",
    "DESTINATION_BASE = 'drcs_activations_new'                          # Where to save COGs\n",
    "\n",
    "# Processing Options\n",
    "OVERWRITE = False  # Set to True to replace existing files\n",
    "VERIFY = True      # Set to True to verify results after processing\n",
    "\n",
    "print(\"‚úÖ Basic configuration loaded\")\n",
    "print(f\"Event: {EVENT_NAME}\")\n",
    "print(f\"Source: s3://{BUCKET}/{SOURCE_PATH}\")\n",
    "print(f\"Destination: s3://{BUCKET}/{DESTINATION_BASE}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Step 2: Connect to S3 and List Files\n",
    "\n",
    "Let's see what files are available before configuring filename transformations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.insert(0, str(Path('..').resolve()))\n",
    "\n",
    "# Import S3 operations\n",
    "from core.s3_operations import (\n",
    "    initialize_s3_client,\n",
    "    list_s3_files,\n",
    "    get_file_size_from_s3\n",
    ")\n",
    "\n",
    "# Initialize S3 client\n",
    "print(\"üåê Connecting to S3...\")\n",
    "s3_client, _ = initialize_s3_client(bucket_name=BUCKET, verbose=False)\n",
    "\n",
    "if s3_client:\n",
    "    print(\"‚úÖ Connected to S3\\n\")\n",
    "    \n",
    "    # List all TIF files\n",
    "    print(f\"üìÇ Files in s3://{BUCKET}/{SOURCE_PATH}:\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    files = list_s3_files(s3_client, BUCKET, SOURCE_PATH, suffix='.tif')\n",
    "    \n",
    "    if files:\n",
    "        print(f\"Found {len(files)} .tif files:\\n\")\n",
    "        for i, file_path in enumerate(files[:10], 1):  # Show first 10 files\n",
    "            filename = os.path.basename(file_path)\n",
    "            try:\n",
    "                size_gb = get_file_size_from_s3(s3_client, BUCKET, file_path)\n",
    "                print(f\"{i:2}. {filename:<60} ({size_gb:.2f} GB)\")\n",
    "            except:\n",
    "                print(f\"{i:2}. {filename}\")\n",
    "        \n",
    "        if len(files) > 10:\n",
    "            print(f\"\\n... and {len(files) - 10} more files\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"üìù Now you can see the file naming patterns!\")\n",
    "        print(\"   Use this information to create filename functions in the next step.\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No .tif files found in the specified path.\")\n",
    "        print(\"   Check your SOURCE_PATH configuration.\")\n",
    "else:\n",
    "    print(\"‚ùå Could not connect to S3. Check your AWS credentials.\")\n",
    "    files = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üè∑Ô∏è Step 3: Define Filename Transformations\n",
    "\n",
    "Based on the files you see above, define how to transform the filenames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# FILENAME GENERATION FUNCTIONS\n",
    "# Modify these based on the files you see above\n",
    "# ========================================\n",
    "\n",
    "import re\n",
    "\n",
    "def extract_date_from_filename(filename):\n",
    "    \"\"\"Extract date from filename in YYYYMMDD format.\"\"\"\n",
    "    dates = re.findall(r'\\d{8}', filename)\n",
    "    if dates:\n",
    "        date_str = dates[0]\n",
    "        return f\"{date_str[0:4]}-{date_str[4:6]}-{date_str[6:8]}\"\n",
    "    return None\n",
    "\n",
    "def create_truecolor_filename(original_path, event_name):\n",
    "    \"\"\"Create filename for trueColor products.\"\"\"\n",
    "    filename = os.path.basename(original_path)\n",
    "    stem = os.path.splitext(filename)[0]\n",
    "    date = extract_date_from_filename(stem)\n",
    "    \n",
    "    if date:\n",
    "        stem_clean = re.sub(r'_\\d{8}', '', stem)\n",
    "        return f\"{event_name}_{stem_clean}_{date}_day.tif\"\n",
    "    return f\"{event_name}_{stem}_day.tif\"\n",
    "\n",
    "def create_colorinfrared_filename(original_path, event_name):\n",
    "    \"\"\"Create filename for colorInfrared products.\"\"\"\n",
    "    filename = os.path.basename(original_path)\n",
    "    stem = os.path.splitext(filename)[0]\n",
    "    date = extract_date_from_filename(stem)\n",
    "    \n",
    "    if date:\n",
    "        stem_clean = re.sub(r'_\\d{8}', '', stem)\n",
    "        return f\"{event_name}_{stem_clean}_{date}_day.tif\"\n",
    "    return f\"{event_name}_{stem}_day.tif\"\n",
    "\n",
    "def create_naturalcolor_filename(original_path, event_name):\n",
    "    \"\"\"Create filename for naturalColor products.\"\"\"\n",
    "    filename = os.path.basename(original_path)\n",
    "    stem = os.path.splitext(filename)[0]\n",
    "    date = extract_date_from_filename(stem)\n",
    "    \n",
    "    if date:\n",
    "        stem_clean = re.sub(r'_\\d{8}', '', stem)\n",
    "        return f\"{event_name}_{stem_clean}_{date}_day.tif\"\n",
    "    return f\"{event_name}_{stem}_day.tif\"\n",
    "\n",
    "def create_generic_filename(original_path, event_name):\n",
    "    \"\"\"Generic filename creator for any file type.\"\"\"\n",
    "    filename = os.path.basename(original_path)\n",
    "    stem = os.path.splitext(filename)[0]\n",
    "    date = extract_date_from_filename(stem)\n",
    "    \n",
    "    if date:\n",
    "        stem_clean = re.sub(r'_\\d{8}', '', stem)\n",
    "        return f\"{event_name}_{stem_clean}_{date}_day.tif\"\n",
    "    return f\"{event_name}_{stem}_day.tif\"\n",
    "\n",
    "# Map product types to filename creators\n",
    "FILENAME_CREATORS = {\n",
    "    'trueColor': create_truecolor_filename,\n",
    "    'colorInfrared': create_colorinfrared_filename,\n",
    "    'naturalColor': create_naturalcolor_filename,\n",
    "    # Add more mappings as needed based on your files\n",
    "}\n",
    "\n",
    "# Optional: Override output directories for each category\n",
    "OUTPUT_DIRS = {\n",
    "    'trueColor': 'Landsat/trueColor',\n",
    "    'colorInfrared': 'Landsat/colorIR',\n",
    "    'naturalColor': 'Landsat/naturalColor',\n",
    "    # Add more as needed\n",
    "}\n",
    "\n",
    "# Optional: Manual no-data values (None = auto-detect)\n",
    "NODATA_VALUES = {\n",
    "    'NDVI': -9999,\n",
    "    'MNDWI': -9999,\n",
    "    # Leave empty or set to None for auto-detection\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Filename functions defined\")\n",
    "\n",
    "# Test with a sample filename if files exist\n",
    "if files:\n",
    "    sample_file = files[0]\n",
    "    sample_name = os.path.basename(sample_file)\n",
    "    \n",
    "    # Try to detect which function to use\n",
    "    if 'trueColor' in sample_name or 'truecolor' in sample_name:\n",
    "        new_name = create_truecolor_filename(sample_file, EVENT_NAME)\n",
    "    elif 'colorInfrared' in sample_name or 'colorIR' in sample_name:\n",
    "        new_name = create_colorinfrared_filename(sample_file, EVENT_NAME)\n",
    "    elif 'naturalColor' in sample_name:\n",
    "        new_name = create_naturalcolor_filename(sample_file, EVENT_NAME)\n",
    "    else:\n",
    "        new_name = create_generic_filename(sample_file, EVENT_NAME)\n",
    "    \n",
    "    print(f\"\\nüìù Example transformation:\")\n",
    "    print(f\"   Original: {sample_name}\")\n",
    "    print(f\"   ‚Üí New:    {new_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Step 4: Initialize Processor and Preview\n",
    "\n",
    "Now let's set up the processor and preview all transformations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our simplified helper\n",
    "from notebooks.notebook_helpers import SimpleProcessor\n",
    "\n",
    "# Create full configuration\n",
    "config = {\n",
    "    'event_name': EVENT_NAME,\n",
    "    'bucket': BUCKET,\n",
    "    'source_path': SOURCE_PATH,\n",
    "    'destination_base': DESTINATION_BASE,\n",
    "    'overwrite': OVERWRITE,\n",
    "    'verify': VERIFY,\n",
    "    'filename_creators': FILENAME_CREATORS,\n",
    "    'output_dirs': OUTPUT_DIRS,\n",
    "    'nodata_values': NODATA_VALUES\n",
    "}\n",
    "\n",
    "# Initialize processor\n",
    "processor = SimpleProcessor(config)\n",
    "\n",
    "# Connect to S3 (already connected, but needed for processor)\n",
    "if processor.connect_to_s3():\n",
    "    print(\"‚úÖ Processor ready\\n\")\n",
    "    \n",
    "    # Discover and categorize files\n",
    "    num_files = processor.discover_files()\n",
    "    \n",
    "    if num_files > 0:\n",
    "        # Show preview of transformations\n",
    "        processor.preview_processing()\n",
    "        \n",
    "        print(\"\\nüìå Review the transformations above.\")\n",
    "        print(\"   If they look incorrect, go back to Step 3 and adjust the filename functions.\")\n",
    "        print(\"   When ready, proceed to Step 5 to process the files.\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No files found to process.\")\n",
    "else:\n",
    "    print(\"‚ùå Could not initialize processor.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Step 5: Process Files\n",
    "\n",
    "Run this cell to start processing all files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all files\n",
    "if 'num_files' in locals() and num_files > 0:\n",
    "    print(\"üöÄ Starting processing...\")\n",
    "    print(\"This may take several minutes depending on file sizes.\\n\")\n",
    "    \n",
    "    # Process everything\n",
    "    results = processor.process_all()\n",
    "    \n",
    "    # Display results\n",
    "    if not results.empty:\n",
    "        print(\"\\nüìä Processing Complete!\")\n",
    "        display(results) if 'display' in dir() else print(results)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No files to process. Complete Steps 1-4 first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Step 6: Review Results (Optional)\n",
    "\n",
    "View detailed results and statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze results\n",
    "if 'results' in locals() and not results.empty:\n",
    "    print(\"üìä PROCESSING STATISTICS\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    # Success rate\n",
    "    total = len(results)\n",
    "    success = len(results[results['status'] == 'success'])\n",
    "    failed = len(results[results['status'] == 'failed'])\n",
    "    skipped = len(results[results['status'] == 'skipped'])\n",
    "    \n",
    "    print(f\"Total files: {total}\")\n",
    "    print(f\"‚úÖ Success: {success}\")\n",
    "    print(f\"‚ùå Failed: {failed}\")\n",
    "    print(f\"‚è≠Ô∏è Skipped: {skipped}\")\n",
    "    print(f\"\\nSuccess rate: {(success/total*100):.1f}%\")\n",
    "    \n",
    "    # Failed files\n",
    "    if failed > 0:\n",
    "        print(\"\\n‚ùå Failed files:\")\n",
    "        failed_df = results[results['status'] == 'failed']\n",
    "        for idx, row in failed_df.iterrows():\n",
    "            print(f\"  - {row['file']}: {row.get('error', 'Unknown error')}\")\n",
    "    \n",
    "    # Processing times\n",
    "    if 'time_seconds' in results.columns:\n",
    "        success_df = results[results['status'] == 'success']\n",
    "        if not success_df.empty:\n",
    "            avg_time = success_df['time_seconds'].mean()\n",
    "            max_time = success_df['time_seconds'].max()\n",
    "            print(f\"\\n‚è±Ô∏è Timing:\")\n",
    "            print(f\"Average: {avg_time:.1f} seconds per file\")\n",
    "            print(f\"Slowest: {max_time:.1f} seconds\")\n",
    "else:\n",
    "    print(\"No results to analyze. Run Step 5 first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí° Tips & Troubleshooting\n",
    "\n",
    "### Workflow Summary:\n",
    "1. **Configure** basic settings (Step 1)\n",
    "2. **List files** from S3 to see naming patterns (Step 2)\n",
    "3. **Define functions** to transform filenames (Step 3)\n",
    "4. **Preview** transformations (Step 4)\n",
    "5. **Process** all files (Step 5)\n",
    "6. **Review** results (Step 6)\n",
    "\n",
    "### Common Issues:\n",
    "\n",
    "1. **\"No files found\"**\n",
    "   - Check `SOURCE_PATH` in Step 1\n",
    "   - Verify bucket permissions\n",
    "   - Ensure files have `.tif` extension\n",
    "\n",
    "2. **Wrong filename transformations**\n",
    "   - Review actual filenames in Step 2\n",
    "   - Adjust functions in Step 3\n",
    "   - Re-run Step 4 to preview\n",
    "\n",
    "3. **Files being skipped**\n",
    "   - Files already exist in destination\n",
    "   - Set `OVERWRITE = True` in Step 1\n",
    "\n",
    "4. **Processing errors**\n",
    "   - Check AWS credentials\n",
    "   - Verify S3 write permissions\n",
    "   - Check available disk space for temp files\n",
    "\n",
    "### Need More Control?\n",
    "\n",
    "Use the full template at `disaster_processing_template.ipynb` for:\n",
    "- Manual chunk configuration\n",
    "- Custom compression settings\n",
    "- Detailed memory management\n",
    "- Advanced processing options"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}