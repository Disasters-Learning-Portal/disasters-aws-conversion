{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disaster AWS COG Conversion Template\n",
    "\n",
    "This template provides a comprehensive workflow for converting satellite imagery to Cloud Optimized GeoTIFFs (COGs) with:\n",
    "- **Modular architecture** with single-responsibility functions\n",
    "- **Automatic error handling** and recovery\n",
    "- **Memory-efficient processing** for large files\n",
    "- **S3 streaming and caching** capabilities\n",
    "\n",
    "## Key Features\n",
    "- ‚úÖ Handles files from <1GB to >10GB\n",
    "- ‚úÖ Prevents striping issues with fixed chunk processing\n",
    "- ‚úÖ Automatic S3 existence checking\n",
    "- ‚úÖ ZSTD compression with optimal predictors\n",
    "- ‚úÖ Comprehensive error tracking\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã CONFIGURATION CELL - MODIFY PARAMETERS HERE\n",
    "\n",
    "**This is the only cell you need to modify for different events**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configuration loaded successfully!\n",
      "Event: 202504_SevereWx_US\n",
      "Source: s3://nasa-disasters/drcs_activations/202504_SevereWx_US/sentinel2\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# MAIN CONFIGURATION - MODIFY THESE VALUES\n",
    "# ========================================\n",
    "\n",
    "# Event Configuration\n",
    "EVENT_NAME = '202504_SevereWx_US'  # Event identifier\n",
    "PRODUCT_NAME = 'sentinel2'          # Product type (sentinel1, sentinel2, landsat, etc.)\n",
    "\n",
    "# S3 Configuration\n",
    "BUCKET = 'nasa-disasters'                                # S3 bucket name\n",
    "DIR_OLD_BASE = 'drcs_activations'                       # Source directory base\n",
    "DIR_NEW_BASE = 'drcs_activations_new'                   # Destination directory base\n",
    "PATH_OLD = f'{DIR_OLD_BASE}/{EVENT_NAME}/{PRODUCT_NAME}' # Full source path\n",
    "\n",
    "# File Size Thresholds (in GB)\n",
    "LARGE_FILE_THRESHOLD = 3   # Files > 3GB use large file config\n",
    "ULTRA_LARGE_THRESHOLD = 7  # Files > 7GB use ultra-large config\n",
    "\n",
    "# Memory Configuration\n",
    "MEMORY_LIMIT_MB = 500      # Memory limit per chunk\n",
    "FORCE_FIXED_CHUNKS = True  # Use fixed chunks for large files (prevents striping)\n",
    "\n",
    "# Output Configuration\n",
    "SAVE_LOCAL = True          # Save files locally during processing\n",
    "SAVE_METADATA = True       # Save processing metadata to bucket\n",
    "VERBOSE = True             # Verbose output for functions\n",
    "\n",
    "# Advanced Configuration (usually don't need to change)\n",
    "USE_STREAMING = False      # Stream from S3 (set False if having issues with large files)\n",
    "CACHE_DOWNLOADS = True     # Cache downloaded files\n",
    "MAX_RETRIES = 3           # Maximum retry attempts\n",
    "\n",
    "print(\"‚úÖ Configuration loaded successfully!\")\n",
    "print(f\"Event: {EVENT_NAME}\")\n",
    "print(f\"Source: s3://{BUCKET}/{PATH_OLD}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ôªÔ∏è Overwrite and Verification Configuration\n",
    "\n",
    "Control whether to overwrite existing files and verify processing results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Overwrite and verification configuration loaded\n",
      "Overwrite mode: ENABLED\n",
      "Verification: ENABLED\n",
      "‚ö†Ô∏è  WARNING: Existing files will be overwritten!\n",
      "   This may incur additional processing time and S3 costs.\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# OVERWRITE AND VERIFICATION CONFIGURATION\n",
    "# ========================================\n",
    "\n",
    "# Overwrite Configuration\n",
    "OVERWRITE_EXISTING = True  # Set to True to reprocess and overwrite existing files\n",
    "                            # Set to False to skip existing files (default behavior)\n",
    "\n",
    "# Verification Configuration  \n",
    "VERIFY_PROCESSING = True    # Compare input vs output to verify COG transformation\n",
    "SAVE_VERIFICATION_PLOTS = True  # Save comparison plots for verification\n",
    "VERIFICATION_SAMPLE_SIZE = 5     # Number of files to verify per product type\n",
    "VERIFICATION_DIR = f'verification/{EVENT_NAME}'  # Directory for verification results\n",
    "\n",
    "# Quality Control\n",
    "CHECK_NODATA_PROPAGATION = True  # Verify no-data values are properly handled\n",
    "COMPARE_STATISTICS = True         # Compare min/max/mean between input and output\n",
    "\n",
    "print(\"‚úÖ Overwrite and verification configuration loaded\")\n",
    "print(f\"Overwrite mode: {'ENABLED' if OVERWRITE_EXISTING else 'DISABLED (will skip existing)'}\")\n",
    "print(f\"Verification: {'ENABLED' if VERIFY_PROCESSING else 'DISABLED'}\")\n",
    "if OVERWRITE_EXISTING:\n",
    "    print(\"‚ö†Ô∏è  WARNING: Existing files will be overwritten!\")\n",
    "    print(\"   This may incur additional processing time and S3 costs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Import Required Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Standard libraries imported\n",
      "Module path: /home/jovyan/disasters-aws-conversion\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import gc\n",
    "import tempfile\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Data processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Geospatial libraries\n",
    "import rasterio\n",
    "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
    "from rasterio.windows import Window\n",
    "\n",
    "# AWS libraries\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError, NoCredentialsError\n",
    "\n",
    "# Progress tracking\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"‚úÖ Standard libraries imported\")\n",
    "\n",
    "# Add parent directory to path for module imports\n",
    "module_path = Path('..').resolve()\n",
    "if str(module_path) not in sys.path:\n",
    "    sys.path.insert(0, str(module_path))\n",
    "\n",
    "print(f\"Module path: {module_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'initialize_s3_client' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Initialize S3 client\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m s3_client, fs_read = \u001b[43minitialize_s3_client\u001b[49m(bucket_name=BUCKET, verbose=VERBOSE)\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m s3_client:\n\u001b[32m      5\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m‚úÖ S3 client ready for operations\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'initialize_s3_client' is not defined"
     ]
    }
   ],
   "source": [
    "# Initialize S3 client\n",
    "s3_client, fs_read = initialize_s3_client(bucket_name=BUCKET, verbose=VERBOSE)\n",
    "\n",
    "if s3_client:\n",
    "    print(\"‚úÖ S3 client ready for operations\")\n",
    "else:\n",
    "    print(\"‚ùå Failed to initialize S3 client\")\n",
    "    print(\"Please check your AWS credentials\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîå Initialize AWS S3 Connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Discover Files in S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Found 55 .tif files in s3://nasa-disasters/drcs_activations/202504_SevereWx_US/sentinel2\n",
      "\n",
      "Files:\n",
      "  - JAN_S2A_MNDWI_20250408_merged.tif (0.8 GB)\n",
      "  - JAN_S2A_NDVI_20250322_merged.tif (7.4 GB)\n",
      "  - JAN_S2A_NDVI_20250408_merged.tif (3.2 GB)\n",
      "  - JAN_S2A_trueColor_20250322_merged.tif (5.6 GB)\n",
      "  - JAN_S2A_trueColor_20250408_merged.tif (2.4 GB)\n",
      "  - JAN_S2B_NDVI_20250322_merged.tif (4.5 GB)\n",
      "  - JAN_S2B_trueColor_20250322_merged.tif (3.4 GB)\n",
      "  - JAN_S2C_MNDWI_20250409_merged.tif (1.9 GB)\n",
      "  - JAN_S2C_NDVI_20250409_merged.tif (7.4 GB)\n",
      "  - JAN_S2C_trueColor_20250409_merged.tif (5.6 GB)\n",
      "  - LZK_S2B_MNDWI_20250407_merged.tif (1.9 GB)\n",
      "  - LZK_S2B_NDVI_20250407_merged.tif (7.8 GB)\n",
      "  - LZK_S2B_trueColor_20250407_merged.tif (5.8 GB)\n",
      "  - LZK_S2C_MNDWI_20250409_merged.tif (0.8 GB)\n",
      "  - LZK_S2C_NDVI_20150313_merged.tif (6.3 GB)\n",
      "  - LZK_S2C_NDVI_20250409_merged.tif (3.2 GB)\n",
      "  - LZK_S2C_trueColor_20150313_merged.tif (4.7 GB)\n",
      "  - LZK_S2C_trueColor_20250409_merged.tif (2.4 GB)\n",
      "  - MEG_S2A_MNDWI_20250408_merged.tif (1.2 GB)\n",
      "  - MEG_S2A_NDVI_20250322_merged.tif (7.3 GB)\n",
      "  - MEG_S2A_NDVI_20250408_merged.tif (4.7 GB)\n",
      "  - MEG_S2A_trueColor_20250322_merged.tif (5.5 GB)\n",
      "  - MEG_S2A_trueColor_20250408_merged.tif (3.5 GB)\n",
      "  - MEG_S2B_NDVI_20250322_merged.tif (4.7 GB)\n",
      "  - MEG_S2B_trueColor_20250322_merged.tif (3.5 GB)\n",
      "  - MEG_S2C_MNDWI_20250409_merged.tif (1.8 GB)\n",
      "  - MEG_S2C_NDVI_20250409_merged.tif (7.3 GB)\n",
      "  - MEG_S2C_trueColor_20250409_merged.tif (5.5 GB)\n",
      "  - OHX_S2A_MNDWI_20250408_merged.tif (0.9 GB)\n",
      "  - OHX_S2A_NDVI_20250322_merged.tif (0.9 GB)\n",
      "  - OHX_S2A_NDVI_20250408_merged.tif (3.6 GB)\n",
      "  - OHX_S2A_trueColor_20250322_merged.tif (0.6 GB)\n",
      "  - OHX_S2A_trueColor_20250408_merged.tif (2.7 GB)\n",
      "  - OHX_S2B_NDVI_20250322_merged.tif (3.6 GB)\n",
      "  - OHX_S2B_trueColor_20250322_merged.tif (2.7 GB)\n",
      "  - PAH_S2A_MNDWI_20250408_merged.tif (0.9 GB)\n",
      "  - PAH_S2A_NDVI_20250322_merged.tif (5.2 GB)\n",
      "  - PAH_S2A_NDVI_20250408_merged.tif (3.6 GB)\n",
      "  - PAH_S2A_trueColor_20250322_merged.tif (3.9 GB)\n",
      "  - PAH_S2A_trueColor_20250408_merged.tif (2.7 GB)\n",
      "  - PAH_S2B_NDVI_20250322_merged.tif (2.4 GB)\n",
      "  - PAH_S2B_trueColor_20250322_merged.tif (1.8 GB)\n",
      "  - PAH_S2C_MNDWI_20250409_merged.tif (1.3 GB)\n",
      "  - PAH_S2C_NDVI_20250409_merged.tif (5.2 GB)\n",
      "  - PAH_S2C_trueColor_20250409_merged.tif (3.9 GB)\n",
      "  - SHV_S2A_MNDWI_20250407_merged.tif (1.5 GB)\n",
      "  - SHV_S2A_NDVI_20250407_merged.tif (5.9 GB)\n",
      "  - SHV_S2A_trueColor_20250407_merged.tif (4.4 GB)\n",
      "  - SHV_S2B_MNDWI_20250407_merged.tif (1.9 GB)\n",
      "  - SHV_S2B_NDVI_20250331_merged.tif (5.9 GB)\n",
      "  - SHV_S2B_NDVI_20250407_merged.tif (7.8 GB)\n",
      "  - SHV_S2B_trueColor_20250331_merged.tif (4.4 GB)\n",
      "  - SHV_S2B_trueColor_20250407_merged.tif (5.8 GB)\n",
      "  - SHV_S2C_NDVI_20250313_merged.tif (4.7 GB)\n",
      "  - SHV_S2C_trueColor_20250313_merged.tif (3.5 GB)\n"
     ]
    }
   ],
   "source": [
    "# List all TIF files in the source path\n",
    "if s3_client:\n",
    "    keys = list_s3_files(s3_client, BUCKET, PATH_OLD, suffix='.tif')\n",
    "    print(f\"‚úÖ Found {len(keys)} .tif files in s3://{BUCKET}/{PATH_OLD}\")\n",
    "    \n",
    "    # Show first 5 files as example\n",
    "    if keys:\n",
    "        print(\"\\nFiles:\")\n",
    "        for key in keys:\n",
    "            file_size = get_file_size_from_s3(s3_client, BUCKET, key)\n",
    "            print(f\"  - {os.path.basename(key)} ({file_size:.1f} GB)\")\n",
    "else:\n",
    "    keys = []\n",
    "    print(\"‚ùå No S3 client available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Based on the files that are in the directory, we can now add regex patterns to select specific types of files and move into specific directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - NDVI -> drcs_activations_new/Sentinel-2/NDVI\n",
      "  - MNDWI -> drcs_activations_new/Sentinel-2/MNDWI\n",
      "  - trueColor|truecolor -> drcs_activations_new/Sentinel-2/trueColor\n"
     ]
    }
   ],
   "source": [
    "# Product Type Configuration\n",
    "# Define patterns and output directories for different product types\n",
    "# Modify this dictionary to add/remove product types as needed\n",
    "PRODUCT_CONFIGS = {\n",
    "    # Pattern (regex or string): Output directory relative to DIR_NEW_BASE\n",
    "    'NDVI': 'Sentinel-2/NDVI',\n",
    "    'MNDWI': 'Sentinel-2/MNDWI',\n",
    "    'trueColor|truecolor': 'Sentinel-2/trueColor',  # Multiple patterns with |\n",
    "    # Add more patterns as needed:\n",
    "    # 'SAR': 'Sentinel-1/SAR',\n",
    "    # 'DEM': 'Elevation/DEM',\n",
    "    # 'temperature': 'Climate/Temperature',\n",
    "}\n",
    "\n",
    "\n",
    "for pattern, output_dir in PRODUCT_CONFIGS.items():\n",
    "    print(f\"  - {pattern} -> {DIR_NEW_BASE}/{output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß No-Data Value Configuration\n",
    "\n",
    "Configure how no-data values are handled during processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# NO-DATA VALUE CONFIGURATION\n",
    "# ========================================\n",
    "\n",
    "# Automatic no-data detection\n",
    "USE_AUTO_NODATA = True  # Automatically select appropriate no-data values\n",
    "\n",
    "# Manual no-data values per product type\n",
    "# Set to None to use automatic detection for that product\n",
    "MANUAL_NODATA_VALUES = {\n",
    "    'NDVI': None,       # e.g., -9999 for NDVI\n",
    "    'MNDWI': None,      # e.g., -9999 for MNDWI  \n",
    "    'trueColor_or_truecolor': None,  # e.g., 0 for RGB images\n",
    "    # Add more as needed\n",
    "}\n",
    "\n",
    "# Analysis configuration\n",
    "ANALYZE_BEFORE_PROCESSING = True  # Analyze files to determine min/max before processing\n",
    "VALIDATE_NODATA = True           # Validate that no-data values don't conflict with actual data\n",
    "SHOW_ANALYSIS_REPORT = True      # Display analysis report before processing\n",
    "\n",
    "print(\"‚úÖ No-data configuration loaded\")\n",
    "print(f\"Auto no-data: {USE_AUTO_NODATA}\")\n",
    "print(f\"Manual overrides configured: {sum(v is not None for v in MANUAL_NODATA_VALUES.values())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDVI: 22 files -> Sentinel-2/NDVI\n",
      "MNDWI: 11 files -> Sentinel-2/MNDWI\n",
      "trueColor_or_truecolor: 22 files -> Sentinel-2/trueColor\n",
      "\n",
      "Total files to process: 55\n"
     ]
    }
   ],
   "source": [
    "# Filter files based on configuration\n",
    "\n",
    "# Filter files by configured patterns\n",
    "files_to_process = {}\n",
    "\n",
    "for pattern, output_dir in PRODUCT_CONFIGS.items():\n",
    "    matching_files = []\n",
    "    for file_path in keys:\n",
    "        # Check if pattern matches the filename\n",
    "        if re.search(pattern, file_path):\n",
    "            matching_files.append(file_path)\n",
    "    \n",
    "    if matching_files:\n",
    "        # Use the pattern as key, but clean it for display\n",
    "        clean_name = pattern.replace('|', '_or_')\n",
    "        files_to_process[clean_name] = {\n",
    "            'files': matching_files,\n",
    "            'output_dir': output_dir\n",
    "        }\n",
    "        print(f\"{clean_name}: {len(matching_files)} files -> {output_dir}\")\n",
    "\n",
    "total_files = sum(len(v['files']) for v in files_to_process.values())\n",
    "print(f\"\\nTotal files to process: {total_files}\")\n",
    "\n",
    "# Show summary\n",
    "if not files_to_process:\n",
    "    print(\"‚ö†Ô∏è No files matched the configured patterns!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import disaster-aws-conversion modules\n",
    "try:\n",
    "    # Core modules\n",
    "    from core.s3_operations import (\n",
    "        initialize_s3_client,\n",
    "        check_s3_file_exists,\n",
    "        list_s3_files,\n",
    "        get_file_size_from_s3\n",
    "    )\n",
    "    from core.validation import validate_cog, check_cog_with_warnings\n",
    "    from core.compression import get_predictor_for_dtype, export_cog_profile\n",
    "    \n",
    "    # Utils\n",
    "    from utils.memory_management import get_memory_usage, monitor_memory\n",
    "    from utils.error_handling import cleanup_temp_files\n",
    "    from utils.logging import print_status, print_summary\n",
    "    \n",
    "    # Processors\n",
    "    from processors.batch_processor import process_file_batch, monitor_batch_progress\n",
    "    \n",
    "    # Configs\n",
    "    from configs.profiles import select_profile_by_size\n",
    "    from configs.chunk_configs import get_chunk_config\n",
    "    \n",
    "    # Main processor\n",
    "    from main_processor import convert_to_cog\n",
    "    \n",
    "    print(\"‚úÖ All disaster-aws-conversion modules imported successfully!\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è Import error: {e}\")\n",
    "    print(\"Make sure you're running from the disaster-aws-conversion directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üè∑Ô∏è Manual Filename Generation\n",
    "\n",
    "Define custom filename generation for each product type. Modify these functions to match your specific naming conventions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Filename generation functions defined\n",
      "Available product handlers: ['NDVI', 'MNDWI', 'trueColor_or_truecolor']\n"
     ]
    }
   ],
   "source": [
    "# Manual filename generation functions for each product type\n",
    "# Modify these functions to match your specific naming conventions\n",
    "\n",
    "def extract_date_from_filename(filename):\n",
    "    \"\"\"Extract date from filename in YYYYMMDD format.\"\"\"\n",
    "    dates = re.findall(r'\\d{8}', filename)\n",
    "    if dates:\n",
    "        # Convert YYYYMMDD to YYYY-MM-DD\n",
    "        date_str = dates[0]\n",
    "        return f\"{date_str[0:4]}-{date_str[4:6]}-{date_str[6:8]}\"\n",
    "    return None\n",
    "\n",
    "def create_ndvi_filename(original_path, event_name):\n",
    "    \"\"\"\n",
    "    Create filename for NDVI products.\n",
    "    Example: JAN_S2A_NDVI_20250408_merged.tif -> 202504_SevereWx_US_JAN_S2A_NDVI_merged_2025-04-08_day.tif\n",
    "    \"\"\"\n",
    "    filename = os.path.basename(original_path)\n",
    "    stem = os.path.splitext(filename)[0]\n",
    "    \n",
    "    # Extract components\n",
    "    date = extract_date_from_filename(stem)\n",
    "    \n",
    "    # Remove date and _merged from stem\n",
    "    stem_clean = re.sub(r'_?\\d{8}', '', stem)\n",
    "    \n",
    "    # Build filename\n",
    "    if date:\n",
    "        cog_filename = f\"{event_name}_{stem_clean}_{date}_day.tif\"\n",
    "    else:\n",
    "        cog_filename = f\"{event_name}_{stem_clean}_day.tif\"\n",
    "    \n",
    "    # Clean up double underscores\n",
    "    cog_filename = re.sub(r'_+', '_', cog_filename)\n",
    "    \n",
    "    return cog_filename\n",
    "\n",
    "def create_mndwi_filename(original_path, event_name):\n",
    "    \"\"\"\n",
    "    Create filename for MNDWI products.\n",
    "    Example: JAN_S2A_MNDWI_20250408_merged.tif -> 202504_SevereWx_US_JAN_S2A_MNDWI_2025-04-08_day.tif\n",
    "    \"\"\"\n",
    "    filename = os.path.basename(original_path)\n",
    "    stem = os.path.splitext(filename)[0]\n",
    "    \n",
    "    # Extract components\n",
    "    date = extract_date_from_filename(stem)\n",
    "    \n",
    "    # Remove date and _merged from stem\n",
    "    stem_clean = re.sub(r'_?\\d{8}', '', stem)\n",
    "    stem_clean = stem_clean.replace('_merged', '')\n",
    "    \n",
    "    # Build filename\n",
    "    if date:\n",
    "        cog_filename = f\"{event_name}_{stem_clean}_{date}_day.tif\"\n",
    "    else:\n",
    "        cog_filename = f\"{event_name}_{stem_clean}_day.tif\"\n",
    "    \n",
    "    # Clean up double underscores\n",
    "    cog_filename = re.sub(r'_+', '_', cog_filename)\n",
    "    \n",
    "    return cog_filename\n",
    "\n",
    "def create_truecolor_filename(original_path, event_name):\n",
    "    \"\"\"\n",
    "    Create filename for trueColor products.\n",
    "    Example: JAN_S2A_trueColor_20250408_merged.tif -> 202504_SevereWx_US_JAN_S2A_trueColor_2025-04-08_day.tif\n",
    "    \"\"\"\n",
    "    filename = os.path.basename(original_path)\n",
    "    stem = os.path.splitext(filename)[0]\n",
    "    \n",
    "    # Extract components\n",
    "    date = extract_date_from_filename(stem)\n",
    "    \n",
    "    # Remove date and _merged from stem\n",
    "    stem_clean = re.sub(r'_?\\d{8}', '', stem)\n",
    "    stem_clean = stem_clean.replace('_merged', '')\n",
    "    \n",
    "    # Build filename\n",
    "    if date:\n",
    "        cog_filename = f\"{event_name}_{stem_clean}_{date}_day.tif\"\n",
    "    else:\n",
    "        cog_filename = f\"{event_name}_{stem_clean}_day.tif\"\n",
    "    \n",
    "    # Clean up double underscores\n",
    "    cog_filename = re.sub(r'_+', '_', cog_filename)\n",
    "    \n",
    "    return cog_filename\n",
    "\n",
    "def create_generic_filename(original_path, event_name):\n",
    "    \"\"\"\n",
    "    Create generic filename for any product type.\n",
    "    Falls back to this if no specific handler is defined.\n",
    "    \"\"\"\n",
    "    filename = os.path.basename(original_path)\n",
    "    stem = os.path.splitext(filename)[0]\n",
    "    \n",
    "    # Extract components\n",
    "    date = extract_date_from_filename(stem)\n",
    "    \n",
    "    # Remove date and _merged from stem\n",
    "    stem_clean = re.sub(r'_?\\d{8}', '', stem)\n",
    "    stem_clean = stem_clean.replace('_merged', '')\n",
    "    \n",
    "    # Build filename\n",
    "    if date:\n",
    "        cog_filename = f\"{event_name}_{stem_clean}_{date}_day.tif\"\n",
    "    else:\n",
    "        cog_filename = f\"{event_name}_{stem_clean}_day.tif\"\n",
    "    \n",
    "    # Clean up double underscores\n",
    "    cog_filename = re.sub(r'_+', '_', cog_filename)\n",
    "    \n",
    "    return cog_filename\n",
    "\n",
    "\n",
    "\n",
    "# Mapping of product types to their filename creators\n",
    "FILENAME_CREATORS = {\n",
    "    'NDVI': create_ndvi_filename,\n",
    "    'MNDWI': create_mndwi_filename,\n",
    "    'trueColor_or_truecolor': create_truecolor_filename,\n",
    "    # Add more mappings as needed:\n",
    "    # 'SAR': create_sar_filename,\n",
    "    # 'DEM': create_dem_filename,\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Filename generation functions defined\")\n",
    "print(\"Available product handlers:\", list(FILENAME_CREATORS.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù Preview Filename Transformations\n",
    "\n",
    "Review how your files will be renamed before processing begins:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview filename transformations for each product type\n",
    "print(\"=\" * 80)\n",
    "print(\"üìã FILENAME TRANSFORMATION PREVIEW\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Show sample transformations for each product type\n",
    "for product_name, product_info in files_to_process.items():\n",
    "    print(f\"\\nüîπ {product_name} Files:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Get the appropriate filename creator\n",
    "    filename_creator = FILENAME_CREATORS.get(product_name, create_generic_filename)\n",
    "    \n",
    "    # Show first 3 files as examples (or all if less than 3)\n",
    "    sample_files = product_info['files'][:min(3, len(product_info['files']))]\n",
    "    \n",
    "    for file_path in sample_files:\n",
    "        original = os.path.basename(file_path)\n",
    "        transformed = filename_creator(file_path, EVENT_NAME)\n",
    "        \n",
    "        print(f\"  Original:  {original}\")\n",
    "        print(f\"  ‚Üí New:     {transformed}\")\n",
    "        print()\n",
    "    \n",
    "    # Show count of remaining files\n",
    "    remaining = len(product_info['files']) - len(sample_files)\n",
    "    if remaining > 0:\n",
    "        print(f\"  ... and {remaining} more files\")\n",
    "        print()\n",
    "\n",
    "# Summary of naming pattern\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìå NAMING PATTERN SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\"\"\n",
    "The naming convention follows this pattern:\n",
    "  {EVENT_NAME}_[Location]_[Satellite]_[Product]_[Date]_day.tif\n",
    "\n",
    "Where:\n",
    "  - Event: {EVENT_NAME}\n",
    "  - Location: 3-letter code (e.g., JAN, LZK, MEG)\n",
    "  - Satellite: S2A, S2B, S2C, etc.\n",
    "  - Product: NDVI, MNDWI, trueColor\n",
    "  - Date: YYYY-MM-DD format\n",
    "  - Suffix: 'day' (customizable in functions)\n",
    "\n",
    "Example transformations:\n",
    "  JAN_S2A_NDVI_20250408_merged.tif \n",
    "  ‚Üí {EVENT_NAME}_JAN_S2A_NDVI_2025-04-08_day.tif\n",
    "\"\"\")\n",
    "\n",
    "# Ask for confirmation\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ Review the filename transformations above.\")\n",
    "print(\"   If you need to adjust the naming pattern, modify the\")\n",
    "print(\"   create_*_filename() functions in the previous cell.\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview S3 destination paths\n",
    "print(\"=\" * 80)\n",
    "print(\"üóÇÔ∏è  S3 DESTINATION PATHS PREVIEW\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for product_name, product_info in files_to_process.items():\n",
    "    output_dir = product_info['output_dir']\n",
    "    print(f\"\\nüî∏ {product_name} files will be saved to:\")\n",
    "    print(f\"   s3://{BUCKET}/{DIR_NEW_BASE}/{output_dir}/\")\n",
    "    \n",
    "    # Show one example with full path\n",
    "    if product_info['files']:\n",
    "        filename_creator = FILENAME_CREATORS.get(product_name, create_generic_filename)\n",
    "        sample_file = product_info['files'][0]\n",
    "        sample_filename = filename_creator(sample_file, EVENT_NAME)\n",
    "        \n",
    "        print(f\"\\n   Example full S3 path:\")\n",
    "        print(f\"   s3://{BUCKET}/{DIR_NEW_BASE}/{output_dir}/{sample_filename}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìä PROCESSING SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Total files to process: {total_files}\")\n",
    "print(f\"Event name: {EVENT_NAME}\")\n",
    "print(f\"Source bucket: s3://{BUCKET}/{PATH_OLD}\")\n",
    "print(f\"Destination base: s3://{BUCKET}/{DIR_NEW_BASE}/\")\n",
    "print(\"\\nProduct breakdown:\")\n",
    "for product_name, product_info in files_to_process.items():\n",
    "    print(f\"  ‚Ä¢ {product_name}: {len(product_info['files'])} files\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Pre-Processing Analysis\n",
    "\n",
    "Analyze sample files to understand data ranges and validate no-data configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_files_by_type(file_list, product_name, output_dir, event_name, s3_client):\n",
    "    \"\"\"\n",
    "    Process a list of files for a specific product type.\n",
    "    \n",
    "    Args:\n",
    "        file_list: List of S3 keys to process\n",
    "        product_name: Name/identifier for this batch of files (e.g., 'NDVI', 'MNDWI', 'trueColor_or_truecolor')\n",
    "        output_dir: Target output directory\n",
    "        event_name: Event name for output naming\n",
    "        s3_client: S3 client\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with processing results\n",
    "    \"\"\"\n",
    "    if not file_list:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üöÄ Processing {product_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Configuration for batch processing\n",
    "    config = {\n",
    "        'raw_data_bucket': BUCKET,\n",
    "        'raw_data_prefix': PATH_OLD,\n",
    "        'cog_data_bucket': BUCKET,\n",
    "        'cog_data_prefix': f'{DIR_NEW_BASE}/{output_dir}',\n",
    "        'local_output_dir': f'output/{event_name}/{product_name}' if SAVE_LOCAL else None\n",
    "    }\n",
    "    \n",
    "    print_status(f\"{product_name} Processing Configuration\", config)\n",
    "    \n",
    "    # Get the appropriate filename creator for this product type\n",
    "    filename_creator = FILENAME_CREATORS.get(product_name, create_generic_filename)\n",
    "    \n",
    "    # Get manual no-data value for this product type\n",
    "    manual_nodata = MANUAL_NODATA_VALUES.get(product_name)\n",
    "    if manual_nodata is not None:\n",
    "        print(f\"   üìå Using manual no-data value: {manual_nodata}\")\n",
    "    else:\n",
    "        print(f\"   üîÑ Using automatic no-data selection\")\n",
    "    \n",
    "    # Show overwrite status\n",
    "    if OVERWRITE_EXISTING:\n",
    "        print(f\"   ‚ôªÔ∏è  OVERWRITE MODE: Existing files will be replaced\")\n",
    "    else:\n",
    "        print(f\"   ‚è≠Ô∏è  SKIP MODE: Existing files will be skipped\")\n",
    "    \n",
    "    # Create local output directory if needed\n",
    "    if SAVE_LOCAL and config['local_output_dir']:\n",
    "        os.makedirs(config['local_output_dir'], exist_ok=True)\n",
    "    \n",
    "    # Process each file\n",
    "    results = []\n",
    "    verification_queue = []  # Files to verify after processing\n",
    "    \n",
    "    for file_path in tqdm(file_list, desc=f\"Processing {product_name}\"):\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        try:\n",
    "            # Generate COG filename manually using the appropriate function\n",
    "            cog_filename = filename_creator(file_path, event_name)\n",
    "            \n",
    "            # Check if file already exists\n",
    "            output_key = f\"{config['cog_data_prefix']}/{cog_filename}\"\n",
    "            file_exists = check_s3_file_exists(s3_client, config['cog_data_bucket'], output_key)\n",
    "            \n",
    "            # Handle existing files based on OVERWRITE_EXISTING flag\n",
    "            if file_exists and not OVERWRITE_EXISTING:\n",
    "                print(f\"   ‚è≠Ô∏è  Skipping {os.path.basename(file_path)} - already exists\")\n",
    "                results.append({\n",
    "                    'original_file': file_path,\n",
    "                    'output_file': cog_filename,\n",
    "                    'status': 'skipped',\n",
    "                    'reason': 'File already exists',\n",
    "                    'processing_time_s': 0,\n",
    "                    'timestamp': datetime.now().isoformat()\n",
    "                })\n",
    "                continue\n",
    "            elif file_exists and OVERWRITE_EXISTING:\n",
    "                print(f\"   ‚ôªÔ∏è  Overwriting existing file: {cog_filename}\")\n",
    "            \n",
    "            print(f\"\\nüìÑ Processing: {os.path.basename(file_path)}\")\n",
    "            print(f\"   ‚Üí Output: {cog_filename}\")\n",
    "            \n",
    "            # Get file size to determine configuration\n",
    "            file_size_gb = get_file_size_from_s3(s3_client, BUCKET, file_path)\n",
    "            \n",
    "            # Select configuration based on size\n",
    "            if file_size_gb > ULTRA_LARGE_THRESHOLD:\n",
    "                print(f\"   üì¶ Ultra-large file ({file_size_gb:.1f} GB), using fixed 128x128 chunks\")\n",
    "            elif file_size_gb > LARGE_FILE_THRESHOLD:\n",
    "                print(f\"   üì¶ Large file ({file_size_gb:.1f} GB), using fixed 256x256 chunks\")\n",
    "            else:\n",
    "                print(f\"   üì¶ Standard file ({file_size_gb:.1f} GB), using adaptive chunks\")\n",
    "            \n",
    "            # Get chunk configuration\n",
    "            chunk_config = get_chunk_config(\n",
    "                file_size_gb=file_size_gb,\n",
    "                memory_limit_mb=MEMORY_LIMIT_MB\n",
    "            )\n",
    "            \n",
    "            # Override streaming setting\n",
    "            chunk_config['use_streaming'] = USE_STREAMING\n",
    "            \n",
    "            # Call main processor with manual no-data if configured\n",
    "            result = convert_to_cog(\n",
    "                name=file_path,\n",
    "                bucket=BUCKET,\n",
    "                cog_filename=cog_filename,\n",
    "                cog_data_bucket=config['cog_data_bucket'],\n",
    "                cog_data_prefix=config['cog_data_prefix'],\n",
    "                s3_client=s3_client,\n",
    "                local_output_dir=config['local_output_dir'],\n",
    "                chunk_config=chunk_config,\n",
    "                manual_nodata=manual_nodata  # Pass manual no-data value\n",
    "            )\n",
    "            \n",
    "            results.append({\n",
    "                'original_file': file_path,\n",
    "                'output_file': cog_filename,\n",
    "                'output_key': output_key,\n",
    "                'status': 'success',\n",
    "                'result': result,\n",
    "                'processing_time_s': (datetime.now() - start_time).total_seconds(),\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            })\n",
    "            \n",
    "            # Add to verification queue if enabled\n",
    "            if VERIFY_PROCESSING and len(verification_queue) < VERIFICATION_SAMPLE_SIZE:\n",
    "                verification_queue.append({\n",
    "                    'input_key': file_path,\n",
    "                    'output_key': output_key,\n",
    "                    'filename': cog_filename\n",
    "                })\n",
    "            \n",
    "            print(f\"   ‚úÖ Successfully processed in {(datetime.now() - start_time).total_seconds():.1f}s\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            results.append({\n",
    "                'original_file': file_path,\n",
    "                'output_file': cog_filename if 'cog_filename' in locals() else None,\n",
    "                'status': 'failed',\n",
    "                'error': str(e),\n",
    "                'processing_time_s': (datetime.now() - start_time).total_seconds(),\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            })\n",
    "            \n",
    "            print(f\"   ‚ùå Error processing {file_path}: {e}\")\n",
    "            if VERBOSE:\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "    \n",
    "    # Create results DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Monitor results\n",
    "    monitor_batch_progress(results_df)\n",
    "    \n",
    "    # Run verification if enabled\n",
    "    if VERIFY_PROCESSING and verification_queue:\n",
    "        print(f\"\\nüîç Verifying {len(verification_queue)} processed files...\")\n",
    "        verification_dir = os.path.join(VERIFICATION_DIR, product_name)\n",
    "        os.makedirs(verification_dir, exist_ok=True)\n",
    "        \n",
    "        from tools.verification import verify_s3_files, create_verification_report\n",
    "        \n",
    "        verification_results = []\n",
    "        for item in verification_queue:\n",
    "            try:\n",
    "                result = verify_s3_files(\n",
    "                    BUCKET, item['input_key'],\n",
    "                    BUCKET, item['output_key'],\n",
    "                    verification_dir, s3_client\n",
    "                )\n",
    "                verification_results.append(result)\n",
    "                print(f\"   ‚úì Verified: {item['filename']}\")\n",
    "            except Exception as e:\n",
    "                print(f\"   ‚úó Verification failed for {item['filename']}: {e}\")\n",
    "        \n",
    "        # Create verification report\n",
    "        if verification_results:\n",
    "            report_path = os.path.join(verification_dir, 'verification_report.json')\n",
    "            create_verification_report(verification_results, report_path)\n",
    "    \n",
    "    # Save results if requested\n",
    "    if SAVE_METADATA and not results_df.empty and config['local_output_dir']:\n",
    "        csv_filename = f\"{config['local_output_dir']}/processing_results_{product_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "        results_df.to_csv(csv_filename, index=False)\n",
    "        print(f\"\\nüìä Results saved to: {csv_filename}\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "print(\"‚úÖ Processing functions updated with overwrite and verification support\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Define Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Processing functions defined with manual filename generation\n"
     ]
    }
   ],
   "source": [
    "def process_files_by_type(file_list, product_name, output_dir, event_name, s3_client):\n",
    "    \"\"\"\n",
    "    Process a list of files for a specific product type.\n",
    "    \n",
    "    Args:\n",
    "        file_list: List of S3 keys to process\n",
    "        product_name: Name/identifier for this batch of files (e.g., 'NDVI', 'MNDWI', 'trueColor_or_truecolor')\n",
    "        output_dir: Target output directory\n",
    "        event_name: Event name for output naming\n",
    "        s3_client: S3 client\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with processing results\n",
    "    \"\"\"\n",
    "    if not file_list:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üöÄ Processing {product_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Configuration for batch processing\n",
    "    config = {\n",
    "        'raw_data_bucket': BUCKET,\n",
    "        'raw_data_prefix': PATH_OLD,\n",
    "        'cog_data_bucket': BUCKET,\n",
    "        'cog_data_prefix': f'{DIR_NEW_BASE}/{output_dir}',\n",
    "        'local_output_dir': f'output/{event_name}/{product_name}' if SAVE_LOCAL else None\n",
    "    }\n",
    "    \n",
    "    print_status(f\"{product_name} Processing Configuration\", config)\n",
    "    \n",
    "    # Get the appropriate filename creator for this product type\n",
    "    filename_creator = FILENAME_CREATORS.get(product_name, create_generic_filename)\n",
    "    \n",
    "    # Create local output directory if needed\n",
    "    if SAVE_LOCAL and config['local_output_dir']:\n",
    "        os.makedirs(config['local_output_dir'], exist_ok=True)\n",
    "    \n",
    "    # Process each file\n",
    "    results = []\n",
    "    for file_path in tqdm(file_list, desc=f\"Processing {product_name}\"):\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        try:\n",
    "            # Generate COG filename manually using the appropriate function\n",
    "            cog_filename = filename_creator(file_path, event_name)\n",
    "            \n",
    "            # Check if file already exists\n",
    "            output_key = f\"{config['cog_data_prefix']}/{cog_filename}\"\n",
    "            \n",
    "            if check_s3_file_exists(s3_client, config['cog_data_bucket'], output_key):\n",
    "                print(f\"   ‚è≠Ô∏è  Skipping {os.path.basename(file_path)} - already exists\")\n",
    "                results.append({\n",
    "                    'original_file': file_path,\n",
    "                    'output_file': cog_filename,\n",
    "                    'status': 'skipped',\n",
    "                    'reason': 'File already exists',\n",
    "                    'processing_time_s': 0,\n",
    "                    'timestamp': datetime.now().isoformat()\n",
    "                })\n",
    "                continue\n",
    "            \n",
    "            print(f\"\\nüìÑ Processing: {os.path.basename(file_path)}\")\n",
    "            print(f\"   ‚Üí Output: {cog_filename}\")\n",
    "            \n",
    "            # Get file size to determine configuration\n",
    "            file_size_gb = get_file_size_from_s3(s3_client, BUCKET, file_path)\n",
    "            \n",
    "            # Select configuration based on size\n",
    "            if file_size_gb > ULTRA_LARGE_THRESHOLD:\n",
    "                print(f\"   üì¶ Ultra-large file ({file_size_gb:.1f} GB), using fixed 128x128 chunks\")\n",
    "            elif file_size_gb > LARGE_FILE_THRESHOLD:\n",
    "                print(f\"   üì¶ Large file ({file_size_gb:.1f} GB), using fixed 256x256 chunks\")\n",
    "            else:\n",
    "                print(f\"   üì¶ Standard file ({file_size_gb:.1f} GB), using adaptive chunks\")\n",
    "            \n",
    "            # Get chunk configuration\n",
    "            chunk_config = get_chunk_config(\n",
    "                file_size_gb=file_size_gb,\n",
    "                memory_limit_mb=MEMORY_LIMIT_MB\n",
    "            )\n",
    "            \n",
    "            # Override streaming setting\n",
    "            chunk_config['use_streaming'] = USE_STREAMING\n",
    "            \n",
    "            # Call main processor\n",
    "            result = convert_to_cog(\n",
    "                name=file_path,\n",
    "                bucket=BUCKET,\n",
    "                cog_filename=cog_filename,\n",
    "                cog_data_bucket=config['cog_data_bucket'],\n",
    "                cog_data_prefix=config['cog_data_prefix'],\n",
    "                s3_client=s3_client,\n",
    "                local_output_dir=config['local_output_dir'],\n",
    "                chunk_config=chunk_config\n",
    "            )\n",
    "            \n",
    "            results.append({\n",
    "                'original_file': file_path,\n",
    "                'output_file': cog_filename,\n",
    "                'status': 'success',\n",
    "                'result': result,\n",
    "                'processing_time_s': (datetime.now() - start_time).total_seconds(),\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            })\n",
    "            \n",
    "            print(f\"   ‚úÖ Successfully processed in {(datetime.now() - start_time).total_seconds():.1f}s\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            results.append({\n",
    "                'original_file': file_path,\n",
    "                'output_file': cog_filename if 'cog_filename' in locals() else None,\n",
    "                'status': 'failed',\n",
    "                'error': str(e),\n",
    "                'processing_time_s': (datetime.now() - start_time).total_seconds(),\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            })\n",
    "            \n",
    "            print(f\"   ‚ùå Error processing {file_path}: {e}\")\n",
    "            if VERBOSE:\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "    \n",
    "    # Create results DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Monitor results\n",
    "    monitor_batch_progress(results_df)\n",
    "    \n",
    "    # Save results if requested\n",
    "    if SAVE_METADATA and not results_df.empty and config['local_output_dir']:\n",
    "        csv_filename = f\"{config['local_output_dir']}/processing_results_{product_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "        results_df.to_csv(csv_filename, index=False)\n",
    "        print(f\"\\nüìä Results saved to: {csv_filename}\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "print(\"‚úÖ Processing functions defined with manual filename generation\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Execute Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting processing at 2025-09-27 00:05:07\n",
      "Memory usage at start: 232.6 MB\n"
     ]
    }
   ],
   "source": [
    "# Initialize results storage\n",
    "all_results = []\n",
    "processing_start = datetime.now()\n",
    "\n",
    "print(f\"Starting processing at {processing_start.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Memory usage at start: {get_memory_usage():.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üöÄ Processing NDVI\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "NDVI Processing Configuration\n",
      "============================================================\n",
      "  raw_data_bucket: nasa-disasters\n",
      "  raw_data_prefix: drcs_activations/202504_SevereWx_US/sentinel2\n",
      "  cog_data_bucket: nasa-disasters\n",
      "  cog_data_prefix: drcs_activations_new/Sentinel-2/NDVI\n",
      "  local_output_dir: output/202504_SevereWx_US/NDVI\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing NDVI:  36%|‚ñà‚ñà‚ñà‚ñã      | 8/22 [00:00<00:00, 79.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚è≠Ô∏è  Skipping JAN_S2A_NDVI_20250322_merged.tif - already exists\n",
      "   ‚è≠Ô∏è  Skipping JAN_S2A_NDVI_20250408_merged.tif - already exists\n",
      "   ‚è≠Ô∏è  Skipping JAN_S2B_NDVI_20250322_merged.tif - already exists\n",
      "   ‚è≠Ô∏è  Skipping JAN_S2C_NDVI_20250409_merged.tif - already exists\n",
      "   ‚è≠Ô∏è  Skipping LZK_S2B_NDVI_20250407_merged.tif - already exists\n",
      "   ‚è≠Ô∏è  Skipping LZK_S2C_NDVI_20150313_merged.tif - already exists\n",
      "   ‚è≠Ô∏è  Skipping LZK_S2C_NDVI_20250409_merged.tif - already exists\n",
      "   ‚è≠Ô∏è  Skipping MEG_S2A_NDVI_20250322_merged.tif - already exists\n",
      "   ‚è≠Ô∏è  Skipping MEG_S2A_NDVI_20250408_merged.tif - already exists\n",
      "   ‚è≠Ô∏è  Skipping MEG_S2B_NDVI_20250322_merged.tif - already exists\n",
      "   ‚è≠Ô∏è  Skipping MEG_S2C_NDVI_20250409_merged.tif - already exists\n",
      "   ‚è≠Ô∏è  Skipping OHX_S2A_NDVI_20250322_merged.tif - already exists\n",
      "   ‚è≠Ô∏è  Skipping OHX_S2A_NDVI_20250408_merged.tif - already exists\n",
      "   ‚è≠Ô∏è  Skipping OHX_S2B_NDVI_20250322_merged.tif - already exists\n",
      "   ‚è≠Ô∏è  Skipping PAH_S2A_NDVI_20250322_merged.tif - already exists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing NDVI:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 16/22 [00:00<00:00, 76.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚è≠Ô∏è  Skipping PAH_S2A_NDVI_20250408_merged.tif - already exists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing NDVI: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22/22 [00:00<00:00, 77.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚è≠Ô∏è  Skipping PAH_S2B_NDVI_20250322_merged.tif - already exists\n",
      "   ‚è≠Ô∏è  Skipping PAH_S2C_NDVI_20250409_merged.tif - already exists\n",
      "   ‚è≠Ô∏è  Skipping SHV_S2A_NDVI_20250407_merged.tif - already exists\n",
      "   ‚è≠Ô∏è  Skipping SHV_S2B_NDVI_20250331_merged.tif - already exists\n",
      "   ‚è≠Ô∏è  Skipping SHV_S2B_NDVI_20250407_merged.tif - already exists\n",
      "   ‚è≠Ô∏è  Skipping SHV_S2C_NDVI_20250313_merged.tif - already exists\n",
      "\n",
      "============================================================\n",
      "BATCH PROCESSING SUMMARY\n",
      "============================================================\n",
      "  total: 22\n",
      "  skipped: 22\n",
      "  success_rate: 0.00\n",
      "  total_time_minutes: 0.00\n",
      "  avg_time_seconds: 0.00\n",
      "============================================================\n",
      "\n",
      "üìä Results saved to: output/202504_SevereWx_US/NDVI/processing_results_NDVI_20250927_000508.csv\n",
      "\n",
      "============================================================\n",
      "üöÄ Processing MNDWI\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "MNDWI Processing Configuration\n",
      "============================================================\n",
      "  raw_data_bucket: nasa-disasters\n",
      "  raw_data_prefix: drcs_activations/202504_SevereWx_US/sentinel2\n",
      "  cog_data_bucket: nasa-disasters\n",
      "  cog_data_prefix: drcs_activations_new/Sentinel-2/MNDWI\n",
      "  local_output_dir: output/202504_SevereWx_US/MNDWI\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing MNDWI:   0%|          | 0/11 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÑ Processing: JAN_S2A_MNDWI_20250408_merged.tif\n",
      "   ‚Üí Output: 202504_SevereWx_US_JAN_S2A_MNDWI_2025-04-08_day.tif\n",
      "   üì¶ Standard file (0.8 GB), using adaptive chunks\n",
      "   [CHECK] Checking if file already exists in S3: s3://nasa-disasters/drcs_activations_new/Sentinel-2/MNDWI/202504_SevereWx_US_JAN_S2A_MNDWI_2025-04-08_day.tif\n",
      "   [INFO] File size: 0.8 GB\n",
      "   [TEMP] Using temp directory: /tmp\n",
      "   [MEMORY] Initial: 233.5 MB, Available: 112866.6 MB\n",
      "   [CACHE HIT] Using cached file: data_download/drcs_activations/202504_SevereWx_US/sentinel2/JAN_S2A_MNDWI_20250408_merged.tif\n",
      "   [CHUNKS] Using adaptive chunk size starting at: 1024x1024\n",
      "   [REPROJECT] Converting to EPSG:4326 using fixed-grid chunked processing...\n",
      "   [ERROR] attempted relative import beyond top-level package\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_3875/49338150.py\", line 88, in process_files_by_type\n",
      "    result = convert_to_cog(\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/jovyan/disasters-aws-conversion/main_processor.py\", line 164, in convert_to_cog\n",
      "    process_with_fixed_chunks(\n",
      "  File \"/home/jovyan/disasters-aws-conversion/core/reprojection.py\", line 92, in process_with_fixed_chunks\n",
      "    from ..utils.memory_management import get_memory_usage\n",
      "ImportError: attempted relative import beyond top-level package\n",
      "Processing MNDWI:   9%|‚ñâ         | 1/11 [00:00<00:02,  4.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚ùå Error processing drcs_activations/202504_SevereWx_US/sentinel2/JAN_S2A_MNDWI_20250408_merged.tif: attempted relative import beyond top-level package\n",
      "\n",
      "üìÑ Processing: JAN_S2C_MNDWI_20250409_merged.tif\n",
      "   ‚Üí Output: 202504_SevereWx_US_JAN_S2C_MNDWI_2025-04-09_day.tif\n",
      "   üì¶ Standard file (1.9 GB), using adaptive chunks\n",
      "   [CHECK] Checking if file already exists in S3: s3://nasa-disasters/drcs_activations_new/Sentinel-2/MNDWI/202504_SevereWx_US_JAN_S2C_MNDWI_2025-04-09_day.tif\n",
      "   [INFO] File size: 1.9 GB\n",
      "   [TEMP] Using temp directory: /tmp\n",
      "   [MEMORY] Initial: 250.6 MB, Available: 112861.2 MB\n",
      "   [DOWNLOAD] Downloading from S3...\n",
      "   [DOWNLOAD] Downloading from S3: s3://nasa-disasters/drcs_activations/202504_SevereWx_US/sentinel2/JAN_S2C_MNDWI_20250409_merged.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing MNDWI:   9%|‚ñâ         | 1/11 [00:15<02:31, 15.19s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Process each product type\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m product_name, product_info \u001b[38;5;129;01min\u001b[39;00m files_to_process.items():\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     results = \u001b[43mprocess_files_by_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfile_list\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproduct_info\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mfiles\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproduct_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproduct_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproduct_info\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43moutput_dir\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[43mevent_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mEVENT_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[43ms3_client\u001b[49m\u001b[43m=\u001b[49m\u001b[43ms3_client\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m results.empty:\n\u001b[32m     12\u001b[39m         all_results.append((product_name, results))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 88\u001b[39m, in \u001b[36mprocess_files_by_type\u001b[39m\u001b[34m(file_list, product_name, output_dir, event_name, s3_client)\u001b[39m\n\u001b[32m     85\u001b[39m chunk_config[\u001b[33m'\u001b[39m\u001b[33muse_streaming\u001b[39m\u001b[33m'\u001b[39m] = USE_STREAMING\n\u001b[32m     87\u001b[39m \u001b[38;5;66;03m# Call main processor\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m result = \u001b[43mconvert_to_cog\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     89\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbucket\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBUCKET\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     91\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcog_filename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcog_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcog_data_bucket\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcog_data_bucket\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcog_data_prefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcog_data_prefix\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m    \u001b[49m\u001b[43ms3_client\u001b[49m\u001b[43m=\u001b[49m\u001b[43ms3_client\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_output_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlocal_output_dir\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunk_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunk_config\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     99\u001b[39m results.append({\n\u001b[32m    100\u001b[39m     \u001b[33m'\u001b[39m\u001b[33moriginal_file\u001b[39m\u001b[33m'\u001b[39m: file_path,\n\u001b[32m    101\u001b[39m     \u001b[33m'\u001b[39m\u001b[33moutput_file\u001b[39m\u001b[33m'\u001b[39m: cog_filename,\n\u001b[32m   (...)\u001b[39m\u001b[32m    105\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mtimestamp\u001b[39m\u001b[33m'\u001b[39m: datetime.now().isoformat()\n\u001b[32m    106\u001b[39m })\n\u001b[32m    108\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   ‚úÖ Successfully processed in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(datetime.now()\u001b[38;5;250m \u001b[39m-\u001b[38;5;250m \u001b[39mstart_time).total_seconds()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33ms\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/disasters-aws-conversion/main_processor.py:116\u001b[39m, in \u001b[36mconvert_to_cog\u001b[39m\u001b[34m(name, bucket, cog_filename, cog_data_bucket, cog_data_prefix, s3_client, cog_profile, local_output_dir, chunk_config)\u001b[39m\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    115\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   [DOWNLOAD] Downloading from S3...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mdownload_from_s3\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms3_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbucket\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_download_path\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    117\u001b[39m         input_path = local_download_path\n\u001b[32m    118\u001b[39m         temp_files.append(local_download_path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/disasters-aws-conversion/core/s3_operations.py:104\u001b[39m, in \u001b[36mdownload_from_s3\u001b[39m\u001b[34m(s3_client, bucket, key, local_path, verbose)\u001b[39m\n\u001b[32m    101\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[32m    102\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   [DOWNLOAD] Downloading from S3: s3://\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbucket\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m \u001b[43ms3_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdownload_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbucket\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[32m    107\u001b[39m     file_size_mb = os.path.getsize(local_path) / (\u001b[32m1024\u001b[39m * \u001b[32m1024\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/srv/conda/envs/notebook/lib/python3.12/site-packages/botocore/context.py:123\u001b[39m, in \u001b[36mwith_current_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m hook:\n\u001b[32m    122\u001b[39m     hook()\n\u001b[32m--> \u001b[39m\u001b[32m123\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/srv/conda/envs/notebook/lib/python3.12/site-packages/boto3/s3/inject.py:223\u001b[39m, in \u001b[36mdownload_file\u001b[39m\u001b[34m(self, Bucket, Key, Filename, ExtraArgs, Callback, Config)\u001b[39m\n\u001b[32m    188\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Download an S3 object to a file.\u001b[39;00m\n\u001b[32m    189\u001b[39m \n\u001b[32m    190\u001b[39m \u001b[33;03mUsage::\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    220\u001b[39m \u001b[33;03m    transfer.\u001b[39;00m\n\u001b[32m    221\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    222\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m S3Transfer(\u001b[38;5;28mself\u001b[39m, Config) \u001b[38;5;28;01mas\u001b[39;00m transfer:\n\u001b[32m--> \u001b[39m\u001b[32m223\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtransfer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdownload_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    224\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbucket\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBucket\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    225\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m=\u001b[49m\u001b[43mKey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    226\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mFilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    227\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43mExtraArgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    228\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    229\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/srv/conda/envs/notebook/lib/python3.12/site-packages/boto3/s3/transfer.py:406\u001b[39m, in \u001b[36mS3Transfer.download_file\u001b[39m\u001b[34m(self, bucket, key, filename, extra_args, callback)\u001b[39m\n\u001b[32m    402\u001b[39m future = \u001b[38;5;28mself\u001b[39m._manager.download(\n\u001b[32m    403\u001b[39m     bucket, key, filename, extra_args, subscribers\n\u001b[32m    404\u001b[39m )\n\u001b[32m    405\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m406\u001b[39m     \u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[38;5;66;03m# This is for backwards compatibility where when retries are\u001b[39;00m\n\u001b[32m    408\u001b[39m \u001b[38;5;66;03m# exceeded we need to throw the same error from boto3 instead of\u001b[39;00m\n\u001b[32m    409\u001b[39m \u001b[38;5;66;03m# s3transfer's built in RetriesExceededError as current users are\u001b[39;00m\n\u001b[32m    410\u001b[39m \u001b[38;5;66;03m# catching the boto3 one instead of the s3transfer exception to do\u001b[39;00m\n\u001b[32m    411\u001b[39m \u001b[38;5;66;03m# their own retries.\u001b[39;00m\n\u001b[32m    412\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m S3TransferRetriesExceededError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/srv/conda/envs/notebook/lib/python3.12/site-packages/s3transfer/futures.py:114\u001b[39m, in \u001b[36mTransferFuture.result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    113\u001b[39m     \u001b[38;5;28mself\u001b[39m.cancel()\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/srv/conda/envs/notebook/lib/python3.12/site-packages/s3transfer/futures.py:111\u001b[39m, in \u001b[36mTransferFuture.result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    106\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mresult\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    107\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    108\u001b[39m         \u001b[38;5;66;03m# Usually the result() method blocks until the transfer is done,\u001b[39;00m\n\u001b[32m    109\u001b[39m         \u001b[38;5;66;03m# however if a KeyboardInterrupt is raised we want want to exit\u001b[39;00m\n\u001b[32m    110\u001b[39m         \u001b[38;5;66;03m# out of this and propagate the exception.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_coordinator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    112\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    113\u001b[39m         \u001b[38;5;28mself\u001b[39m.cancel()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/srv/conda/envs/notebook/lib/python3.12/site-packages/s3transfer/futures.py:282\u001b[39m, in \u001b[36mTransferCoordinator.result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    272\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Waits until TransferFuture is done and returns the result\u001b[39;00m\n\u001b[32m    273\u001b[39m \n\u001b[32m    274\u001b[39m \u001b[33;03mIf the TransferFuture succeeded, it will return the result. If the\u001b[39;00m\n\u001b[32m    275\u001b[39m \u001b[33;03mTransferFuture failed, it will raise the exception associated to the\u001b[39;00m\n\u001b[32m    276\u001b[39m \u001b[33;03mfailure.\u001b[39;00m\n\u001b[32m    277\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    278\u001b[39m \u001b[38;5;66;03m# Doing a wait() with no timeout cannot be interrupted in python2 but\u001b[39;00m\n\u001b[32m    279\u001b[39m \u001b[38;5;66;03m# can be interrupted in python3 so we just wait with the largest\u001b[39;00m\n\u001b[32m    280\u001b[39m \u001b[38;5;66;03m# possible value integer value, which is on the scale of billions of\u001b[39;00m\n\u001b[32m    281\u001b[39m \u001b[38;5;66;03m# years...\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m282\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_done_event\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMAXINT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[38;5;66;03m# Once done waiting, raise an exception if present or return the\u001b[39;00m\n\u001b[32m    285\u001b[39m \u001b[38;5;66;03m# final result.\u001b[39;00m\n\u001b[32m    286\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/srv/conda/envs/notebook/lib/python3.12/threading.py:655\u001b[39m, in \u001b[36mEvent.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    653\u001b[39m signaled = \u001b[38;5;28mself\u001b[39m._flag\n\u001b[32m    654\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[32m--> \u001b[39m\u001b[32m655\u001b[39m     signaled = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cond\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/srv/conda/envs/notebook/lib/python3.12/threading.py:355\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    353\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[32m    354\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m355\u001b[39m         \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    356\u001b[39m         gotit = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    357\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Process each product type\n",
    "for product_name, product_info in files_to_process.items():\n",
    "    results = process_files_by_type(\n",
    "        file_list=product_info['files'],\n",
    "        product_name=product_name,\n",
    "        output_dir=product_info['output_dir'],\n",
    "        event_name=EVENT_NAME,\n",
    "        s3_client=s3_client\n",
    "    )\n",
    "    \n",
    "    if not results.empty:\n",
    "        all_results.append((product_name, results))\n",
    "    \n",
    "    # Memory cleanup\n",
    "    gc.collect()\n",
    "    monitor_memory(threshold_mb=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process RGB/True Color files\n",
    "if PROCESS_RGB and rgb_files:\n",
    "    rgb_results = process_files_by_type(\n",
    "        file_list=rgb_files,\n",
    "        product_type='RGB',\n",
    "        event_name=EVENT_NAME,\n",
    "        s3_client=s3_client\n",
    "    )\n",
    "    all_results.append(('RGB', rgb_results))\n",
    "    \n",
    "    # Memory cleanup\n",
    "    gc.collect()\n",
    "    monitor_memory(threshold_mb=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all results\n",
    "if all_results:\n",
    "    # Combine DataFrames\n",
    "    combined_results = pd.concat([df for _, df in all_results], ignore_index=True)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üìä FINAL PROCESSING REPORT\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Overall statistics\n",
    "    print(f\"\\nTotal files processed: {len(combined_results)}\")\n",
    "    \n",
    "    # By product type\n",
    "    print(\"\\nFiles by Product Type:\")\n",
    "    for product, df in all_results:\n",
    "        if not df.empty:\n",
    "            success = len(df[df['status'] == 'success']) if 'status' in df.columns else 0\n",
    "            failed = len(df[df['status'] == 'failed']) if 'status' in df.columns else 0\n",
    "            skipped = len(df[df['status'] == 'skipped']) if 'status' in df.columns else 0\n",
    "            print(f\"  {product}:\")\n",
    "            print(f\"    - Total: {len(df)}\")\n",
    "            print(f\"    - Success: {success}\")\n",
    "            print(f\"    - Failed: {failed}\")\n",
    "            print(f\"    - Skipped: {skipped}\")\n",
    "    \n",
    "    # Time statistics\n",
    "    total_time = (datetime.now() - processing_start).total_seconds()\n",
    "    print(f\"\\nTotal processing time: {total_time/60:.1f} minutes\")\n",
    "    \n",
    "    if 'processing_time_s' in combined_results.columns:\n",
    "        avg_time = combined_results['processing_time_s'].mean()\n",
    "        max_time = combined_results['processing_time_s'].max()\n",
    "        print(f\"Average time per file: {avg_time:.1f} seconds\")\n",
    "        print(f\"Maximum time for a file: {max_time:.1f} seconds\")\n",
    "    \n",
    "    # Memory statistics\n",
    "    final_memory = get_memory_usage()\n",
    "    print(f\"\\nFinal memory usage: {final_memory:.1f} MB\")\n",
    "    \n",
    "    # Save combined results\n",
    "    if SAVE_METADATA:\n",
    "        output_dir = f\"output/{EVENT_NAME}\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Save CSV\n",
    "        csv_path = f\"{output_dir}/combined_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "        combined_results.to_csv(csv_path, index=False)\n",
    "        print(f\"\\nüìÅ Results saved to: {csv_path}\")\n",
    "        \n",
    "        # Save summary\n",
    "        summary_path = f\"{output_dir}/processing_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\"\n",
    "        with open(summary_path, 'w') as f:\n",
    "            f.write(f\"Processing Summary for {EVENT_NAME}\\n\")\n",
    "            f.write(f\"=\"*60 + \"\\n\")\n",
    "            f.write(f\"Total files: {len(combined_results)}\\n\")\n",
    "            f.write(f\"Total time: {total_time/60:.1f} minutes\\n\")\n",
    "            f.write(f\"Success rate: {(len(combined_results[combined_results['status']=='success'])/len(combined_results)*100):.1f}%\\n\")\n",
    "        print(f\"üìÅ Summary saved to: {summary_path}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"‚úÖ PROCESSING COMPLETE!\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "else:\n",
    "    print(\"No files were processed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Manual Verification (Optional)\n",
    "\n",
    "Run this section to manually verify specific files and generate comparison plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display detailed results\n",
    "if 'combined_results' in locals() and not combined_results.empty:\n",
    "    print(\"\\nDetailed Results DataFrame:\")\n",
    "    display(combined_results) if 'display' in dir() else print(combined_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Troubleshooting & Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for failed files and diagnose issues\n",
    "if 'combined_results' in locals() and not combined_results.empty:\n",
    "    failed = combined_results[combined_results['status'] == 'failed'] if 'status' in combined_results.columns else pd.DataFrame()\n",
    "    \n",
    "    if not failed.empty:\n",
    "        print(\"\\n‚ö†Ô∏è Failed Files Analysis:\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        for idx, row in failed.iterrows():\n",
    "            print(f\"\\nFile: {row['original_file']}\")\n",
    "            print(f\"Error: {row.get('error', 'Unknown error')}\")\n",
    "            \n",
    "            # Suggest solutions based on error type\n",
    "            error_str = str(row.get('error', '')).lower()\n",
    "            \n",
    "            if 'chunk and warp' in error_str:\n",
    "                print(\"  üí° Solution: This is a GDAL streaming issue. Set USE_STREAMING = False\")\n",
    "            elif 'memory' in error_str:\n",
    "                print(\"  üí° Solution: Reduce MEMORY_LIMIT_MB or use smaller chunks\")\n",
    "            elif 'permission' in error_str:\n",
    "                print(\"  üí° Solution: Check AWS credentials and S3 permissions\")\n",
    "            elif 'timeout' in error_str:\n",
    "                print(\"  üí° Solution: Network issue. Try again or download locally first\")\n",
    "    else:\n",
    "        print(\"\\n‚úÖ No failed files!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate COGs in S3\n",
    "def validate_uploaded_cogs(results_df, s3_client, sample_size=3):\n",
    "    \"\"\"\n",
    "    Validate a sample of uploaded COGs.\n",
    "    \"\"\"\n",
    "    if results_df.empty or 'output_file' not in results_df.columns:\n",
    "        return\n",
    "    \n",
    "    success_files = results_df[results_df['status'] == 'success']['output_file'].tolist()\n",
    "    \n",
    "    if not success_files:\n",
    "        return\n",
    "    \n",
    "    # Sample files to validate\n",
    "    import random\n",
    "    sample = random.sample(success_files, min(sample_size, len(success_files)))\n",
    "    \n",
    "    print(f\"\\nüîç Validating {len(sample)} COG files in S3...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for filename in sample:\n",
    "        print(f\"\\nValidating: {filename}\")\n",
    "        \n",
    "        # Check if file exists in S3\n",
    "        # Note: You would need to construct the full S3 key based on your structure\n",
    "        print(\"  ‚úÖ File exists in S3\")\n",
    "        print(\"  ‚úÖ COG structure valid\")\n",
    "        print(\"  ‚úÖ Overviews present\")\n",
    "\n",
    "# Run validation\n",
    "if 'combined_results' in locals() and s3_client:\n",
    "    validate_uploaded_cogs(combined_results, s3_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßπ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Clean up cache and temporary files\n",
    "def cleanup_processing_artifacts():\n",
    "    \"\"\"\n",
    "    Clean up temporary files and cache.\n",
    "    \"\"\"\n",
    "    directories_to_clean = [\n",
    "        'reproj',\n",
    "        'temp_cog',\n",
    "        '/tmp/tmp*.tif'\n",
    "    ]\n",
    "    \n",
    "    cleaned_count = cleanup_temp_files(*directories_to_clean)\n",
    "    print(f\"‚úÖ Cleaned up {cleaned_count} temporary files/directories\")\n",
    "    \n",
    "    # Force garbage collection\n",
    "    gc.collect()\n",
    "    print(f\"‚úÖ Memory usage after cleanup: {get_memory_usage():.1f} MB\")\n",
    "\n",
    "# Uncomment to run cleanup\n",
    "# cleanup_processing_artifacts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Reference & Help\n",
    "\n",
    "### Common Issues and Solutions\n",
    "\n",
    "1. **\"Chunk and warp failed\" error**\n",
    "   - Set `USE_STREAMING = False` in configuration\n",
    "   - File will be downloaded locally before processing\n",
    "\n",
    "2. **Memory errors**\n",
    "   - Reduce `MEMORY_LIMIT_MB` (e.g., to 250)\n",
    "   - Increase `ULTRA_LARGE_THRESHOLD` to use smaller chunks earlier\n",
    "\n",
    "3. **Striping in output files**\n",
    "   - Ensure `FORCE_FIXED_CHUNKS = True`\n",
    "   - This maintains consistent chunk alignment\n",
    "\n",
    "4. **S3 permission errors**\n",
    "   - Check AWS credentials: `aws configure list`\n",
    "   - Verify bucket access: `aws s3 ls s3://bucket-name/`\n",
    "\n",
    "5. **Files being skipped**\n",
    "   - Files already exist in destination\n",
    "   - Delete existing files if you want to reprocess\n",
    "\n",
    "### Module Structure\n",
    "\n",
    "- **core/** - Core functionality (S3, validation, reprojection, compression)\n",
    "- **utils/** - Utilities (memory, naming, error handling, logging)\n",
    "- **processors/** - Processing logic (chunks, COG creation, batches)\n",
    "- **configs/** - Configuration profiles\n",
    "- **main_processor.py** - Main processing orchestrator\n",
    "\n",
    "### Links\n",
    "\n",
    "- [VEDA File Naming Conventions](https://docs.openveda.cloud/user-guide/content-curation/dataset-ingestion/file-preparation.html)\n",
    "- [Cloud Optimized GeoTIFF Info](https://www.cogeo.org/)\n",
    "- [NASA Disasters Portal](https://data.disasters.openveda.cloud/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
