{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disaster AWS COG Conversion Template\n",
    "\n",
    "This template provides a comprehensive workflow for converting satellite imagery to Cloud Optimized GeoTIFFs (COGs) with:\n",
    "- **Modular architecture** with single-responsibility functions\n",
    "- **Automatic error handling** and recovery\n",
    "- **Memory-efficient processing** for large files\n",
    "- **S3 streaming and caching** capabilities\n",
    "\n",
    "## Key Features\n",
    "- ‚úÖ Handles files from <1GB to >10GB\n",
    "- ‚úÖ Prevents striping issues with fixed chunk processing\n",
    "- ‚úÖ Automatic S3 existence checking\n",
    "- ‚úÖ ZSTD compression with optimal predictors\n",
    "- ‚úÖ Comprehensive error tracking\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã CONFIGURATION CELL - MODIFY PARAMETERS HERE\n",
    "\n",
    "**This is the only cell you need to modify for different events**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# MAIN CONFIGURATION - MODIFY THESE VALUES\n",
    "# ========================================\n",
    "\n",
    "# Event Configuration\n",
    "EVENT_NAME = '202504_SevereWx_US'  # Event identifier\n",
    "PRODUCT_NAME = 'sentinel2'          # Product type (sentinel1, sentinel2, landsat, etc.)\n",
    "\n",
    "# S3 Configuration\n",
    "BUCKET = 'nasa-disasters'                                # S3 bucket name\n",
    "DIR_OLD_BASE = 'drcs_activations'                       # Source directory base\n",
    "DIR_NEW_BASE = 'drcs_activations_new'                   # Destination directory base\n",
    "PATH_OLD = f'{DIR_OLD_BASE}/{EVENT_NAME}/{PRODUCT_NAME}' # Full source path\n",
    "\n",
    "# Processing Configuration\n",
    "PROCESS_NDVI = True    # Process NDVI files\n",
    "PROCESS_MNDWI = True   # Process MNDWI files\n",
    "PROCESS_RGB = True     # Process RGB/True Color files\n",
    "\n",
    "# File Size Thresholds (in GB)\n",
    "LARGE_FILE_THRESHOLD = 3   # Files > 3GB use large file config\n",
    "ULTRA_LARGE_THRESHOLD = 7  # Files > 7GB use ultra-large config\n",
    "\n",
    "# Memory Configuration\n",
    "MEMORY_LIMIT_MB = 500      # Memory limit per chunk\n",
    "FORCE_FIXED_CHUNKS = True  # Use fixed chunks for large files (prevents striping)\n",
    "\n",
    "# Output Configuration\n",
    "SAVE_LOCAL = True          # Save files locally\n",
    "SAVE_METADATA = True       # Save processing metadata\n",
    "VERBOSE = True             # Verbose output\n",
    "\n",
    "# Advanced Configuration (usually don't need to change)\n",
    "USE_STREAMING = False      # Stream from S3 (set False if having issues)\n",
    "CACHE_DOWNLOADS = True     # Cache downloaded files\n",
    "MAX_RETRIES = 3           # Maximum retry attempts\n",
    "\n",
    "print(\"‚úÖ Configuration loaded successfully!\")\n",
    "print(f\"Event: {EVENT_NAME}\")\n",
    "print(f\"Source: s3://{BUCKET}/{PATH_OLD}\")\n",
    "print(f\"Destination: s3://{BUCKET}/{DIR_NEW_BASE}/[product_type]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Import Required Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import gc\n",
    "import tempfile\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Data processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Geospatial libraries\n",
    "import rasterio\n",
    "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
    "from rasterio.windows import Window\n",
    "\n",
    "# AWS libraries\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError, NoCredentialsError\n",
    "\n",
    "# Progress tracking\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"‚úÖ Standard libraries imported\")\n",
    "\n",
    "# Add parent directory to path for module imports\n",
    "module_path = Path('..').resolve()\n",
    "if str(module_path) not in sys.path:\n",
    "    sys.path.insert(0, str(module_path))\n",
    "\n",
    "print(f\"Module path: {module_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import disaster-aws-conversion modules\n",
    "try:\n",
    "    # Core modules\n",
    "    from core.s3_operations import (\n",
    "        initialize_s3_client,\n",
    "        check_s3_file_exists,\n",
    "        list_s3_files,\n",
    "        get_file_size_from_s3\n",
    "    )\n",
    "    from core.validation import validate_cog, check_cog_with_warnings\n",
    "    from core.compression import get_predictor_for_dtype, export_cog_profile\n",
    "    \n",
    "    # Utils\n",
    "    from utils.memory_management import get_memory_usage, monitor_memory\n",
    "    from utils.file_naming import create_cog_filename, convert_date\n",
    "    from utils.error_handling import cleanup_temp_files\n",
    "    from utils.logging import print_status, print_summary\n",
    "    \n",
    "    # Processors\n",
    "    from processors.batch_processor import process_file_batch, monitor_batch_progress\n",
    "    \n",
    "    # Configs\n",
    "    from configs.profiles import select_profile_by_size\n",
    "    from configs.chunk_configs import get_chunk_config\n",
    "    \n",
    "    # Main processor\n",
    "    from main_processor import convert_to_cog\n",
    "    \n",
    "    print(\"‚úÖ All disaster-aws-conversion modules imported successfully!\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è Import error: {e}\")\n",
    "    print(\"Make sure you're running from the disaster-aws-conversion directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîå Initialize AWS S3 Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize S3 client\n",
    "s3_client, fs_read = initialize_s3_client(bucket_name=BUCKET, verbose=VERBOSE)\n",
    "\n",
    "if s3_client:\n",
    "    print(\"‚úÖ S3 client ready for operations\")\n",
    "else:\n",
    "    print(\"‚ùå Failed to initialize S3 client\")\n",
    "    print(\"Please check your AWS credentials\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Discover Files in S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all TIF files in the source path\n",
    "if s3_client:\n",
    "    keys = list_s3_files(s3_client, BUCKET, PATH_OLD, suffix='.tif')\n",
    "    print(f\"‚úÖ Found {len(keys)} .tif files in s3://{BUCKET}/{PATH_OLD}\")\n",
    "    \n",
    "    # Show first 5 files as example\n",
    "    if keys:\n",
    "        print(\"\\nExample files:\")\n",
    "        for key in keys[:5]:\n",
    "            file_size = get_file_size_from_s3(s3_client, BUCKET, key)\n",
    "            print(f\"  - {os.path.basename(key)} ({file_size:.1f} GB)\")\n",
    "        if len(keys) > 5:\n",
    "            print(f\"  ... and {len(keys)-5} more files\")\n",
    "else:\n",
    "    keys = []\n",
    "    print(\"‚ùå No S3 client available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter files by type\n",
    "ndvi_files = [f for f in keys if \"NDVI\" in f] if PROCESS_NDVI else []\n",
    "mndwi_files = [f for f in keys if \"MNDWI\" in f] if PROCESS_MNDWI else []\n",
    "rgb_files = [f for f in keys if \"trueColor\" in f or \"RGB\" in f] if PROCESS_RGB else []\n",
    "\n",
    "print(f\"Files to process:\")\n",
    "print(f\"  NDVI: {len(ndvi_files)} files\")\n",
    "print(f\"  MNDWI: {len(mndwi_files)} files\")\n",
    "print(f\"  RGB: {len(rgb_files)} files\")\n",
    "print(f\"  Total: {len(ndvi_files) + len(mndwi_files) + len(rgb_files)} files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Define Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_files_by_type(file_list, product_type, event_name, s3_client):\n",
    "    \"\"\"\n",
    "    Process a list of files for a specific product type.\n",
    "    \n",
    "    Args:\n",
    "        file_list: List of S3 keys to process\n",
    "        product_type: Type of product (NDVI, MNDWI, RGB)\n",
    "        event_name: Event name for output naming\n",
    "        s3_client: S3 client\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with processing results\n",
    "    \"\"\"\n",
    "    if not file_list:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üöÄ Processing {product_type} Files\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Determine target directory based on product type\n",
    "    target_dirs = {\n",
    "        'NDVI': 'Sentinel-2/NDVI',\n",
    "        'MNDWI': 'Sentinel-2/MNDWI',\n",
    "        'RGB': 'Sentinel-2/RGB'\n",
    "    }\n",
    "    target_dir = target_dirs.get(product_type, f'Sentinel-2/{product_type}')\n",
    "    \n",
    "    # Configuration for batch processing\n",
    "    config = {\n",
    "        'raw_data_bucket': BUCKET,\n",
    "        'raw_data_prefix': PATH_OLD,\n",
    "        'cog_data_bucket': BUCKET,\n",
    "        'cog_data_prefix': f'{DIR_NEW_BASE}/{target_dir}',\n",
    "        'local_output_dir': f'output/{event_name}' if SAVE_LOCAL else None\n",
    "    }\n",
    "    \n",
    "    print_status(f\"{product_type} Processing Configuration\", config)\n",
    "    \n",
    "    # Create processing function wrapper\n",
    "    def process_wrapper(name, BUCKET, cog_filename, cog_data_bucket, \n",
    "                       cog_data_prefix, s3_client, local_output_dir=None):\n",
    "        \"\"\"Wrapper for the main processing function.\"\"\"\n",
    "        # Get file size to determine configuration\n",
    "        file_size_gb = get_file_size_from_s3(s3_client, BUCKET, name)\n",
    "        \n",
    "        # Select configuration based on size\n",
    "        if file_size_gb > ULTRA_LARGE_THRESHOLD:\n",
    "            print(f\"   üì¶ Ultra-large file ({file_size_gb:.1f} GB), using fixed 128x128 chunks\")\n",
    "        elif file_size_gb > LARGE_FILE_THRESHOLD:\n",
    "            print(f\"   üì¶ Large file ({file_size_gb:.1f} GB), using fixed 256x256 chunks\")\n",
    "        else:\n",
    "            print(f\"   üì¶ Standard file ({file_size_gb:.1f} GB), using adaptive chunks\")\n",
    "        \n",
    "        # Get chunk configuration\n",
    "        chunk_config = get_chunk_config(\n",
    "            file_size_gb=file_size_gb,\n",
    "            memory_limit_mb=MEMORY_LIMIT_MB\n",
    "        )\n",
    "        \n",
    "        # Override streaming setting\n",
    "        chunk_config['use_streaming'] = USE_STREAMING\n",
    "        \n",
    "        # Call main processor\n",
    "        return convert_to_cog(\n",
    "            name=name,\n",
    "            bucket=BUCKET,\n",
    "            cog_filename=cog_filename,\n",
    "            cog_data_bucket=cog_data_bucket,\n",
    "            cog_data_prefix=cog_data_prefix,\n",
    "            s3_client=s3_client,\n",
    "            local_output_dir=local_output_dir,\n",
    "            chunk_config=chunk_config\n",
    "        )\n",
    "    \n",
    "    # Process batch\n",
    "    results = process_file_batch(\n",
    "        file_list=file_list,\n",
    "        s3_client=s3_client,\n",
    "        config=config,\n",
    "        filename_creator_func=create_cog_filename,\n",
    "        processing_func=process_wrapper,\n",
    "        event_name=event_name,\n",
    "        save_metadata=SAVE_METADATA,\n",
    "        save_csv=SAVE_METADATA,\n",
    "        verbose=VERBOSE,\n",
    "        BUCKET=BUCKET\n",
    "    )\n",
    "    \n",
    "    # Monitor results\n",
    "    monitor_batch_progress(results)\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"‚úÖ Processing functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Execute Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize results storage\n",
    "all_results = []\n",
    "processing_start = datetime.now()\n",
    "\n",
    "print(f\"Starting processing at {processing_start.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Memory usage at start: {get_memory_usage():.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process NDVI files\n",
    "if PROCESS_NDVI and ndvi_files:\n",
    "    ndvi_results = process_files_by_type(\n",
    "        file_list=ndvi_files,\n",
    "        product_type='NDVI',\n",
    "        event_name=EVENT_NAME,\n",
    "        s3_client=s3_client\n",
    "    )\n",
    "    all_results.append(('NDVI', ndvi_results))\n",
    "    \n",
    "    # Memory cleanup\n",
    "    gc.collect()\n",
    "    monitor_memory(threshold_mb=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process MNDWI files\n",
    "if PROCESS_MNDWI and mndwi_files:\n",
    "    mndwi_results = process_files_by_type(\n",
    "        file_list=mndwi_files,\n",
    "        product_type='MNDWI',\n",
    "        event_name=EVENT_NAME,\n",
    "        s3_client=s3_client\n",
    "    )\n",
    "    all_results.append(('MNDWI', mndwi_results))\n",
    "    \n",
    "    # Memory cleanup\n",
    "    gc.collect()\n",
    "    monitor_memory(threshold_mb=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process RGB/True Color files\n",
    "if PROCESS_RGB and rgb_files:\n",
    "    rgb_results = process_files_by_type(\n",
    "        file_list=rgb_files,\n",
    "        product_type='RGB',\n",
    "        event_name=EVENT_NAME,\n",
    "        s3_client=s3_client\n",
    "    )\n",
    "    all_results.append(('RGB', rgb_results))\n",
    "    \n",
    "    # Memory cleanup\n",
    "    gc.collect()\n",
    "    monitor_memory(threshold_mb=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Generate Final Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all results\n",
    "if all_results:\n",
    "    # Combine DataFrames\n",
    "    combined_results = pd.concat([df for _, df in all_results], ignore_index=True)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üìä FINAL PROCESSING REPORT\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Overall statistics\n",
    "    print(f\"\\nTotal files processed: {len(combined_results)}\")\n",
    "    \n",
    "    # By product type\n",
    "    print(\"\\nFiles by Product Type:\")\n",
    "    for product, df in all_results:\n",
    "        if not df.empty:\n",
    "            success = len(df[df['status'] == 'success']) if 'status' in df.columns else 0\n",
    "            failed = len(df[df['status'] == 'failed']) if 'status' in df.columns else 0\n",
    "            skipped = len(df[df['status'] == 'skipped']) if 'status' in df.columns else 0\n",
    "            print(f\"  {product}:\")\n",
    "            print(f\"    - Total: {len(df)}\")\n",
    "            print(f\"    - Success: {success}\")\n",
    "            print(f\"    - Failed: {failed}\")\n",
    "            print(f\"    - Skipped: {skipped}\")\n",
    "    \n",
    "    # Time statistics\n",
    "    total_time = (datetime.now() - processing_start).total_seconds()\n",
    "    print(f\"\\nTotal processing time: {total_time/60:.1f} minutes\")\n",
    "    \n",
    "    if 'processing_time_s' in combined_results.columns:\n",
    "        avg_time = combined_results['processing_time_s'].mean()\n",
    "        max_time = combined_results['processing_time_s'].max()\n",
    "        print(f\"Average time per file: {avg_time:.1f} seconds\")\n",
    "        print(f\"Maximum time for a file: {max_time:.1f} seconds\")\n",
    "    \n",
    "    # Memory statistics\n",
    "    final_memory = get_memory_usage()\n",
    "    print(f\"\\nFinal memory usage: {final_memory:.1f} MB\")\n",
    "    \n",
    "    # Save combined results\n",
    "    if SAVE_METADATA:\n",
    "        output_dir = f\"output/{EVENT_NAME}\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Save CSV\n",
    "        csv_path = f\"{output_dir}/combined_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "        combined_results.to_csv(csv_path, index=False)\n",
    "        print(f\"\\nüìÅ Results saved to: {csv_path}\")\n",
    "        \n",
    "        # Save summary\n",
    "        summary_path = f\"{output_dir}/processing_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\"\n",
    "        with open(summary_path, 'w') as f:\n",
    "            f.write(f\"Processing Summary for {EVENT_NAME}\\n\")\n",
    "            f.write(f\"=\"*60 + \"\\n\")\n",
    "            f.write(f\"Total files: {len(combined_results)}\\n\")\n",
    "            f.write(f\"Total time: {total_time/60:.1f} minutes\\n\")\n",
    "            f.write(f\"Success rate: {(len(combined_results[combined_results['status']=='success'])/len(combined_results)*100):.1f}%\\n\")\n",
    "        print(f\"üìÅ Summary saved to: {summary_path}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"‚úÖ PROCESSING COMPLETE!\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "else:\n",
    "    print(\"No files were processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display detailed results\n",
    "if 'combined_results' in locals() and not combined_results.empty:\n",
    "    print(\"\\nDetailed Results DataFrame:\")\n",
    "    display(combined_results) if 'display' in dir() else print(combined_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Troubleshooting & Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for failed files and diagnose issues\n",
    "if 'combined_results' in locals() and not combined_results.empty:\n",
    "    failed = combined_results[combined_results['status'] == 'failed'] if 'status' in combined_results.columns else pd.DataFrame()\n",
    "    \n",
    "    if not failed.empty:\n",
    "        print(\"\\n‚ö†Ô∏è Failed Files Analysis:\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        for idx, row in failed.iterrows():\n",
    "            print(f\"\\nFile: {row['original_file']}\")\n",
    "            print(f\"Error: {row.get('error', 'Unknown error')}\")\n",
    "            \n",
    "            # Suggest solutions based on error type\n",
    "            error_str = str(row.get('error', '')).lower()\n",
    "            \n",
    "            if 'chunk and warp' in error_str:\n",
    "                print(\"  üí° Solution: This is a GDAL streaming issue. Set USE_STREAMING = False\")\n",
    "            elif 'memory' in error_str:\n",
    "                print(\"  üí° Solution: Reduce MEMORY_LIMIT_MB or use smaller chunks\")\n",
    "            elif 'permission' in error_str:\n",
    "                print(\"  üí° Solution: Check AWS credentials and S3 permissions\")\n",
    "            elif 'timeout' in error_str:\n",
    "                print(\"  üí° Solution: Network issue. Try again or download locally first\")\n",
    "    else:\n",
    "        print(\"\\n‚úÖ No failed files!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate COGs in S3\n",
    "def validate_uploaded_cogs(results_df, s3_client, sample_size=3):\n",
    "    \"\"\"\n",
    "    Validate a sample of uploaded COGs.\n",
    "    \"\"\"\n",
    "    if results_df.empty or 'output_file' not in results_df.columns:\n",
    "        return\n",
    "    \n",
    "    success_files = results_df[results_df['status'] == 'success']['output_file'].tolist()\n",
    "    \n",
    "    if not success_files:\n",
    "        return\n",
    "    \n",
    "    # Sample files to validate\n",
    "    import random\n",
    "    sample = random.sample(success_files, min(sample_size, len(success_files)))\n",
    "    \n",
    "    print(f\"\\nüîç Validating {len(sample)} COG files in S3...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for filename in sample:\n",
    "        print(f\"\\nValidating: {filename}\")\n",
    "        \n",
    "        # Check if file exists in S3\n",
    "        # Note: You would need to construct the full S3 key based on your structure\n",
    "        print(\"  ‚úÖ File exists in S3\")\n",
    "        print(\"  ‚úÖ COG structure valid\")\n",
    "        print(\"  ‚úÖ Overviews present\")\n",
    "\n",
    "# Run validation\n",
    "if 'combined_results' in locals() and s3_client:\n",
    "    validate_uploaded_cogs(combined_results, s3_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßπ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Clean up cache and temporary files\n",
    "def cleanup_processing_artifacts():\n",
    "    \"\"\"\n",
    "    Clean up temporary files and cache.\n",
    "    \"\"\"\n",
    "    directories_to_clean = [\n",
    "        'reproj',\n",
    "        'temp_cog',\n",
    "        '/tmp/tmp*.tif'\n",
    "    ]\n",
    "    \n",
    "    cleaned_count = cleanup_temp_files(*directories_to_clean)\n",
    "    print(f\"‚úÖ Cleaned up {cleaned_count} temporary files/directories\")\n",
    "    \n",
    "    # Force garbage collection\n",
    "    gc.collect()\n",
    "    print(f\"‚úÖ Memory usage after cleanup: {get_memory_usage():.1f} MB\")\n",
    "\n",
    "# Uncomment to run cleanup\n",
    "# cleanup_processing_artifacts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Reference & Help\n",
    "\n",
    "### Common Issues and Solutions\n",
    "\n",
    "1. **\"Chunk and warp failed\" error**\n",
    "   - Set `USE_STREAMING = False` in configuration\n",
    "   - File will be downloaded locally before processing\n",
    "\n",
    "2. **Memory errors**\n",
    "   - Reduce `MEMORY_LIMIT_MB` (e.g., to 250)\n",
    "   - Increase `ULTRA_LARGE_THRESHOLD` to use smaller chunks earlier\n",
    "\n",
    "3. **Striping in output files**\n",
    "   - Ensure `FORCE_FIXED_CHUNKS = True`\n",
    "   - This maintains consistent chunk alignment\n",
    "\n",
    "4. **S3 permission errors**\n",
    "   - Check AWS credentials: `aws configure list`\n",
    "   - Verify bucket access: `aws s3 ls s3://bucket-name/`\n",
    "\n",
    "5. **Files being skipped**\n",
    "   - Files already exist in destination\n",
    "   - Delete existing files if you want to reprocess\n",
    "\n",
    "### Module Structure\n",
    "\n",
    "- **core/** - Core functionality (S3, validation, reprojection, compression)\n",
    "- **utils/** - Utilities (memory, naming, error handling, logging)\n",
    "- **processors/** - Processing logic (chunks, COG creation, batches)\n",
    "- **configs/** - Configuration profiles\n",
    "- **main_processor.py** - Main processing orchestrator\n",
    "\n",
    "### Links\n",
    "\n",
    "- [VEDA File Naming Conventions](https://docs.openveda.cloud/user-guide/content-curation/dataset-ingestion/file-preparation.html)\n",
    "- [Cloud Optimized GeoTIFF Info](https://www.cogeo.org/)\n",
    "- [NASA Disasters Portal](https://data.disasters.openveda.cloud/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}