{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üåç Simple Disaster COG Processing\n",
    "\n",
    "This simplified notebook converts disaster satellite imagery to Cloud Optimized GeoTIFFs (COGs) with just a few cells.\n",
    "\n",
    "## ‚ú® Features\n",
    "- **See files first** - List S3 files before configuring\n",
    "- **Smart configuration** - Define filename functions after seeing actual files\n",
    "- **Auto-discovery** - Automatically categorizes your files\n",
    "- **Simple processing** - Just run the cells in order\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Step 1: Basic Configuration\n",
    "\n",
    "Set your event details and S3 paths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Basic configuration loaded\n",
      "Event: temp-planet\n",
      "Source: s3://nasa-disasters/temp-planet/\n",
      "Destination: s3://nasa-disasters/temp_planet_cog/\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# BASIC CONFIGURATION\n",
    "# ========================================\n",
    "\n",
    "# Event Details\n",
    "EVENT_NAME = 'temp-planet'  # Your disaster event name\n",
    "PRODUCT_NAME = ''                   # Product type (sentinel1, sentinel2, landsat, etc.)\n",
    "\n",
    "# S3 Paths\n",
    "BUCKET = 'nasa-disasters'                                          # S3 bucket\n",
    "SOURCE_PATH = f'{EVENT_NAME}/{PRODUCT_NAME}'      # Where your files are\n",
    "DESTINATION_BASE = 'temp_planet_cog'                          # Where to save COGs\n",
    "\n",
    "# Processing Options\n",
    "OVERWRITE = False      # Set to True to replace existing files\n",
    "VERIFY = True          # Set to True to verify results after processing\n",
    "SAVE_RESULTS = True    # Set to False to skip saving results CSV to /output directory\n",
    "\n",
    "print(\"‚úÖ Basic configuration loaded\")\n",
    "print(f\"Event: {EVENT_NAME}\")\n",
    "print(f\"Source: s3://{BUCKET}/{SOURCE_PATH}\")\n",
    "print(f\"Destination: s3://{BUCKET}/{DESTINATION_BASE}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to add... satellite name within the above\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Step 2: Connect to S3 and List Files\n",
    "\n",
    "Let's see what files are available before configuring filename transformations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåê Connecting to S3...\n",
      "‚úÖ Connected to S3\n",
      "\n",
      "üìÇ Files in s3://nasa-disasters/temp-planet/:\n",
      "============================================================\n",
      "Found 16 .tif files:\n",
      "\n",
      " 1. Planet_NDVI_20250602_170712_43_8106770.tif                   (0.26 GB)\n",
      " 2. Planet_NDVI_20250602_170714_26_8106770.tif                   (0.26 GB)\n",
      " 3. Planet_NDVI_20250602_170716_08_8106770.tif                   (0.26 GB)\n",
      " 4. Planet_NDVI_20250602_170717_90_8106770.tif                   (0.26 GB)\n",
      " 5. Planet_NDVI_20250602_172810_91_8106814.tif                   (0.37 GB)\n",
      " 6. Planet_NDVI_20250602_172813_10_8106814.tif                   (0.37 GB)\n",
      " 7. Planet_NDVI_20250602_172815_30_8106814.tif                   (0.37 GB)\n",
      " 8. Planet_NDVI_20250602_173839_64_8106856.tif                   (0.35 GB)\n",
      " 9. Planet_NDVI_20250602_173841_74_8106856.tif                   (0.35 GB)\n",
      "10. Planet_NDVI_20250602_173843_85_8106856.tif                   (0.35 GB)\n",
      "\n",
      "... and 6 more files\n",
      "\n",
      "============================================================\n",
      "\n",
      "üí° Use this information to create filename functions in Step 3\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.insert(0, str(Path('..').resolve()))\n",
    "\n",
    "# Import S3 operations\n",
    "from core.s3_operations import (\n",
    "    initialize_s3_client,\n",
    "    list_s3_files,\n",
    "    get_file_size_from_s3\n",
    ")\n",
    "\n",
    "# Initialize S3 client\n",
    "print(\"üåê Connecting to S3...\")\n",
    "s3_client, _ = initialize_s3_client(bucket_name=BUCKET, verbose=False)\n",
    "\n",
    "if s3_client:\n",
    "    print(\"‚úÖ Connected to S3\\n\")\n",
    "    \n",
    "    # List all TIF files\n",
    "    print(f\"üìÇ Files in s3://{BUCKET}/{SOURCE_PATH}:\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    files = list_s3_files(s3_client, BUCKET, SOURCE_PATH, suffix='.tif')\n",
    "    \n",
    "    if files:\n",
    "        print(f\"Found {len(files)} .tif files:\\n\")\n",
    "        for i, file_path in enumerate(files[:10], 1):  # Show first 10\n",
    "            filename = os.path.basename(file_path)\n",
    "            try:\n",
    "                size_gb = get_file_size_from_s3(s3_client, BUCKET, file_path)\n",
    "                print(f\"{i:2}. {filename:<60} ({size_gb:.2f} GB)\")\n",
    "            except:\n",
    "                print(f\"{i:2}. {filename}\")\n",
    "        \n",
    "        if len(files) > 10:\n",
    "            print(f\"\\n... and {len(files) - 10} more files\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"\\nüí° Use this information to create filename functions in Step 3\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No .tif files found in the specified path.\")\n",
    "        print(\"   Check your SOURCE_PATH configuration.\")\n",
    "else:\n",
    "    print(\"‚ùå Could not connect to S3. Check your AWS credentials.\")\n",
    "    files = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temp-planet_Planet_NDVI_20250602_170712_43_8106770.tif\n"
     ]
    }
   ],
   "source": [
    "a='Planet_NDVI_20250602_170712_43_8106770.tif'\n",
    "\n",
    "print (f'{EVENT_NAME}_{a}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üè∑Ô∏è Step 3: Define Categorization and Filename Transformations\n",
    "\n",
    "Based on the files you see above, configure:\n",
    "1. **Categorization patterns** - Regex patterns to identify file types\n",
    "2. **Filename functions** - How to transform filenames\n",
    "3. **Output directories** - Where each category should be saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configuration defined\n",
      "\n",
      "üìÇ Categories and output paths:\n",
      "   ‚Ä¢ trueColor:\n",
      "     Pattern: trueColor|truecolor|true_color\n",
      "     Output:  temp_planet_cog/Landsat/trueColor\n",
      "   ‚Ä¢ colorInfrared:\n",
      "     Pattern: colorInfrared|colorIR|color_infrared\n",
      "     Output:  temp_planet_cog/Landsat/colorIR\n",
      "   ‚Ä¢ naturalColor:\n",
      "     Pattern: naturalColor|natural_color\n",
      "     Output:  temp_planet_cog/Landsat/naturalColor\n",
      "   ‚Ä¢ NDVI:\n",
      "     Pattern: NDVI|ndvi\n",
      "     Output:  temp_planet_cog/cog\n",
      "\n",
      "üìù Example transformation:\n",
      "   Original: Planet_NDVI_20250602_170712_43_8106770.tif\n",
      "   Category: NDVI\n",
      "   ‚Üí New:    temp-planet/Planet_NDVI_20250602_170712_43_8106770.tif\n",
      "   ‚Üí Output: temp_planet_cog/cog/temp-planet/Planet_NDVI_20250602_170712_43_8106770.tif\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# CATEGORIZATION AND OUTPUT CONFIGURATION\n",
    "# ========================================\n",
    "\n",
    "import re\n",
    "\n",
    "# STEP 1: Define how to extract dates from filenames\n",
    "def extract_date_from_filename(filename):\n",
    "    \"\"\"Extract date from filename in YYYYMMDD format.\"\"\"\n",
    "    dates = re.findall(r'\\d{8}', filename)\n",
    "    if dates:\n",
    "        date_str = dates[0]\n",
    "        return f\"{date_str[0:4]}-{date_str[4:6]}-{date_str[6:8]}\"\n",
    "    return None\n",
    "\n",
    "# STEP 2: Define filename transformation functions for each category\n",
    "def create_truecolor_filename(original_path, event_name):\n",
    "    \"\"\"Create filename for trueColor products.\"\"\"\n",
    "    filename = os.path.basename(original_path)\n",
    "    stem = os.path.splitext(filename)[0]\n",
    "    date = extract_date_from_filename(stem)\n",
    "    \n",
    "    if date:\n",
    "        stem_clean = re.sub(r'_\\d{8}', '', stem)\n",
    "        return f\"{event_name}_{stem_clean}_{date}_day.tif\"\n",
    "    return f\"{event_name}_{stem}_day.tif\"\n",
    "\n",
    "def create_colorinfrared_filename(original_path, event_name):\n",
    "    \"\"\"Create filename for colorInfrared products.\"\"\"\n",
    "    filename = os.path.basename(original_path)\n",
    "    stem = os.path.splitext(filename)[0]\n",
    "    date = extract_date_from_filename(stem)\n",
    "    \n",
    "    if date:\n",
    "        stem_clean = re.sub(r'_\\d{8}', '', stem)\n",
    "        return f\"{event_name}_{stem_clean}_{date}_day.tif\"\n",
    "    return f\"{event_name}_{stem}_day.tif\"\n",
    "\n",
    "def create_naturalcolor_filename(original_path, event_name):\n",
    "    \"\"\"Create filename for naturalColor products.\"\"\"\n",
    "    filename = os.path.basename(original_path)\n",
    "    stem = os.path.splitext(filename)[0]\n",
    "    date = extract_date_from_filename(stem)\n",
    "    \n",
    "    if date:\n",
    "        stem_clean = re.sub(r'_\\d{8}', '', stem)\n",
    "        return f\"{event_name}_{stem_clean}_{date}_day.tif\"\n",
    "    return f\"{event_name}_{stem}_day.tif\"\n",
    "\n",
    "def planet_test(original_path, event_name):\n",
    "    return (f'{original_path}')\n",
    "\n",
    "# STEP 3: Configure categorization patterns (REQUIRED)\n",
    "# These regex patterns determine which files belong to which category\n",
    "CATEGORIZATION_PATTERNS = {\n",
    "    'trueColor': r'trueColor|truecolor|true_color',\n",
    "    'colorInfrared': r'colorInfrared|colorIR|color_infrared',\n",
    "    'naturalColor': r'naturalColor|natural_color',\n",
    "    'NDVI': r'NDVI|ndvi'\n",
    "    # Add patterns for ALL file types you want to process\n",
    "    # Files not matching any pattern will be skipped with a warning\n",
    "}\n",
    "\n",
    "# STEP 4: Map categories to filename transformation functions\n",
    "FILENAME_CREATORS = {\n",
    "    'trueColor': create_truecolor_filename,\n",
    "    'colorInfrared': create_colorinfrared_filename,\n",
    "    'naturalColor': create_naturalcolor_filename,\n",
    "    'NDVI': planet_test\n",
    "    # Must have an entry for each category in CATEGORIZATION_PATTERNS\n",
    "}\n",
    "\n",
    "# STEP 5: Specify output directories for each category\n",
    "OUTPUT_DIRS = {\n",
    "    'trueColor': 'Landsat/trueColor',\n",
    "    'colorInfrared': 'Landsat/colorIR',\n",
    "    'naturalColor': 'Landsat/naturalColor',\n",
    "    'NDVI': 'cog',\n",
    "    # Must have an entry for each category in CATEGORIZATION_PATTERNS\n",
    "}\n",
    "\n",
    "# OPTIONAL: Specify no-data values (None = auto-detect)\n",
    "NODATA_VALUES = {\n",
    "    'NDVI': -9999.0,\n",
    "    'MNDWI': -9999.0,\n",
    "    # Leave empty or set to None for auto-detection\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Configuration defined\")\n",
    "print(f\"\\nüìÇ Categories and output paths:\")\n",
    "for category, path in OUTPUT_DIRS.items():\n",
    "    pattern = CATEGORIZATION_PATTERNS.get(category, 'No pattern defined')\n",
    "    print(f\"   ‚Ä¢ {category}:\")\n",
    "    print(f\"     Pattern: {pattern}\")\n",
    "    print(f\"     Output:  {DESTINATION_BASE}/{path}\")\n",
    "\n",
    "# Test with sample filename if files exist\n",
    "if files:\n",
    "    sample_file = files[0]\n",
    "    sample_name = os.path.basename(sample_file)\n",
    "    \n",
    "    # Check which category it would match\n",
    "    matched_category = None\n",
    "    for cat, pattern in CATEGORIZATION_PATTERNS.items():\n",
    "        if re.search(pattern, sample_name, re.IGNORECASE):\n",
    "            matched_category = cat\n",
    "            break\n",
    "    \n",
    "    if matched_category:\n",
    "        new_name = FILENAME_CREATORS[matched_category](sample_file, EVENT_NAME)\n",
    "        print(f\"\\nüìù Example transformation:\")\n",
    "        print(f\"   Original: {sample_name}\")\n",
    "        print(f\"   Category: {matched_category}\")\n",
    "        print(f\"   ‚Üí New:    {new_name}\")\n",
    "        print(f\"   ‚Üí Output: {DESTINATION_BASE}/{OUTPUT_DIRS[matched_category]}/{new_name}\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è Warning: Sample file doesn't match any category pattern:\")\n",
    "        print(f\"   File: {sample_name}\")\n",
    "        print(f\"   Add a pattern to CATEGORIZATION_PATTERNS to process this file type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Step 4: Initialize Processor and Preview\n",
    "\n",
    "Now let's set up the processor and preview all transformations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Step 5: Process Files\n",
    "\n",
    "Run this cell to start processing all files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All modules loaded successfully\n",
      "\n",
      "üåê Connecting to S3...\n",
      "‚úÖ Connected to S3 successfully\n",
      "‚úÖ Processor ready\n",
      "\n",
      "\n",
      "üîç Searching for files in: temp-planet/\n",
      "‚úÖ Found 16 files\n",
      "\n",
      "üìä File Categories:\n",
      "  ‚Ä¢ NDVI: 16 files\n",
      "\n",
      "============================================================\n",
      "üìã PROCESSING PREVIEW\n",
      "============================================================\n",
      "\n",
      "Total files to process: 16\n",
      "Event: temp-planet\n",
      "Source: s3://nasa-disasters/temp-planet/\n",
      "Destination: s3://nasa-disasters/temp_planet_cog/\n",
      "\n",
      "File categories:\n",
      "  ‚Ä¢ NDVI: 16 files\n",
      "    Example: Planet_NDVI_20250602_170712_43_8106770.tif\n",
      "    ‚Üí temp-planet/Planet_NDVI_20250602_170712_43_8106770.tif\n",
      "\n",
      "Settings:\n",
      "  ‚Ä¢ Compression: ZSTD level 22\n",
      "  ‚Ä¢ Overwrite existing: False\n",
      "  ‚Ä¢ Verify results: True\n",
      "============================================================\n",
      "\n",
      "üìå Review the transformations above.\n",
      "   ‚Ä¢ Files will be saved to the directories specified in OUTPUT_DIRS\n",
      "   ‚Ä¢ If files appear as 'uncategorized', add patterns to CATEGORIZATION_PATTERNS\n",
      "   ‚Ä¢ When ready, proceed to Step 5 to process the files.\n"
     ]
    }
   ],
   "source": [
    "# Import our simplified helper\n",
    "from notebooks.notebook_helpers import SimpleProcessor\n",
    "\n",
    "# Create full configuration with categorization patterns\n",
    "config = {\n",
    "    'event_name': EVENT_NAME,\n",
    "    'bucket': BUCKET,\n",
    "    'source_path': SOURCE_PATH,\n",
    "    'destination_base': DESTINATION_BASE,\n",
    "    'overwrite': OVERWRITE,\n",
    "    'verify': VERIFY,\n",
    "    'save_results': SAVE_RESULTS,  # Add save results flag\n",
    "    'categorization_patterns': CATEGORIZATION_PATTERNS,  # IMPORTANT: Include patterns\n",
    "    'filename_creators': FILENAME_CREATORS,\n",
    "    'output_dirs': OUTPUT_DIRS,\n",
    "    'nodata_values': NODATA_VALUES\n",
    "}\n",
    "\n",
    "# Initialize processor\n",
    "processor = SimpleProcessor(config)\n",
    "\n",
    "# Connect to S3 (already connected, but needed for processor)\n",
    "if processor.connect_to_s3():\n",
    "    print(\"‚úÖ Processor ready\\n\")\n",
    "    \n",
    "    # Discover and categorize files\n",
    "    num_files = processor.discover_files()\n",
    "    \n",
    "    if num_files > 0:\n",
    "        # Show preview of transformations\n",
    "        processor.preview_processing()\n",
    "        \n",
    "        print(\"\\nüìå Review the transformations above.\")\n",
    "        print(\"   ‚Ä¢ Files will be saved to the directories specified in OUTPUT_DIRS\")\n",
    "        print(\"   ‚Ä¢ If files appear as 'uncategorized', add patterns to CATEGORIZATION_PATTERNS\")\n",
    "        print(\"   ‚Ä¢ When ready, proceed to Step 5 to process the files.\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No files found to process.\")\n",
    "else:\n",
    "    print(\"‚ùå Could not initialize processor.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting processing...\n",
      "This may take several minutes depending on file sizes.\n",
      "\n",
      "\n",
      "üöÄ Starting processing...\n",
      "\n",
      "üì¶ Processing NDVI (16 files)\n",
      "  ‚öôÔ∏è Processing: Planet_NDVI_20250602_170712_43_8106770.tif (0.3GB)\n",
      "   [CHECK] Checking if file already exists in S3: s3://nasa-disasters/temp_planet_cog/cog/temp-planet/Planet_NDVI_20250602_170712_43_8106770.tif\n",
      "   [INFO] File size: 0.3 GB\n",
      "   [CONFIG] Using fixed chunks\n",
      "   [TEMP] Using temp directory: /home/jovyan/disasters-aws-conversion/templates/temp_cog\n",
      "   [MEMORY] Initial: 228.6 MB, Available: 29459.7 MB\n",
      "   [DOWNLOAD] Downloading from S3...\n",
      "   [DOWNLOAD] Downloading from S3: s3://nasa-disasters/temp-planet/Planet_NDVI_20250602_170712_43_8106770.tif\n",
      "   [DOWNLOAD] ‚úÖ Downloaded 266.4 MB to data_download/temp-planet/Planet_NDVI_20250602_170712_43_8106770.tif\n",
      "   [OPTIMIZED] Using GDAL COG driver for maximum performance\n",
      "   [NODATA] Using manual no-data value: -9999.0\n",
      "   [GDAL-COG] Creating COG with native GDAL driver...\n",
      "   [GDAL-COG] Data type: float32 ‚Üí Resampling: bilinear, Overviews: average\n",
      "   [GDAL-COG] Stage 1: Reprojecting to EPSG:4326 using bilinear resampling...\n",
      "   [GDAL-COG] Stage 2: Creating COG...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "gdal_translate error: Warning 6: driver COG does not support creation option ZSTD_LEVEL\n",
      "ERROR 4: Attempt to create new tiff file `cog_temp-planet/Planet_NDVI_20250602_170712_43_8106770.tif' failed: cog_temp-planet/Planet_NDVI_20250602_170712_43_8106770.tif: No such file or directory\n",
      "\n",
      "/srv/conda/envs/notebook/lib/python3.12/site-packages/rio_cogeo/profiles.py:182: UserWarning: Non-standard compression schema: zstd. The output COG might not be fully supported by software not build against latest libtiff.\n",
      "  warnings.warn(\n",
      "Reading input: <open WarpedVRT name='WarpedVRT(data_download/temp-planet/Planet_NDVI_20250602_170712_43_8106770.tif)' mode='r'>\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   [GDAL-COG] ‚ùå COG creation failed: Warning 6: driver COG does not support creation option ZSTD_LEVEL\n",
      "ERROR 4: Attempt to create new tiff file `cog_temp-planet/Planet_NDVI_20250602_170712_43_8106770.tif' failed: cog_temp-planet/Planet_NDVI_20250602_170712_43_8106770.tif: No such file or directory\n",
      "\n",
      "   [GDAL-COG] Failed, trying rio-cogeo fallback...\n",
      "   [OPTIMIZED] Using rio-cogeo for single-pass COG creation\n",
      "   [NODATA] Using manual no-data value: -9999.0\n",
      "   [COG] Creating COG with reprojection in single pass...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding overviews...\n",
      "Updating dataset tags...\n",
      "Writing output to: cog_temp-planet/Planet_NDVI_20250602_170712_43_8106770.tif\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   [ERROR] Attempt to create new tiff file 'cog_temp-planet/Planet_NDVI_20250602_170712_43_8106770.tif' failed: cog_temp-planet/Planet_NDVI_20250602_170712_43_8106770.tif: No such file or directory\n",
      "   [CLEANUP] Removed: data_download/temp-planet/Planet_NDVI_20250602_170712_43_8106770.tif\n",
      "  ‚ùå Failed: Planet_NDVI_20250602_170712_43_8106770.tif - Attempt to create new tiff file 'cog_temp-planet/Planet_NDVI_20250602_170712_43_8106770.tif' failed: cog_temp-planet/Planet_NDVI_20250602_170712_43_8106770.tif: No such file or directory\n",
      "  ‚öôÔ∏è Processing: Planet_NDVI_20250602_170714_26_8106770.tif (0.3GB)\n",
      "   [CHECK] Checking if file already exists in S3: s3://nasa-disasters/temp_planet_cog/cog/temp-planet/Planet_NDVI_20250602_170714_26_8106770.tif\n",
      "   [INFO] File size: 0.3 GB\n",
      "   [CONFIG] Using fixed chunks\n",
      "   [TEMP] Using temp directory: /home/jovyan/disasters-aws-conversion/templates/temp_cog\n",
      "   [MEMORY] Initial: 748.3 MB, Available: 28950.0 MB\n",
      "   [DOWNLOAD] Downloading from S3...\n",
      "   [DOWNLOAD] Downloading from S3: s3://nasa-disasters/temp-planet/Planet_NDVI_20250602_170714_26_8106770.tif\n",
      "   [DOWNLOAD] ‚úÖ Downloaded 266.4 MB to data_download/temp-planet/Planet_NDVI_20250602_170714_26_8106770.tif\n",
      "   [OPTIMIZED] Using GDAL COG driver for maximum performance\n",
      "   [NODATA] Using manual no-data value: -9999.0\n",
      "   [GDAL-COG] Creating COG with native GDAL driver...\n",
      "   [GDAL-COG] Data type: float32 ‚Üí Resampling: bilinear, Overviews: average\n",
      "   [GDAL-COG] Stage 1: Reprojecting to EPSG:4326 using bilinear resampling...\n",
      "   [GDAL-COG] Stage 2: Creating COG...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "gdal_translate error: Warning 6: driver COG does not support creation option ZSTD_LEVEL\n",
      "ERROR 4: Attempt to create new tiff file `cog_temp-planet/Planet_NDVI_20250602_170714_26_8106770.tif' failed: cog_temp-planet/Planet_NDVI_20250602_170714_26_8106770.tif: No such file or directory\n",
      "\n",
      "/srv/conda/envs/notebook/lib/python3.12/site-packages/rio_cogeo/profiles.py:182: UserWarning: Non-standard compression schema: zstd. The output COG might not be fully supported by software not build against latest libtiff.\n",
      "  warnings.warn(\n",
      "Reading input: <open WarpedVRT name='WarpedVRT(data_download/temp-planet/Planet_NDVI_20250602_170714_26_8106770.tif)' mode='r'>\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   [GDAL-COG] ‚ùå COG creation failed: Warning 6: driver COG does not support creation option ZSTD_LEVEL\n",
      "ERROR 4: Attempt to create new tiff file `cog_temp-planet/Planet_NDVI_20250602_170714_26_8106770.tif' failed: cog_temp-planet/Planet_NDVI_20250602_170714_26_8106770.tif: No such file or directory\n",
      "\n",
      "   [GDAL-COG] Failed, trying rio-cogeo fallback...\n",
      "   [OPTIMIZED] Using rio-cogeo for single-pass COG creation\n",
      "   [NODATA] Using manual no-data value: -9999.0\n",
      "   [COG] Creating COG with reprojection in single pass...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding overviews...\n",
      "Updating dataset tags...\n",
      "Writing output to: cog_temp-planet/Planet_NDVI_20250602_170714_26_8106770.tif\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   [ERROR] Attempt to create new tiff file 'cog_temp-planet/Planet_NDVI_20250602_170714_26_8106770.tif' failed: cog_temp-planet/Planet_NDVI_20250602_170714_26_8106770.tif: No such file or directory\n",
      "   [CLEANUP] Removed: data_download/temp-planet/Planet_NDVI_20250602_170714_26_8106770.tif\n",
      "  ‚ùå Failed: Planet_NDVI_20250602_170714_26_8106770.tif - Attempt to create new tiff file 'cog_temp-planet/Planet_NDVI_20250602_170714_26_8106770.tif' failed: cog_temp-planet/Planet_NDVI_20250602_170714_26_8106770.tif: No such file or directory\n",
      "  ‚öôÔ∏è Processing: Planet_NDVI_20250602_170716_08_8106770.tif (0.3GB)\n",
      "   [CHECK] Checking if file already exists in S3: s3://nasa-disasters/temp_planet_cog/cog/temp-planet/Planet_NDVI_20250602_170716_08_8106770.tif\n",
      "   [INFO] File size: 0.3 GB\n",
      "   [CONFIG] Using fixed chunks\n",
      "   [TEMP] Using temp directory: /home/jovyan/disasters-aws-conversion/templates/temp_cog\n",
      "   [MEMORY] Initial: 760.4 MB, Available: 28937.2 MB\n",
      "   [DOWNLOAD] Downloading from S3...\n",
      "   [DOWNLOAD] Downloading from S3: s3://nasa-disasters/temp-planet/Planet_NDVI_20250602_170716_08_8106770.tif\n",
      "   [DOWNLOAD] ‚úÖ Downloaded 266.4 MB to data_download/temp-planet/Planet_NDVI_20250602_170716_08_8106770.tif\n",
      "   [OPTIMIZED] Using GDAL COG driver for maximum performance\n",
      "   [NODATA] Using manual no-data value: -9999.0\n",
      "   [GDAL-COG] Creating COG with native GDAL driver...\n",
      "   [GDAL-COG] Data type: float32 ‚Üí Resampling: bilinear, Overviews: average\n",
      "   [GDAL-COG] Stage 1: Reprojecting to EPSG:4326 using bilinear resampling...\n"
     ]
    }
   ],
   "source": [
    "# Process all files\n",
    "if 'num_files' in locals() and num_files > 0:\n",
    "    print(\"üöÄ Starting processing...\")\n",
    "    print(\"This may take several minutes depending on file sizes.\\n\")\n",
    "    \n",
    "    # Process everything\n",
    "    results = processor.process_all()\n",
    "    \n",
    "    # Display results\n",
    "    if not results.empty:\n",
    "        print(\"\\nüìä Processing Complete!\")\n",
    "        display(results) if 'display' in dir() else print(results)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No files to process. Complete Steps 1-4 first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze results\n",
    "if 'results' in locals() and not results.empty:\n",
    "    print(\"üìä PROCESSING STATISTICS\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    # Success rate\n",
    "    total = len(results)\n",
    "    success = len(results[results['status'] == 'success'])\n",
    "    failed = len(results[results['status'] == 'failed'])\n",
    "    skipped = len(results[results['status'] == 'skipped'])\n",
    "    \n",
    "    print(f\"Total files: {total}\")\n",
    "    print(f\"‚úÖ Success: {success}\")\n",
    "    print(f\"‚ùå Failed: {failed}\")\n",
    "    print(f\"‚è≠Ô∏è Skipped: {skipped}\")\n",
    "    print(f\"\\nSuccess rate: {(success/total*100):.1f}%\")\n",
    "    \n",
    "    # Failed files\n",
    "    if failed > 0:\n",
    "        print(\"\\n‚ùå Failed files:\")\n",
    "        failed_df = results[results['status'] == 'failed']\n",
    "        for idx, row in failed_df.iterrows():\n",
    "            print(f\"  - {row['source_file']}: {row.get('error', 'Unknown error')}\")\n",
    "    \n",
    "    # Processing times\n",
    "    if 'time_seconds' in results.columns:\n",
    "        success_df = results[results['status'] == 'success']\n",
    "        if not success_df.empty:\n",
    "            avg_time = success_df['time_seconds'].mean()\n",
    "            max_time = success_df['time_seconds'].max()\n",
    "            print(f\"\\n‚è±Ô∏è Timing:\")\n",
    "            print(f\"Average: {avg_time:.1f} seconds per file\")\n",
    "            print(f\"Slowest: {max_time:.1f} seconds\")\n",
    "else:\n",
    "    print(\"No results to analyze. Run Step 5 first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze results\n",
    "if 'results' in locals() and not results.empty:\n",
    "    print(\"üìä PROCESSING STATISTICS\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    # Success rate\n",
    "    total = len(results)\n",
    "    success = len(results[results['status'] == 'success'])\n",
    "    failed = len(results[results['status'] == 'failed'])\n",
    "    skipped = len(results[results['status'] == 'skipped'])\n",
    "    \n",
    "    print(f\"Total files: {total}\")\n",
    "    print(f\"‚úÖ Success: {success}\")\n",
    "    print(f\"‚ùå Failed: {failed}\")\n",
    "    print(f\"‚è≠Ô∏è Skipped: {skipped}\")\n",
    "    print(f\"\\nSuccess rate: {(success/total*100):.1f}%\")\n",
    "    \n",
    "    # Failed files\n",
    "    if failed > 0:\n",
    "        print(\"\\n‚ùå Failed files:\")\n",
    "        failed_df = results[results['status'] == 'failed']\n",
    "        for idx, row in failed_df.iterrows():\n",
    "            print(f\"  - {row['file']}: {row.get('error', 'Unknown error')}\")\n",
    "    \n",
    "    # Processing times\n",
    "    if 'time_seconds' in results.columns:\n",
    "        success_df = results[results['status'] == 'success']\n",
    "        if not success_df.empty:\n",
    "            avg_time = success_df['time_seconds'].mean()\n",
    "            max_time = success_df['time_seconds'].max()\n",
    "            print(f\"\\n‚è±Ô∏è Timing:\")\n",
    "            print(f\"Average: {avg_time:.1f} seconds per file\")\n",
    "            print(f\"Slowest: {max_time:.1f} seconds\")\n",
    "else:\n",
    "    print(\"No results to analyze. Run Step 5 first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí° Tips & Troubleshooting\n",
    "\n",
    "### Workflow Summary:\n",
    "1. **Configure** basic settings (Step 1)\n",
    "2. **List files** from S3 to see naming patterns (Step 2)\n",
    "3. **Define functions** to transform filenames (Step 3)\n",
    "4. **Preview** transformations (Step 4)\n",
    "5. **Process** all files (Step 5)\n",
    "6. **Review** results (Step 6)\n",
    "\n",
    "### Common Issues:\n",
    "\n",
    "1. **\"No files found\"**\n",
    "   - Check `SOURCE_PATH` in Step 1\n",
    "   - Verify bucket permissions\n",
    "   - Ensure files have `.tif` extension\n",
    "\n",
    "2. **Wrong filename transformations**\n",
    "   - Review actual filenames in Step 2\n",
    "   - Adjust functions in Step 3\n",
    "   - Re-run Step 4 to preview\n",
    "\n",
    "3. **Files being skipped**\n",
    "   - Files already exist in destination\n",
    "   - Set `OVERWRITE = True` in Step 1\n",
    "\n",
    "4. **Processing errors**\n",
    "   - Check AWS credentials\n",
    "   - Verify S3 write permissions\n",
    "   - Check available disk space for temp files\n",
    "\n",
    "### Need More Control?\n",
    "\n",
    "Use the full template at `disaster_processing_template.ipynb` for:\n",
    "- Manual chunk configuration\n",
    "- Custom compression settings\n",
    "- Detailed memory management\n",
    "- Advanced processing options"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
