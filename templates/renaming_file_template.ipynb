{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# üìù Filename Renaming Template\n\nThis notebook creates a mapping CSV of original filenames to new filenames without processing the files.\n\n## üìä Output CSV Columns\n- `original_s3_path` - Full S3 path to original file\n- `original_filename` - Original filename only\n- `new_filename` - Proposed new filename\n- `category` - File category\n- `file_size_gb` - File size in GB\n- `nodata_value` - Nodata value detected in the file\n- `status` - Validation status\n- `output_s3_path` - Proposed destination path\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Step 1: Basic Configuration\n",
    "\n",
    "Set your event details and S3 paths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event: 202510_Flood_AK\n",
      "Source: s3://nasa-disasters/drcs_activations/202510_Flood_AK/sentinel2\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# INPUTS\n",
    "# ========================================\n",
    "\n",
    "# S3 Paths (DO NOT CHANGE)\n",
    "BUCKET = 'nasa-disasters'    # S3 bucket\n",
    "DESTINATION_BASE = 'drcs_activations_new'  # Where to save COGs in S3 bucket\n",
    "GEOTIFF_DIR = 'drcs_activations' # This is where all raw geotiff files currently are\n",
    "\n",
    "\n",
    "# Event Details\n",
    "EVENT_NAME = '202510_Flood_AK'  # Your event name\n",
    "SUB_PRODUCT_NAME = 'sentinel2'         # Sub-directories within EVENT_NAME (RGB, trueColor, SWIR, etc.). Can leave blank.\n",
    "SOURCE_PATH = f'{GEOTIFF_DIR}/{EVENT_NAME}/{SUB_PRODUCT_NAME}'      # Where your files are\n",
    "\n",
    "\n",
    "# Output Options\n",
    "SAVE_CSV = True          # Save mapping to CSV file\n",
    "OUTPUT_DIR = 'file-mapping'    # Local directory for CSV output\n",
    "\n",
    "print(f\"Event: {EVENT_NAME}\")\n",
    "print(f\"Source: s3://{BUCKET}/{SOURCE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Step 2: Connect to S3 and List Files\n",
    "\n",
    "Connect to S3 and view available files with their sizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Import necessary modules\nimport sys\nimport os\nimport boto3\nimport pandas as pd\nfrom pathlib import Path\nfrom datetime import datetime\n\n# Add parent directory to path for importing functions\nsys.path.insert(0, str(Path('..').resolve()))\nfrom core.s3_operations import list_s3_files, get_file_size_from_s3\n\n# Initialize S3 client\nprint(\"Connecting to S3...\")\n\nif USE_TEMP_CREDENTIALS and AWS_ACCESS_KEY_ID:\n    # Use temporary credentials\n    s3_client = boto3.client(\n        's3',\n        aws_access_key_id=AWS_ACCESS_KEY_ID,\n        aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\n        aws_session_token=AWS_SESSION_TOKEN\n    )\n    print(\"Connected to S3 using temporary credentials\\n\")\nelse:\n    # Use default credentials (from environment or external ID)\n    from core.s3_operations import initialize_s3_client\n    s3_client, _ = initialize_s3_client(bucket_name=BUCKET, verbose=False)\n    print(\"Connected to S3 using default credentials\\n\")\n\n\n# List all TIF files\nprint(f\"Files in s3://{BUCKET}/{SOURCE_PATH}:\")\nprint(\"=\"*80)\n\nfiles = list_s3_files(s3_client, BUCKET, SOURCE_PATH, suffix='.tif')\n\nif files:\n    print(f\"Found {len(files)} .tif files\\n\")\n    \n    # Create initial DataFrame with file info\n    file_data = []\n    for file_path in files:\n        filename = os.path.basename(file_path)\n        try:\n            size_gb = get_file_size_from_s3(s3_client, BUCKET, file_path)\n        except:\n            size_gb = 0.0\n        \n        file_data.append({\n            'original_s3_path': file_path,\n            'original_filename': filename,\n            'file_size_gb': size_gb\n        })\n    \n    files_df = pd.DataFrame(file_data)\n    \n    # Display summary\n    print(f\"Total files: {len(files)}\")\n    print(f\"Total size: {files_df['file_size_gb'].sum():.2f} GB\\n\")\n    \n    # Display ALL files\n    print(f\"Complete file list:\")\n    print(\"-\" * 80)\n    for i, row in files_df.iterrows():\n        print(f\"{i+1:3}. {row['original_filename']:<65} ({row['file_size_gb']:.2f} GB)\")\nelse:\n    print(\"‚ö†Ô∏è No .tif files found in the specified path.\")\n    print(\"   Check your SOURCE_PATH configuration.\")\n    files_df = pd.DataFrame()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üè∑Ô∏è Step 3: Define Filename Transformation Functions\n",
    "\n",
    "Based on the files you see above, configure:\n",
    "1. **Categorization patterns** - Regex patterns to identify file types\n",
    "2. **Filename functions** - How to transform filenames\n",
    "3. **Output directories** - Where each category should be saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# CATEGORIZATION AND FILENAME TRANSFORMATION\n",
    "# ========================================\n",
    "\n",
    "import re\n",
    "\n",
    "#Define helper function to extract dates from filenames\n",
    "def extract_date_from_filename(filename):\n",
    "    \"\"\"Extract date from filename in YYYY-MM-DD format.\"\"\"\n",
    "    # Try YYYYMMDD format (d{8} finds a sequence of 8 digits from filename)\n",
    "    dates = re.findall(r'\\d{8}', filename)\n",
    "    if dates:\n",
    "        date_str = dates[0]\n",
    "        # Then it splits the date into different sections\n",
    "        return f\"{date_str[0:4]}-{date_str[4:6]}-{date_str[6:8]}\"\n",
    "    \n",
    "    # Try YYYY-MM-DD format\n",
    "    dates = re.findall(r'\\d{4}-\\d{2}-\\d{2}', filename)\n",
    "    if dates:\n",
    "        return dates[0]\n",
    "    \n",
    "    return None\n",
    "\n",
    "#Define filename transformation functions for each category\n",
    "def create_TC_SWIR_NC_CIR_filename(original_path, event_name):\n",
    "    \"\"\"Create filename for trueColor products.\"\"\"\n",
    "    filename = os.path.basename(original_path)\n",
    "    stem = os.path.splitext(filename)[0]\n",
    "    date = extract_date_from_filename(stem)\n",
    "    \n",
    "    if date:\n",
    "        stem_clean = re.sub(r'_?\\d{8}', '', stem)\n",
    "        stem_clean = re.sub(r'_?\\d{4}-\\d{2}-\\d{2}', '', stem_clean)\n",
    "        return f\"{event_name}_{stem_clean}_{date}_day.tif\"\n",
    "    return f\"{event_name}_{stem}_day.tif\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configure categorization patterns (REQUIRED)\n# These regex patterns determine which files belong to which category\nCATEGORIZATION_PATTERNS = {\n    'trueColor': r'trueColor|truecolor|true_color',\n    'colorInfrared': r'colorInfrared|colorIR|color_infrared',\n    'naturalColor': r'naturalColor|naturalcolor|natural_color',\n    'shortwaveIR': r'shortwaveIR|shortwaveinfrared|shortwaveInfrared'\n    # Add patterns for ALL file types you want to process\n    # Files not matching any pattern will be marked as 'uncategorized'\n}\n\n# Map categories to filename transformation functions\nFILENAME_CREATORS = {\n    'trueColor': create_TC_SWIR_NC_CIR_filename,\n    'colorInfrared': create_TC_SWIR_NC_CIR_filename,\n    'naturalColor': create_TC_SWIR_NC_CIR_filename,\n    'shortwaveIR': create_TC_SWIR_NC_CIR_filename\n    # Must have an entry for each category in CATEGORIZATION_PATTERNS\n}\n\n# Specify output directories for each category\nOUTPUT_DIRS = {\n    'trueColor': 'Sentinel-2/trueColor',\n    'colorInfrared': 'Sentinel-2/colorIR',\n    'naturalColor': 'Sentinel-2/naturalColor',\n    'shortwaveIR': 'Sentinel-2/shortwaveIR'\n    # Must have an entry for each category in CATEGORIZATION_PATTERNS\n}\n\n# Specify nodata values for each category\nNODATA_VALUES = {\n    'trueColor': 0,\n    'colorInfrared': 0,\n    'naturalColor': 0,\n    'shortwaveIR': 0\n    # Must have an entry for each category in CATEGORIZATION_PATTERNS\n}\n\nprint(\"Filename transformation functions defined\")\nprint(f\"\\nCategories configured: {len(CATEGORIZATION_PATTERNS)}\")\nfor category in CATEGORIZATION_PATTERNS.keys():\n    print(f\"   ‚Ä¢ {category}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Step 4: Preview Transformations\n",
    "\n",
    "Apply the transformation functions and preview the mapping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if not files_df.empty:\n    print(\"Applying filename transformations...\\n\")\n    \n    # Apply categorization and transformation\n    def categorize_file(filename):\n        \"\"\"Categorize a file based on patterns.\"\"\"\n        for category, pattern in CATEGORIZATION_PATTERNS.items():\n            if re.search(pattern, filename, re.IGNORECASE):\n                return category\n        return 'uncategorized'\n    \n    def transform_filename(row):\n        \"\"\"Transform filename based on category.\"\"\"\n        category = row['category']\n        original_path = row['original_s3_path']\n        \n        if category == 'uncategorized':\n            return os.path.basename(original_path)  # Keep original\n        \n        if category in FILENAME_CREATORS:\n            return FILENAME_CREATORS[category](original_path, EVENT_NAME)\n        \n        return os.path.basename(original_path)\n    \n    def get_output_path(row):\n        \"\"\"Generate output S3 path.\"\"\"\n        category = row['category']\n        new_filename = row['new_filename']\n        \n        if category == 'uncategorized':\n            return f\"{DESTINATION_BASE}/uncategorized/{new_filename}\"\n        \n        if category in OUTPUT_DIRS:\n            return f\"{DESTINATION_BASE}/{OUTPUT_DIRS[category]}/{new_filename}\"\n        \n        return f\"{DESTINATION_BASE}/{category}/{new_filename}\"\n    \n    def get_nodata_value(category):\n        \"\"\"Get nodata value for category.\"\"\"\n        if category in NODATA_VALUES:\n            return NODATA_VALUES[category]\n        return None  # Default for uncategorized\n    \n    # Apply transformations\n    files_df['category'] = files_df['original_filename'].apply(categorize_file)\n    files_df['new_filename'] = files_df.apply(transform_filename, axis=1)\n    files_df['output_s3_path'] = files_df.apply(get_output_path, axis=1)\n    files_df['nodata_value'] = files_df['category'].apply(get_nodata_value)\n    files_df['status'] = 'valid'\n    \n    # Check for uncategorized files\n    uncategorized = files_df[files_df['category'] == 'uncategorized']\n    if not uncategorized.empty:\n        files_df.loc[files_df['category'] == 'uncategorized', 'status'] = 'uncategorized'\n    \n    # Display preview\n    print(\"TRANSFORMATION PREVIEW\")\n    print(\"=\"*80)\n    print(f\"\\nTotal files: {len(files_df)}\")\n    print(f\"Categorized: {len(files_df[files_df['category'] != 'uncategorized'])}\")\n    print(f\"Uncategorized: {len(uncategorized)}\")\n    \n    # Show category breakdown\n    print(\"\\nFiles by category:\")\n    category_counts = files_df['category'].value_counts()\n    for category, count in category_counts.items():\n        nodata = NODATA_VALUES.get(category, 'None')\n        print(f\"   ‚Ä¢ {category}: {count} files (nodata={nodata})\")\n    \n    # Show sample transformations\n    print(\"\\nTransformation information:\")\n    print(\"-\" * 80)\n    for i, row in files_df.iterrows():\n        print(f\"\\n{i+1}. Original: {row['original_filename']}\")\n        print(f\"   Category: {row['category']}\")\n        print(f\"   New name: {row['new_filename']}\")\n        print(f\"   Nodata:   {row['nodata_value']}\")\n        print(f\"   Output:   s3://{BUCKET}/{row['output_s3_path']}\")\n    \n    if len(uncategorized) > 0:\n        print(\"\\n‚ö†Ô∏è  UNCATEGORIZED FILES:\")\n        print(\"-\" * 80)\n        for i, row in uncategorized.iterrows():\n            print(f\"   ‚Ä¢ {row['original_filename']}\")\n        print(\"\\nAdd patterns to CATEGORIZATION_PATTERNS to categorize these files\")\n    \n    print(\"\\n\" + \"=\"*80)\nelse:\n    print(\"‚ö†Ô∏è No files to process. Check Step 2.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ Step 5: Export Mapping to CSV\n",
    "\n",
    "Save the filename mapping to a CSV file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if not files_df.empty and SAVE_CSV:\n    # Create output directory\n    output_path = Path(OUTPUT_DIR) / EVENT_NAME\n    output_path.mkdir(parents=True, exist_ok=True)\n\n    csv_filename = f\"{EVENT_NAME}-{SUB_PRODUCT_NAME}.csv\"\n    csv_path = output_path / csv_filename\n    \n    # Reorder columns for better readability\n    column_order = [\n        'original_filename',\n        'new_filename',\n        'category',\n        'file_size_gb',\n        'nodata_value',\n        'status',\n        'original_s3_path',\n        'output_s3_path'\n    ]\n    \n    # Save to CSV\n    files_df[column_order].to_csv(csv_path, index=False)\n    \n    print(f\"EXPORT COMPLETE. Saved mapping to: {csv_path}\")\n    print(\"=\"*80)\n    print(f\"   Total records: {len(files_df)}\")\n    print(f\"   Total size:    {files_df['file_size_gb'].sum():.2f} GB\")\n    print(f\"   Valid:         {len(files_df[files_df['status'] == 'valid'])}\")\n    print(f\"   Uncategorized: {len(files_df[files_df['status'] == 'uncategorized'])}\")\n    \n    # Show nodata value distribution\n    nodata_counts = files_df['nodata_value'].value_counts()\n    print(f\"\\n   Nodata values:\")\n    for nodata, count in nodata_counts.items():\n        print(f\"      {nodata}: {count} files\")\n    \n    print(\"\\nYou can now use this CSV in a separate script to perform actual file renaming/copying\")\n    \n    # Display the DataFrame\n    print(\"\\nFull mapping table:\")\n    print(\"=\"*80)\n    display(files_df[column_order])\n    \nelif files_df.empty:\n    print(\"‚ö†Ô∏è No files to export. Check previous steps.\")\nelse:\n    print(\"‚ÑπÔ∏è  CSV export disabled (SAVE_CSV = False)\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}