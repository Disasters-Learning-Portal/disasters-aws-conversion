{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìÅ Local File Processing to S3\n",
    "\n",
    "This notebook processes GeoTIFF files from a **local directory**, converts them to Cloud Optimized GeoTIFFs (COGs), and uploads them to S3 with renamed filenames.\n",
    "\n",
    "## ‚ú® Features\n",
    "- **Process local files** - No need to upload to S3 first\n",
    "- **Filename transformation** - Define custom renaming functions\n",
    "- **CSV mapping** - Optional export of filename mappings\n",
    "- **COG conversion** - Automatic optimization with compression\n",
    "- **Direct S3 upload** - Upload with new names to destination bucket\n",
    "\n",
    "## üìã Workflow\n",
    "1. Configure local directory and S3 destination\n",
    "2. List local .tif files\n",
    "3. Define filename transformations\n",
    "4. Preview transformations\n",
    "5. (Optional) Save mapping to CSV\n",
    "6. Connect to S3\n",
    "7. Process and upload files\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Step 1: Configuration\n",
    "\n",
    "Set your local directory path and S3 destination:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# INPUTS\n",
    "# ========================================\n",
    "\n",
    "# Local File Path\n",
    "LOCAL_DIR = '/path/to/your/local/geotiffs'  # Change this to your local directory\n",
    "\n",
    "# S3 Configuration\n",
    "BUCKET = 'nasa-disasters'    # S3 bucket (DO NOT CHANGE)\n",
    "DESTINATION_BASE = 'drcs_activations_new'  # Where to save COGs in S3\n",
    "\n",
    "# Event Details\n",
    "EVENT_NAME = '202510_Flood_AK'  # Your event name\n",
    "SUB_PRODUCT_NAME = 'sentinel2'  # Sub-product identifier\n",
    "\n",
    "# Processing Options\n",
    "OVERWRITE = False      # Set to True to replace existing files in S3\n",
    "VERIFY = True          # Verify COGs after creation\n",
    "SAVE_CSV = True        # Save filename mapping to CSV\n",
    "SAVE_RESULTS = True    # Save processing results to CSV\n",
    "\n",
    "# Output\n",
    "OUTPUT_DIR = 'local-processing-output'  # Directory for CSV files\n",
    "\n",
    "print(f\"Local Directory: {LOCAL_DIR}\")\n",
    "print(f\"Event: {EVENT_NAME}\")\n",
    "print(f\"Destination: s3://{BUCKET}/{DESTINATION_BASE}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÇ Step 2: List Local Files\n",
    "\n",
    "Scan the local directory for GeoTIFF files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import glob\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.insert(0, str(Path('..').resolve()))\n",
    "\n",
    "print(\"üìÇ SCANNING LOCAL DIRECTORY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nSearching for .tif files in: {LOCAL_DIR}\\n\")\n",
    "\n",
    "# Find all .tif files (recursively)\n",
    "local_dir_path = Path(LOCAL_DIR)\n",
    "if not local_dir_path.exists():\n",
    "    print(f\"‚ùå ERROR: Directory does not exist: {LOCAL_DIR}\")\n",
    "    print(\"   Please check your LOCAL_DIR path and try again.\")\n",
    "    files_df = pd.DataFrame()\n",
    "else:\n",
    "    # Find all .tif files\n",
    "    tif_files = list(local_dir_path.rglob('*.tif')) + list(local_dir_path.rglob('*.TIF'))\n",
    "    \n",
    "    if tif_files:\n",
    "        print(f\"‚úÖ Found {len(tif_files)} .tif files\\n\")\n",
    "        \n",
    "        # Create DataFrame with file info\n",
    "        file_data = []\n",
    "        for file_path in tif_files:\n",
    "            file_size_bytes = file_path.stat().st_size\n",
    "            file_size_gb = file_size_bytes / (1024 ** 3)\n",
    "            \n",
    "            file_data.append({\n",
    "                'local_path': str(file_path),\n",
    "                'original_filename': file_path.name,\n",
    "                'file_size_gb': file_size_gb,\n",
    "                'relative_path': str(file_path.relative_to(local_dir_path))\n",
    "            })\n",
    "        \n",
    "        files_df = pd.DataFrame(file_data)\n",
    "        \n",
    "        # Display summary\n",
    "        print(f\"Total files: {len(files_df)}\")\n",
    "        print(f\"Total size: {files_df['file_size_gb'].sum():.2f} GB\\n\")\n",
    "        \n",
    "        # Display file list\n",
    "        print(\"File list:\")\n",
    "        print(\"-\" * 80)\n",
    "        for i, row in files_df.iterrows():\n",
    "            print(f\"{i+1:3}. {row['original_filename']:<60} ({row['file_size_gb']:.3f} GB)\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No .tif files found in the specified directory.\")\n",
    "        print(\"   Check your LOCAL_DIR path.\")\n",
    "        files_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üè∑Ô∏è Step 3: Define Filename Transformations\n",
    "\n",
    "Configure how files should be renamed and categorized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# CATEGORIZATION AND FILENAME TRANSFORMATION\n",
    "# ========================================\n",
    "\n",
    "import re\n",
    "\n",
    "# Define helper function to extract dates\n",
    "def extract_date_from_filename(filename):\n",
    "    \"\"\"Extract date from filename in YYYY-MM-DD format.\"\"\"\n",
    "    # Try YYYYMMDD format\n",
    "    dates = re.findall(r'\\d{8}', filename)\n",
    "    if dates:\n",
    "        date_str = dates[0]\n",
    "        return f\"{date_str[0:4]}-{date_str[4:6]}-{date_str[6:8]}\"\n",
    "    \n",
    "    # Try YYYY-MM-DD format\n",
    "    dates = re.findall(r'\\d{4}-\\d{2}-\\d{2}', filename)\n",
    "    if dates:\n",
    "        return dates[0]\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Define filename transformation functions\n",
    "def create_standard_filename(original_path, event_name):\n",
    "    \"\"\"Create standardized filename.\"\"\"\n",
    "    filename = os.path.basename(original_path)\n",
    "    stem = os.path.splitext(filename)[0]\n",
    "    date = extract_date_from_filename(stem)\n",
    "    \n",
    "    if date:\n",
    "        stem_clean = re.sub(r'_?\\d{8}', '', stem)\n",
    "        stem_clean = re.sub(r'_?\\d{4}-\\d{2}-\\d{2}', '', stem_clean)\n",
    "        return f\"{event_name}_{stem_clean}_{date}_day.tif\"\n",
    "    return f\"{event_name}_{stem}_day.tif\"\n",
    "\n",
    "# Configure categorization patterns\n",
    "CATEGORIZATION_PATTERNS = {\n",
    "    'trueColor': r'trueColor|truecolor|true_color|RGB',\n",
    "    'colorInfrared': r'colorInfrared|colorIR|color_infrared|CIR',\n",
    "    'naturalColor': r'naturalColor|naturalcolor|natural_color',\n",
    "    'shortwaveIR': r'shortwaveIR|SWIR|shortwave'\n",
    "}\n",
    "\n",
    "# Map categories to filename functions\n",
    "FILENAME_CREATORS = {\n",
    "    'trueColor': create_standard_filename,\n",
    "    'colorInfrared': create_standard_filename,\n",
    "    'naturalColor': create_standard_filename,\n",
    "    'shortwaveIR': create_standard_filename\n",
    "}\n",
    "\n",
    "# Output directories in S3\n",
    "OUTPUT_DIRS = {\n",
    "    'trueColor': 'Sentinel-2/trueColor',\n",
    "    'colorInfrared': 'Sentinel-2/colorIR',\n",
    "    'naturalColor': 'Sentinel-2/naturalColor',\n",
    "    'shortwaveIR': 'Sentinel-2/shortwaveIR'\n",
    "}\n",
    "\n",
    "# Nodata values\n",
    "NODATA_VALUES = {\n",
    "    'trueColor': 0,\n",
    "    'colorInfrared': 0,\n",
    "    'naturalColor': 0,\n",
    "    'shortwaveIR': 0\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Transformation functions defined\")\n",
    "print(f\"\\nCategories configured: {len(CATEGORIZATION_PATTERNS)}\")\n",
    "for category in CATEGORIZATION_PATTERNS.keys():\n",
    "    print(f\"   ‚Ä¢ {category}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Step 4: Preview Transformations\n",
    "\n",
    "Apply transformations and preview the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not files_df.empty:\n",
    "    print(\"üìã APPLYING TRANSFORMATIONS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Categorization function\n",
    "    def categorize_file(filename):\n",
    "        for category, pattern in CATEGORIZATION_PATTERNS.items():\n",
    "            if re.search(pattern, filename, re.IGNORECASE):\n",
    "                return category\n",
    "        return 'uncategorized'\n",
    "    \n",
    "    # Transformation function\n",
    "    def transform_filename(row):\n",
    "        category = row['category']\n",
    "        local_path = row['local_path']\n",
    "        \n",
    "        if category == 'uncategorized':\n",
    "            return row['original_filename']\n",
    "        \n",
    "        if category in FILENAME_CREATORS:\n",
    "            return FILENAME_CREATORS[category](local_path, EVENT_NAME)\n",
    "        \n",
    "        return row['original_filename']\n",
    "    \n",
    "    # Generate S3 output path\n",
    "    def get_output_path(row):\n",
    "        category = row['category']\n",
    "        new_filename = row['new_filename']\n",
    "        \n",
    "        if category == 'uncategorized':\n",
    "            return f\"{DESTINATION_BASE}/uncategorized/{new_filename}\"\n",
    "        \n",
    "        if category in OUTPUT_DIRS:\n",
    "            return f\"{DESTINATION_BASE}/{OUTPUT_DIRS[category]}/{new_filename}\"\n",
    "        \n",
    "        return f\"{DESTINATION_BASE}/{category}/{new_filename}\"\n",
    "    \n",
    "    # Get nodata value\n",
    "    def get_nodata_value(category):\n",
    "        return NODATA_VALUES.get(category, None)\n",
    "    \n",
    "    # Apply transformations\n",
    "    files_df['category'] = files_df['original_filename'].apply(categorize_file)\n",
    "    files_df['new_filename'] = files_df.apply(transform_filename, axis=1)\n",
    "    files_df['output_s3_path'] = files_df.apply(get_output_path, axis=1)\n",
    "    files_df['nodata_value'] = files_df['category'].apply(get_nodata_value)\n",
    "    files_df['status'] = files_df['category'].apply(lambda x: 'valid' if x != 'uncategorized' else 'uncategorized')\n",
    "    \n",
    "    # Display summary\n",
    "    print(f\"\\nTotal files: {len(files_df)}\")\n",
    "    print(f\"Categorized: {len(files_df[files_df['category'] != 'uncategorized'])}\")\n",
    "    print(f\"Uncategorized: {len(files_df[files_df['category'] == 'uncategorized'])}\")\n",
    "    \n",
    "    # Category breakdown\n",
    "    print(\"\\nFiles by category:\")\n",
    "    category_counts = files_df['category'].value_counts()\n",
    "    for category, count in category_counts.items():\n",
    "        nodata = NODATA_VALUES.get(category, 'None')\n",
    "        print(f\"   ‚Ä¢ {category}: {count} files (nodata={nodata})\")\n",
    "    \n",
    "    # Show sample transformations\n",
    "    print(\"\\nüìù Sample transformations:\")\n",
    "    print(\"-\" * 80)\n",
    "    for i, row in files_df.head(5).iterrows():\n",
    "        print(f\"\\n{i+1}. Original: {row['original_filename']}\")\n",
    "        print(f\"   Category: {row['category']}\")\n",
    "        print(f\"   New name: {row['new_filename']}\")\n",
    "        print(f\"   Output:   s3://{BUCKET}/{row['output_s3_path']}\")\n",
    "    \n",
    "    if len(files_df.head(5)) < len(files_df):\n",
    "        print(f\"\\n   ... and {len(files_df) - 5} more files\")\n",
    "    \n",
    "    # Show uncategorized files\n",
    "    uncategorized = files_df[files_df['category'] == 'uncategorized']\n",
    "    if not uncategorized.empty:\n",
    "        print(\"\\n‚ö†Ô∏è  UNCATEGORIZED FILES:\")\n",
    "        print(\"-\" * 80)\n",
    "        for _, row in uncategorized.iterrows():\n",
    "            print(f\"   ‚Ä¢ {row['original_filename']}\")\n",
    "        print(\"\\nAdd patterns to CATEGORIZATION_PATTERNS to categorize these files\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No files to process. Check Step 2.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ Step 5: Save Mapping to CSV (Optional)\n",
    "\n",
    "Export the filename mapping for your records:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not files_df.empty and SAVE_CSV:\n",
    "    # Create output directory\n",
    "    output_path = Path(OUTPUT_DIR) / EVENT_NAME\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Generate filename\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    csv_filename = f\"{EVENT_NAME}-{SUB_PRODUCT_NAME}-mapping_{timestamp}.csv\"\n",
    "    csv_path = output_path / csv_filename\n",
    "    \n",
    "    # Column order\n",
    "    column_order = [\n",
    "        'original_filename',\n",
    "        'new_filename',\n",
    "        'category',\n",
    "        'file_size_gb',\n",
    "        'nodata_value',\n",
    "        'status',\n",
    "        'local_path',\n",
    "        'output_s3_path'\n",
    "    ]\n",
    "    \n",
    "    # Save to CSV\n",
    "    files_df[column_order].to_csv(csv_path, index=False)\n",
    "    \n",
    "    print(\"üíæ CSV MAPPING SAVED\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\n‚úÖ Saved to: {csv_path.absolute()}\")\n",
    "    print(f\"\\nüìä Summary:\")\n",
    "    print(f\"   Total records: {len(files_df)}\")\n",
    "    print(f\"   Total size:    {files_df['file_size_gb'].sum():.2f} GB\")\n",
    "    print(f\"   Valid:         {len(files_df[files_df['status'] == 'valid'])}\")\n",
    "    print(f\"   Uncategorized: {len(files_df[files_df['status'] == 'uncategorized'])}\")\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "elif files_df.empty:\n",
    "    print(\"‚ö†Ô∏è No files to save. Check previous steps.\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è  CSV export disabled (SAVE_CSV = False)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üåê Step 6: Connect to S3\n",
    "\n",
    "Initialize S3 client with upload permissions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.core.s3_operations import initialize_s3_client, check_s3_file_exists\n",
    "\n",
    "print(\"üåê Connecting to S3...\")\n",
    "s3_client, fs = initialize_s3_client(bucket_name=BUCKET, verbose=True)\n",
    "\n",
    "if not s3_client:\n",
    "    print(\"\\n‚ùå Failed to connect to S3\")\n",
    "    print(\"   Check your AWS credentials and try again.\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ S3 connection ready\")\n",
    "    print(\"   You can now proceed to process and upload files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Step 7: Process and Upload Files\n",
    "\n",
    "Convert files to COGs and upload to S3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not files_df.empty and s3_client:\n",
    "    # Filter to only valid files\n",
    "    files_to_process = files_df[files_df['status'] == 'valid'].copy()\n",
    "    \n",
    "    if files_to_process.empty:\n",
    "        print(\"‚ö†Ô∏è No valid files to process.\")\n",
    "        print(\"   All files are uncategorized. Update CATEGORIZATION_PATTERNS and retry.\")\n",
    "    else:\n",
    "        print(\"üöÄ STARTING COG PROCESSING AND UPLOAD\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"\\nProcessing {len(files_to_process)} files...\")\n",
    "        print(\"This may take several minutes depending on file sizes.\\n\")\n",
    "        \n",
    "        print(f\"Processing options:\")\n",
    "        print(f\"  Overwrite existing: {OVERWRITE}\")\n",
    "        print(f\"  Verify COGs: {VERIFY}\\n\")\n",
    "        \n",
    "        # Import processing function\n",
    "        from lib.main_processor import convert_to_cog\n",
    "        import time\n",
    "        \n",
    "        # Track results\n",
    "        results = []\n",
    "        \n",
    "        for idx, row in files_to_process.iterrows():\n",
    "            start_time = time.time()\n",
    "            \n",
    "            local_path = row['local_path']\n",
    "            output_key = row['output_s3_path']\n",
    "            nodata = row['nodata_value']\n",
    "            \n",
    "            print(f\"\\n[{idx+1}/{len(files_to_process)}] Processing: {row['original_filename']}\")\n",
    "            print(f\"    Category: {row['category']}\")\n",
    "            print(f\"    Size: {row['file_size_gb']:.2f} GB\")\n",
    "            print(f\"    Output: {row['new_filename']}\")\n",
    "            \n",
    "            # Check if destination exists (unless OVERWRITE)\n",
    "            if not OVERWRITE:\n",
    "                if check_s3_file_exists(s3_client, BUCKET, output_key):\n",
    "                    print(f\"    ‚è≠Ô∏è  SKIPPED (already exists in S3)\")\n",
    "                    results.append({\n",
    "                        'source_file': row['original_filename'],\n",
    "                        'output_file': row['new_filename'],\n",
    "                        'category': row['category'],\n",
    "                        'status': 'skipped',\n",
    "                        'time_seconds': 0,\n",
    "                        'error': 'File already exists'\n",
    "                    })\n",
    "                    continue\n",
    "            \n",
    "            try:\n",
    "                # Convert to COG and upload\n",
    "                success = convert_to_cog(\n",
    "                    name=local_path,\n",
    "                    bucket=BUCKET,\n",
    "                    cog_filename=row['new_filename'],\n",
    "                    cog_data_bucket=BUCKET,\n",
    "                    cog_data_prefix=f\"{DESTINATION_BASE}/{OUTPUT_DIRS[row['category']]}\",\n",
    "                    nodata_value=nodata,\n",
    "                    verify_cog=VERIFY,\n",
    "                    verbose=False\n",
    "                )\n",
    "                \n",
    "                elapsed = time.time() - start_time\n",
    "                \n",
    "                if success:\n",
    "                    print(f\"    ‚úÖ SUCCESS ({elapsed:.1f}s)\")\n",
    "                    results.append({\n",
    "                        'source_file': row['original_filename'],\n",
    "                        'output_file': row['new_filename'],\n",
    "                        'category': row['category'],\n",
    "                        'status': 'success',\n",
    "                        'time_seconds': elapsed,\n",
    "                        'error': None\n",
    "                    })\n",
    "                else:\n",
    "                    print(f\"    ‚ùå FAILED ({elapsed:.1f}s)\")\n",
    "                    results.append({\n",
    "                        'source_file': row['original_filename'],\n",
    "                        'output_file': row['new_filename'],\n",
    "                        'category': row['category'],\n",
    "                        'status': 'failed',\n",
    "                        'time_seconds': elapsed,\n",
    "                        'error': 'Processing failed'\n",
    "                    })\n",
    "            \n",
    "            except Exception as e:\n",
    "                elapsed = time.time() - start_time\n",
    "                print(f\"    ‚ùå ERROR: {str(e)}\")\n",
    "                results.append({\n",
    "                    'source_file': row['original_filename'],\n",
    "                    'output_file': row['new_filename'],\n",
    "                    'category': row['category'],\n",
    "                    'status': 'failed',\n",
    "                    'time_seconds': elapsed,\n",
    "                    'error': str(e)\n",
    "                })\n",
    "        \n",
    "        # Create results DataFrame\n",
    "        results_df = pd.DataFrame(results)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"\\nüéâ PROCESSING COMPLETE!\")\n",
    "        \n",
    "        # Save results if requested\n",
    "        if SAVE_RESULTS and not results_df.empty:\n",
    "            output_path = Path(OUTPUT_DIR) / EVENT_NAME\n",
    "            output_path.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "            results_filename = f\"processing_results_{timestamp}.csv\"\n",
    "            results_path = output_path / results_filename\n",
    "            \n",
    "            results_df.to_csv(results_path, index=False)\n",
    "            print(f\"\\nüíæ Results saved to: {results_path.absolute()}\")\n",
    "\nelse:\n",
    "    print(\"‚ö†Ô∏è Cannot process: No files or S3 not connected\")\n",
    "    results_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Step 8: Review Results\n",
    "\n",
    "Display processing statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'results_df' in locals() and not results_df.empty:\n",
    "    print(\"üìä PROCESSING STATISTICS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Success rate\n",
    "    total = len(results_df)\n",
    "    success = len(results_df[results_df['status'] == 'success'])\n",
    "    failed = len(results_df[results_df['status'] == 'failed'])\n",
    "    skipped = len(results_df[results_df['status'] == 'skipped'])\n",
    "    \n",
    "    print(f\"\\nTotal files: {total}\")\n",
    "    print(f\"‚úÖ Success: {success}\")\n",
    "    print(f\"‚ùå Failed: {failed}\")\n",
    "    print(f\"‚è≠Ô∏è  Skipped: {skipped}\")\n",
    "    print(f\"\\nSuccess rate: {(success/total*100):.1f}%\")\n",
    "    \n",
    "    # Show failed files\n",
    "    if failed > 0:\n",
    "        print(\"\\n‚ùå Failed files:\")\n",
    "        failed_df = results_df[results_df['status'] == 'failed']\n",
    "        for idx, row in failed_df.iterrows():\n",
    "            print(f\"  - {row['source_file']}: {row.get('error', 'Unknown error')}\")\n",
    "    \n",
    "    # Processing times\n",
    "    if 'time_seconds' in results_df.columns:\n",
    "        success_df = results_df[results_df['status'] == 'success']\n",
    "        if not success_df.empty:\n",
    "            avg_time = success_df['time_seconds'].mean()\n",
    "            max_time = success_df['time_seconds'].max()\n",
    "            total_time = success_df['time_seconds'].sum()\n",
    "            print(f\"\\n‚è±Ô∏è  Timing:\")\n",
    "            print(f\"Average: {avg_time:.1f}s per file\")\n",
    "            print(f\"Slowest: {max_time:.1f}s\")\n",
    "            print(f\"Total:   {total_time:.1f}s ({total_time/60:.1f} minutes)\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    \n",
    "    # Display results table\n",
    "    print(\"\\nüìã Detailed Results:\")\n",
    "    display(results_df)\n",
    "else:\n",
    "    print(\"No results to display. Run Step 7 first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí° Tips & Troubleshooting\n",
    "\n",
    "### Common Issues:\n",
    "\n",
    "1. **\"Directory does not exist\"**\n",
    "   - Check `LOCAL_DIR` path is correct\n",
    "   - Use absolute paths (e.g., `/Users/name/data/geotiffs`)\n",
    "   - On Windows, use forward slashes or raw strings (e.g., `r\"C:\\\\path\\\\to\\\\files\"`)\n",
    "\n",
    "2. **\"No .tif files found\"**\n",
    "   - Verify files have `.tif` or `.TIF` extension\n",
    "   - Check subdirectories are included\n",
    "   - Try listing files manually with `ls` or File Explorer\n",
    "\n",
    "3. **\"S3 connection failed\"**\n",
    "   - Check AWS credentials are configured\n",
    "   - For upload permissions, configure external ID in `aws_credentials.py`\n",
    "   - Test with `lib/test_upload.py`\n",
    "\n",
    "4. **\"Files being skipped\"**\n",
    "   - Files already exist in S3 destination\n",
    "   - Set `OVERWRITE = True` to replace existing files\n",
    "\n",
    "5. **\"Processing failures\"**\n",
    "   - Check source files are valid GeoTIFFs\n",
    "   - Verify enough disk space for temporary files\n",
    "   - Check S3 write permissions\n",
    "\n",
    "### Performance Notes:\n",
    "- Processing time varies by file size (typically 30s-5min per file)\n",
    "- COGs use ZSTD compression level 22\n",
    "- Predictor automatically selected based on data type\n",
    "- Large files (>10GB) use optimized block sizes\n",
    "\n",
    "### Next Steps:\n",
    "1. Review processing results and any failures\n",
    "2. Verify uploaded files in S3 console\n",
    "3. Check CSV files for complete mapping records\n",
    "4. Re-run failed files if needed (set `OVERWRITE = False` to skip successful ones)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
